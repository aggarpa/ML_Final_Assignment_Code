{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a4a68c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X = pd.read_csv(\"final_tf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d1e9ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>id</th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms_text</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "      <th>amenities</th>\n",
       "      <th>price</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>review_scores_value</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>room_type</th>\n",
       "      <th>property_type</th>\n",
       "      <th>Unnamed: 0_y</th>\n",
       "      <th>tfidf_0</th>\n",
       "      <th>tfidf_1</th>\n",
       "      <th>tfidf_2</th>\n",
       "      <th>tfidf_3</th>\n",
       "      <th>tfidf_4</th>\n",
       "      <th>tfidf_5</th>\n",
       "      <th>tfidf_6</th>\n",
       "      <th>tfidf_7</th>\n",
       "      <th>tfidf_8</th>\n",
       "      <th>tfidf_9</th>\n",
       "      <th>tfidf_10</th>\n",
       "      <th>tfidf_11</th>\n",
       "      <th>tfidf_12</th>\n",
       "      <th>tfidf_13</th>\n",
       "      <th>tfidf_14</th>\n",
       "      <th>tfidf_15</th>\n",
       "      <th>tfidf_16</th>\n",
       "      <th>tfidf_17</th>\n",
       "      <th>tfidf_18</th>\n",
       "      <th>tfidf_19</th>\n",
       "      <th>tfidf_20</th>\n",
       "      <th>tfidf_21</th>\n",
       "      <th>tfidf_22</th>\n",
       "      <th>tfidf_23</th>\n",
       "      <th>tfidf_24</th>\n",
       "      <th>tfidf_25</th>\n",
       "      <th>tfidf_26</th>\n",
       "      <th>tfidf_27</th>\n",
       "      <th>tfidf_28</th>\n",
       "      <th>tfidf_29</th>\n",
       "      <th>tfidf_30</th>\n",
       "      <th>tfidf_31</th>\n",
       "      <th>tfidf_32</th>\n",
       "      <th>tfidf_33</th>\n",
       "      <th>tfidf_34</th>\n",
       "      <th>tfidf_35</th>\n",
       "      <th>tfidf_36</th>\n",
       "      <th>tfidf_37</th>\n",
       "      <th>tfidf_38</th>\n",
       "      <th>tfidf_39</th>\n",
       "      <th>tfidf_40</th>\n",
       "      <th>tfidf_41</th>\n",
       "      <th>tfidf_42</th>\n",
       "      <th>tfidf_43</th>\n",
       "      <th>tfidf_44</th>\n",
       "      <th>tfidf_45</th>\n",
       "      <th>tfidf_46</th>\n",
       "      <th>tfidf_47</th>\n",
       "      <th>tfidf_48</th>\n",
       "      <th>tfidf_49</th>\n",
       "      <th>tfidf_50</th>\n",
       "      <th>tfidf_51</th>\n",
       "      <th>tfidf_52</th>\n",
       "      <th>tfidf_53</th>\n",
       "      <th>tfidf_54</th>\n",
       "      <th>tfidf_55</th>\n",
       "      <th>tfidf_56</th>\n",
       "      <th>tfidf_57</th>\n",
       "      <th>tfidf_58</th>\n",
       "      <th>tfidf_59</th>\n",
       "      <th>tfidf_60</th>\n",
       "      <th>tfidf_61</th>\n",
       "      <th>tfidf_62</th>\n",
       "      <th>tfidf_63</th>\n",
       "      <th>tfidf_64</th>\n",
       "      <th>tfidf_65</th>\n",
       "      <th>tfidf_66</th>\n",
       "      <th>tfidf_67</th>\n",
       "      <th>tfidf_68</th>\n",
       "      <th>tfidf_69</th>\n",
       "      <th>tfidf_70</th>\n",
       "      <th>tfidf_71</th>\n",
       "      <th>tfidf_72</th>\n",
       "      <th>tfidf_73</th>\n",
       "      <th>tfidf_74</th>\n",
       "      <th>tfidf_75</th>\n",
       "      <th>tfidf_76</th>\n",
       "      <th>tfidf_77</th>\n",
       "      <th>tfidf_78</th>\n",
       "      <th>tfidf_79</th>\n",
       "      <th>tfidf_80</th>\n",
       "      <th>tfidf_81</th>\n",
       "      <th>tfidf_82</th>\n",
       "      <th>tfidf_83</th>\n",
       "      <th>tfidf_84</th>\n",
       "      <th>tfidf_85</th>\n",
       "      <th>tfidf_86</th>\n",
       "      <th>tfidf_87</th>\n",
       "      <th>tfidf_88</th>\n",
       "      <th>tfidf_89</th>\n",
       "      <th>tfidf_90</th>\n",
       "      <th>tfidf_91</th>\n",
       "      <th>tfidf_92</th>\n",
       "      <th>tfidf_93</th>\n",
       "      <th>tfidf_94</th>\n",
       "      <th>tfidf_95</th>\n",
       "      <th>tfidf_96</th>\n",
       "      <th>tfidf_97</th>\n",
       "      <th>tfidf_98</th>\n",
       "      <th>tfidf_99</th>\n",
       "      <th>tfidf_100</th>\n",
       "      <th>tfidf_101</th>\n",
       "      <th>tfidf_102</th>\n",
       "      <th>tfidf_103</th>\n",
       "      <th>tfidf_104</th>\n",
       "      <th>tfidf_105</th>\n",
       "      <th>tfidf_106</th>\n",
       "      <th>tfidf_107</th>\n",
       "      <th>tfidf_108</th>\n",
       "      <th>tfidf_109</th>\n",
       "      <th>tfidf_110</th>\n",
       "      <th>tfidf_111</th>\n",
       "      <th>tfidf_112</th>\n",
       "      <th>tfidf_113</th>\n",
       "      <th>tfidf_114</th>\n",
       "      <th>tfidf_115</th>\n",
       "      <th>tfidf_116</th>\n",
       "      <th>tfidf_117</th>\n",
       "      <th>tfidf_118</th>\n",
       "      <th>tfidf_119</th>\n",
       "      <th>tfidf_120</th>\n",
       "      <th>tfidf_121</th>\n",
       "      <th>tfidf_122</th>\n",
       "      <th>tfidf_123</th>\n",
       "      <th>tfidf_124</th>\n",
       "      <th>tfidf_125</th>\n",
       "      <th>tfidf_126</th>\n",
       "      <th>tfidf_127</th>\n",
       "      <th>tfidf_128</th>\n",
       "      <th>tfidf_129</th>\n",
       "      <th>tfidf_130</th>\n",
       "      <th>tfidf_131</th>\n",
       "      <th>tfidf_132</th>\n",
       "      <th>tfidf_133</th>\n",
       "      <th>tfidf_134</th>\n",
       "      <th>tfidf_135</th>\n",
       "      <th>tfidf_136</th>\n",
       "      <th>tfidf_137</th>\n",
       "      <th>tfidf_138</th>\n",
       "      <th>tfidf_139</th>\n",
       "      <th>tfidf_140</th>\n",
       "      <th>tfidf_141</th>\n",
       "      <th>tfidf_142</th>\n",
       "      <th>tfidf_143</th>\n",
       "      <th>tfidf_144</th>\n",
       "      <th>tfidf_145</th>\n",
       "      <th>tfidf_146</th>\n",
       "      <th>tfidf_147</th>\n",
       "      <th>tfidf_148</th>\n",
       "      <th>tfidf_149</th>\n",
       "      <th>tfidf_150</th>\n",
       "      <th>tfidf_151</th>\n",
       "      <th>tfidf_152</th>\n",
       "      <th>tfidf_153</th>\n",
       "      <th>tfidf_154</th>\n",
       "      <th>tfidf_155</th>\n",
       "      <th>tfidf_156</th>\n",
       "      <th>tfidf_157</th>\n",
       "      <th>tfidf_158</th>\n",
       "      <th>tfidf_159</th>\n",
       "      <th>tfidf_160</th>\n",
       "      <th>tfidf_161</th>\n",
       "      <th>tfidf_162</th>\n",
       "      <th>tfidf_163</th>\n",
       "      <th>tfidf_164</th>\n",
       "      <th>tfidf_165</th>\n",
       "      <th>tfidf_166</th>\n",
       "      <th>tfidf_167</th>\n",
       "      <th>tfidf_168</th>\n",
       "      <th>tfidf_169</th>\n",
       "      <th>tfidf_170</th>\n",
       "      <th>tfidf_171</th>\n",
       "      <th>tfidf_172</th>\n",
       "      <th>tfidf_173</th>\n",
       "      <th>tfidf_174</th>\n",
       "      <th>tfidf_175</th>\n",
       "      <th>tfidf_176</th>\n",
       "      <th>tfidf_177</th>\n",
       "      <th>tfidf_178</th>\n",
       "      <th>tfidf_179</th>\n",
       "      <th>tfidf_180</th>\n",
       "      <th>tfidf_181</th>\n",
       "      <th>tfidf_182</th>\n",
       "      <th>tfidf_183</th>\n",
       "      <th>tfidf_184</th>\n",
       "      <th>tfidf_185</th>\n",
       "      <th>tfidf_186</th>\n",
       "      <th>tfidf_187</th>\n",
       "      <th>tfidf_188</th>\n",
       "      <th>tfidf_189</th>\n",
       "      <th>tfidf_190</th>\n",
       "      <th>tfidf_191</th>\n",
       "      <th>tfidf_192</th>\n",
       "      <th>tfidf_193</th>\n",
       "      <th>tfidf_194</th>\n",
       "      <th>tfidf_195</th>\n",
       "      <th>tfidf_196</th>\n",
       "      <th>tfidf_197</th>\n",
       "      <th>tfidf_198</th>\n",
       "      <th>tfidf_199</th>\n",
       "      <th>tfidf_200</th>\n",
       "      <th>tfidf_201</th>\n",
       "      <th>tfidf_202</th>\n",
       "      <th>tfidf_203</th>\n",
       "      <th>tfidf_204</th>\n",
       "      <th>tfidf_205</th>\n",
       "      <th>tfidf_206</th>\n",
       "      <th>tfidf_207</th>\n",
       "      <th>tfidf_208</th>\n",
       "      <th>tfidf_209</th>\n",
       "      <th>tfidf_210</th>\n",
       "      <th>tfidf_211</th>\n",
       "      <th>tfidf_212</th>\n",
       "      <th>tfidf_213</th>\n",
       "      <th>tfidf_214</th>\n",
       "      <th>tfidf_215</th>\n",
       "      <th>tfidf_216</th>\n",
       "      <th>tfidf_217</th>\n",
       "      <th>tfidf_218</th>\n",
       "      <th>tfidf_219</th>\n",
       "      <th>tfidf_220</th>\n",
       "      <th>tfidf_221</th>\n",
       "      <th>tfidf_222</th>\n",
       "      <th>tfidf_223</th>\n",
       "      <th>tfidf_224</th>\n",
       "      <th>tfidf_225</th>\n",
       "      <th>tfidf_226</th>\n",
       "      <th>tfidf_227</th>\n",
       "      <th>tfidf_228</th>\n",
       "      <th>tfidf_229</th>\n",
       "      <th>tfidf_230</th>\n",
       "      <th>tfidf_231</th>\n",
       "      <th>tfidf_232</th>\n",
       "      <th>tfidf_233</th>\n",
       "      <th>tfidf_234</th>\n",
       "      <th>tfidf_235</th>\n",
       "      <th>tfidf_236</th>\n",
       "      <th>tfidf_237</th>\n",
       "      <th>tfidf_238</th>\n",
       "      <th>tfidf_239</th>\n",
       "      <th>tfidf_240</th>\n",
       "      <th>tfidf_241</th>\n",
       "      <th>tfidf_242</th>\n",
       "      <th>tfidf_243</th>\n",
       "      <th>tfidf_244</th>\n",
       "      <th>tfidf_245</th>\n",
       "      <th>tfidf_246</th>\n",
       "      <th>tfidf_247</th>\n",
       "      <th>tfidf_248</th>\n",
       "      <th>tfidf_249</th>\n",
       "      <th>tfidf_250</th>\n",
       "      <th>tfidf_251</th>\n",
       "      <th>tfidf_252</th>\n",
       "      <th>tfidf_253</th>\n",
       "      <th>tfidf_254</th>\n",
       "      <th>tfidf_255</th>\n",
       "      <th>tfidf_256</th>\n",
       "      <th>tfidf_257</th>\n",
       "      <th>tfidf_258</th>\n",
       "      <th>tfidf_259</th>\n",
       "      <th>tfidf_260</th>\n",
       "      <th>tfidf_261</th>\n",
       "      <th>tfidf_262</th>\n",
       "      <th>tfidf_263</th>\n",
       "      <th>tfidf_264</th>\n",
       "      <th>tfidf_265</th>\n",
       "      <th>tfidf_266</th>\n",
       "      <th>tfidf_267</th>\n",
       "      <th>tfidf_268</th>\n",
       "      <th>tfidf_269</th>\n",
       "      <th>tfidf_270</th>\n",
       "      <th>tfidf_271</th>\n",
       "      <th>tfidf_272</th>\n",
       "      <th>tfidf_273</th>\n",
       "      <th>tfidf_274</th>\n",
       "      <th>tfidf_275</th>\n",
       "      <th>tfidf_276</th>\n",
       "      <th>tfidf_277</th>\n",
       "      <th>tfidf_278</th>\n",
       "      <th>tfidf_279</th>\n",
       "      <th>tfidf_280</th>\n",
       "      <th>tfidf_281</th>\n",
       "      <th>tfidf_282</th>\n",
       "      <th>tfidf_283</th>\n",
       "      <th>tfidf_284</th>\n",
       "      <th>tfidf_285</th>\n",
       "      <th>tfidf_286</th>\n",
       "      <th>tfidf_287</th>\n",
       "      <th>tfidf_288</th>\n",
       "      <th>tfidf_289</th>\n",
       "      <th>tfidf_290</th>\n",
       "      <th>tfidf_291</th>\n",
       "      <th>tfidf_292</th>\n",
       "      <th>tfidf_293</th>\n",
       "      <th>tfidf_294</th>\n",
       "      <th>tfidf_295</th>\n",
       "      <th>tfidf_296</th>\n",
       "      <th>tfidf_297</th>\n",
       "      <th>tfidf_298</th>\n",
       "      <th>tfidf_299</th>\n",
       "      <th>tfidf_300</th>\n",
       "      <th>tfidf_301</th>\n",
       "      <th>tfidf_302</th>\n",
       "      <th>tfidf_303</th>\n",
       "      <th>tfidf_304</th>\n",
       "      <th>tfidf_305</th>\n",
       "      <th>tfidf_306</th>\n",
       "      <th>tfidf_307</th>\n",
       "      <th>tfidf_308</th>\n",
       "      <th>tfidf_309</th>\n",
       "      <th>tfidf_310</th>\n",
       "      <th>tfidf_311</th>\n",
       "      <th>tfidf_312</th>\n",
       "      <th>tfidf_313</th>\n",
       "      <th>tfidf_314</th>\n",
       "      <th>tfidf_315</th>\n",
       "      <th>tfidf_316</th>\n",
       "      <th>tfidf_317</th>\n",
       "      <th>tfidf_318</th>\n",
       "      <th>tfidf_319</th>\n",
       "      <th>tfidf_320</th>\n",
       "      <th>tfidf_321</th>\n",
       "      <th>tfidf_322</th>\n",
       "      <th>tfidf_323</th>\n",
       "      <th>tfidf_324</th>\n",
       "      <th>tfidf_325</th>\n",
       "      <th>tfidf_326</th>\n",
       "      <th>tfidf_327</th>\n",
       "      <th>tfidf_328</th>\n",
       "      <th>tfidf_329</th>\n",
       "      <th>tfidf_330</th>\n",
       "      <th>tfidf_331</th>\n",
       "      <th>tfidf_332</th>\n",
       "      <th>tfidf_333</th>\n",
       "      <th>tfidf_334</th>\n",
       "      <th>tfidf_335</th>\n",
       "      <th>tfidf_336</th>\n",
       "      <th>tfidf_337</th>\n",
       "      <th>tfidf_338</th>\n",
       "      <th>tfidf_339</th>\n",
       "      <th>tfidf_340</th>\n",
       "      <th>tfidf_341</th>\n",
       "      <th>tfidf_342</th>\n",
       "      <th>tfidf_343</th>\n",
       "      <th>tfidf_344</th>\n",
       "      <th>tfidf_345</th>\n",
       "      <th>tfidf_346</th>\n",
       "      <th>tfidf_347</th>\n",
       "      <th>tfidf_348</th>\n",
       "      <th>tfidf_349</th>\n",
       "      <th>tfidf_350</th>\n",
       "      <th>tfidf_351</th>\n",
       "      <th>tfidf_352</th>\n",
       "      <th>tfidf_353</th>\n",
       "      <th>tfidf_354</th>\n",
       "      <th>tfidf_355</th>\n",
       "      <th>tfidf_356</th>\n",
       "      <th>tfidf_357</th>\n",
       "      <th>tfidf_358</th>\n",
       "      <th>tfidf_359</th>\n",
       "      <th>tfidf_360</th>\n",
       "      <th>tfidf_361</th>\n",
       "      <th>tfidf_362</th>\n",
       "      <th>tfidf_363</th>\n",
       "      <th>tfidf_364</th>\n",
       "      <th>tfidf_365</th>\n",
       "      <th>tfidf_366</th>\n",
       "      <th>tfidf_367</th>\n",
       "      <th>tfidf_368</th>\n",
       "      <th>tfidf_369</th>\n",
       "      <th>tfidf_370</th>\n",
       "      <th>tfidf_371</th>\n",
       "      <th>tfidf_372</th>\n",
       "      <th>tfidf_373</th>\n",
       "      <th>tfidf_374</th>\n",
       "      <th>tfidf_375</th>\n",
       "      <th>tfidf_376</th>\n",
       "      <th>tfidf_377</th>\n",
       "      <th>tfidf_378</th>\n",
       "      <th>tfidf_379</th>\n",
       "      <th>tfidf_380</th>\n",
       "      <th>tfidf_381</th>\n",
       "      <th>tfidf_382</th>\n",
       "      <th>tfidf_383</th>\n",
       "      <th>tfidf_384</th>\n",
       "      <th>tfidf_385</th>\n",
       "      <th>tfidf_386</th>\n",
       "      <th>tfidf_387</th>\n",
       "      <th>tfidf_388</th>\n",
       "      <th>tfidf_389</th>\n",
       "      <th>tfidf_390</th>\n",
       "      <th>tfidf_391</th>\n",
       "      <th>tfidf_392</th>\n",
       "      <th>tfidf_393</th>\n",
       "      <th>tfidf_394</th>\n",
       "      <th>tfidf_395</th>\n",
       "      <th>tfidf_396</th>\n",
       "      <th>tfidf_397</th>\n",
       "      <th>tfidf_398</th>\n",
       "      <th>tfidf_399</th>\n",
       "      <th>tfidf_400</th>\n",
       "      <th>tfidf_401</th>\n",
       "      <th>tfidf_402</th>\n",
       "      <th>tfidf_403</th>\n",
       "      <th>tfidf_404</th>\n",
       "      <th>tfidf_405</th>\n",
       "      <th>tfidf_406</th>\n",
       "      <th>tfidf_407</th>\n",
       "      <th>tfidf_408</th>\n",
       "      <th>tfidf_409</th>\n",
       "      <th>tfidf_410</th>\n",
       "      <th>tfidf_411</th>\n",
       "      <th>tfidf_412</th>\n",
       "      <th>tfidf_413</th>\n",
       "      <th>tfidf_414</th>\n",
       "      <th>tfidf_415</th>\n",
       "      <th>tfidf_416</th>\n",
       "      <th>tfidf_417</th>\n",
       "      <th>tfidf_418</th>\n",
       "      <th>tfidf_419</th>\n",
       "      <th>tfidf_420</th>\n",
       "      <th>tfidf_421</th>\n",
       "      <th>tfidf_422</th>\n",
       "      <th>tfidf_423</th>\n",
       "      <th>tfidf_424</th>\n",
       "      <th>tfidf_425</th>\n",
       "      <th>tfidf_426</th>\n",
       "      <th>tfidf_427</th>\n",
       "      <th>tfidf_428</th>\n",
       "      <th>tfidf_429</th>\n",
       "      <th>tfidf_430</th>\n",
       "      <th>tfidf_431</th>\n",
       "      <th>tfidf_432</th>\n",
       "      <th>tfidf_433</th>\n",
       "      <th>tfidf_434</th>\n",
       "      <th>tfidf_435</th>\n",
       "      <th>tfidf_436</th>\n",
       "      <th>tfidf_437</th>\n",
       "      <th>tfidf_438</th>\n",
       "      <th>tfidf_439</th>\n",
       "      <th>tfidf_440</th>\n",
       "      <th>tfidf_441</th>\n",
       "      <th>tfidf_442</th>\n",
       "      <th>tfidf_443</th>\n",
       "      <th>tfidf_444</th>\n",
       "      <th>tfidf_445</th>\n",
       "      <th>tfidf_446</th>\n",
       "      <th>tfidf_447</th>\n",
       "      <th>tfidf_448</th>\n",
       "      <th>tfidf_449</th>\n",
       "      <th>tfidf_450</th>\n",
       "      <th>tfidf_451</th>\n",
       "      <th>tfidf_452</th>\n",
       "      <th>tfidf_453</th>\n",
       "      <th>tfidf_454</th>\n",
       "      <th>tfidf_455</th>\n",
       "      <th>tfidf_456</th>\n",
       "      <th>tfidf_457</th>\n",
       "      <th>tfidf_458</th>\n",
       "      <th>tfidf_459</th>\n",
       "      <th>tfidf_460</th>\n",
       "      <th>tfidf_461</th>\n",
       "      <th>tfidf_462</th>\n",
       "      <th>tfidf_463</th>\n",
       "      <th>tfidf_464</th>\n",
       "      <th>tfidf_465</th>\n",
       "      <th>tfidf_466</th>\n",
       "      <th>tfidf_467</th>\n",
       "      <th>tfidf_468</th>\n",
       "      <th>tfidf_469</th>\n",
       "      <th>tfidf_470</th>\n",
       "      <th>tfidf_471</th>\n",
       "      <th>tfidf_472</th>\n",
       "      <th>tfidf_473</th>\n",
       "      <th>tfidf_474</th>\n",
       "      <th>tfidf_475</th>\n",
       "      <th>tfidf_476</th>\n",
       "      <th>tfidf_477</th>\n",
       "      <th>tfidf_478</th>\n",
       "      <th>tfidf_479</th>\n",
       "      <th>tfidf_480</th>\n",
       "      <th>tfidf_481</th>\n",
       "      <th>tfidf_482</th>\n",
       "      <th>tfidf_483</th>\n",
       "      <th>tfidf_484</th>\n",
       "      <th>tfidf_485</th>\n",
       "      <th>tfidf_486</th>\n",
       "      <th>tfidf_487</th>\n",
       "      <th>tfidf_488</th>\n",
       "      <th>tfidf_489</th>\n",
       "      <th>tfidf_490</th>\n",
       "      <th>tfidf_491</th>\n",
       "      <th>tfidf_492</th>\n",
       "      <th>tfidf_493</th>\n",
       "      <th>tfidf_494</th>\n",
       "      <th>tfidf_495</th>\n",
       "      <th>tfidf_496</th>\n",
       "      <th>tfidf_497</th>\n",
       "      <th>tfidf_498</th>\n",
       "      <th>tfidf_499</th>\n",
       "      <th>tfidf_500</th>\n",
       "      <th>tfidf_501</th>\n",
       "      <th>tfidf_502</th>\n",
       "      <th>tfidf_503</th>\n",
       "      <th>tfidf_504</th>\n",
       "      <th>tfidf_505</th>\n",
       "      <th>tfidf_506</th>\n",
       "      <th>tfidf_507</th>\n",
       "      <th>tfidf_508</th>\n",
       "      <th>tfidf_509</th>\n",
       "      <th>tfidf_510</th>\n",
       "      <th>tfidf_511</th>\n",
       "      <th>tfidf_512</th>\n",
       "      <th>tfidf_513</th>\n",
       "      <th>tfidf_514</th>\n",
       "      <th>tfidf_515</th>\n",
       "      <th>tfidf_516</th>\n",
       "      <th>tfidf_517</th>\n",
       "      <th>tfidf_518</th>\n",
       "      <th>tfidf_519</th>\n",
       "      <th>tfidf_520</th>\n",
       "      <th>tfidf_521</th>\n",
       "      <th>tfidf_522</th>\n",
       "      <th>tfidf_523</th>\n",
       "      <th>tfidf_524</th>\n",
       "      <th>tfidf_525</th>\n",
       "      <th>tfidf_526</th>\n",
       "      <th>tfidf_527</th>\n",
       "      <th>tfidf_528</th>\n",
       "      <th>tfidf_529</th>\n",
       "      <th>tfidf_530</th>\n",
       "      <th>tfidf_531</th>\n",
       "      <th>tfidf_532</th>\n",
       "      <th>tfidf_533</th>\n",
       "      <th>tfidf_534</th>\n",
       "      <th>tfidf_535</th>\n",
       "      <th>tfidf_536</th>\n",
       "      <th>tfidf_537</th>\n",
       "      <th>tfidf_538</th>\n",
       "      <th>tfidf_539</th>\n",
       "      <th>tfidf_540</th>\n",
       "      <th>tfidf_541</th>\n",
       "      <th>tfidf_542</th>\n",
       "      <th>tfidf_543</th>\n",
       "      <th>tfidf_544</th>\n",
       "      <th>tfidf_545</th>\n",
       "      <th>tfidf_546</th>\n",
       "      <th>tfidf_547</th>\n",
       "      <th>tfidf_548</th>\n",
       "      <th>tfidf_549</th>\n",
       "      <th>tfidf_550</th>\n",
       "      <th>tfidf_551</th>\n",
       "      <th>tfidf_552</th>\n",
       "      <th>tfidf_553</th>\n",
       "      <th>tfidf_554</th>\n",
       "      <th>tfidf_555</th>\n",
       "      <th>tfidf_556</th>\n",
       "      <th>tfidf_557</th>\n",
       "      <th>tfidf_558</th>\n",
       "      <th>tfidf_559</th>\n",
       "      <th>tfidf_560</th>\n",
       "      <th>tfidf_561</th>\n",
       "      <th>tfidf_562</th>\n",
       "      <th>tfidf_563</th>\n",
       "      <th>tfidf_564</th>\n",
       "      <th>tfidf_565</th>\n",
       "      <th>tfidf_566</th>\n",
       "      <th>tfidf_567</th>\n",
       "      <th>tfidf_568</th>\n",
       "      <th>tfidf_569</th>\n",
       "      <th>tfidf_570</th>\n",
       "      <th>tfidf_571</th>\n",
       "      <th>tfidf_572</th>\n",
       "      <th>tfidf_573</th>\n",
       "      <th>tfidf_574</th>\n",
       "      <th>tfidf_575</th>\n",
       "      <th>tfidf_576</th>\n",
       "      <th>tfidf_577</th>\n",
       "      <th>tfidf_578</th>\n",
       "      <th>tfidf_579</th>\n",
       "      <th>tfidf_580</th>\n",
       "      <th>tfidf_581</th>\n",
       "      <th>tfidf_582</th>\n",
       "      <th>tfidf_583</th>\n",
       "      <th>tfidf_584</th>\n",
       "      <th>tfidf_585</th>\n",
       "      <th>tfidf_586</th>\n",
       "      <th>tfidf_587</th>\n",
       "      <th>tfidf_588</th>\n",
       "      <th>tfidf_589</th>\n",
       "      <th>tfidf_590</th>\n",
       "      <th>tfidf_591</th>\n",
       "      <th>tfidf_592</th>\n",
       "      <th>tfidf_593</th>\n",
       "      <th>tfidf_594</th>\n",
       "      <th>tfidf_595</th>\n",
       "      <th>tfidf_596</th>\n",
       "      <th>tfidf_597</th>\n",
       "      <th>tfidf_598</th>\n",
       "      <th>tfidf_599</th>\n",
       "      <th>tfidf_600</th>\n",
       "      <th>tfidf_601</th>\n",
       "      <th>tfidf_602</th>\n",
       "      <th>tfidf_603</th>\n",
       "      <th>tfidf_604</th>\n",
       "      <th>tfidf_605</th>\n",
       "      <th>tfidf_606</th>\n",
       "      <th>tfidf_607</th>\n",
       "      <th>tfidf_608</th>\n",
       "      <th>tfidf_609</th>\n",
       "      <th>tfidf_610</th>\n",
       "      <th>tfidf_611</th>\n",
       "      <th>tfidf_612</th>\n",
       "      <th>tfidf_613</th>\n",
       "      <th>tfidf_614</th>\n",
       "      <th>tfidf_615</th>\n",
       "      <th>tfidf_616</th>\n",
       "      <th>tfidf_617</th>\n",
       "      <th>tfidf_618</th>\n",
       "      <th>tfidf_619</th>\n",
       "      <th>tfidf_620</th>\n",
       "      <th>tfidf_621</th>\n",
       "      <th>tfidf_622</th>\n",
       "      <th>tfidf_623</th>\n",
       "      <th>tfidf_624</th>\n",
       "      <th>tfidf_625</th>\n",
       "      <th>tfidf_626</th>\n",
       "      <th>tfidf_627</th>\n",
       "      <th>tfidf_628</th>\n",
       "      <th>tfidf_629</th>\n",
       "      <th>tfidf_630</th>\n",
       "      <th>tfidf_631</th>\n",
       "      <th>tfidf_632</th>\n",
       "      <th>tfidf_633</th>\n",
       "      <th>tfidf_634</th>\n",
       "      <th>tfidf_635</th>\n",
       "      <th>tfidf_636</th>\n",
       "      <th>tfidf_637</th>\n",
       "      <th>tfidf_638</th>\n",
       "      <th>tfidf_639</th>\n",
       "      <th>tfidf_640</th>\n",
       "      <th>tfidf_641</th>\n",
       "      <th>tfidf_642</th>\n",
       "      <th>tfidf_643</th>\n",
       "      <th>tfidf_644</th>\n",
       "      <th>tfidf_645</th>\n",
       "      <th>tfidf_646</th>\n",
       "      <th>tfidf_647</th>\n",
       "      <th>tfidf_648</th>\n",
       "      <th>tfidf_649</th>\n",
       "      <th>tfidf_650</th>\n",
       "      <th>tfidf_651</th>\n",
       "      <th>tfidf_652</th>\n",
       "      <th>tfidf_653</th>\n",
       "      <th>tfidf_654</th>\n",
       "      <th>tfidf_655</th>\n",
       "      <th>tfidf_656</th>\n",
       "      <th>tfidf_657</th>\n",
       "      <th>tfidf_658</th>\n",
       "      <th>tfidf_659</th>\n",
       "      <th>tfidf_660</th>\n",
       "      <th>tfidf_661</th>\n",
       "      <th>tfidf_662</th>\n",
       "      <th>tfidf_663</th>\n",
       "      <th>tfidf_664</th>\n",
       "      <th>tfidf_665</th>\n",
       "      <th>tfidf_666</th>\n",
       "      <th>tfidf_667</th>\n",
       "      <th>tfidf_668</th>\n",
       "      <th>tfidf_669</th>\n",
       "      <th>tfidf_670</th>\n",
       "      <th>tfidf_671</th>\n",
       "      <th>tfidf_672</th>\n",
       "      <th>tfidf_673</th>\n",
       "      <th>tfidf_674</th>\n",
       "      <th>tfidf_675</th>\n",
       "      <th>tfidf_676</th>\n",
       "      <th>tfidf_677</th>\n",
       "      <th>tfidf_678</th>\n",
       "      <th>tfidf_679</th>\n",
       "      <th>tfidf_680</th>\n",
       "      <th>tfidf_681</th>\n",
       "      <th>tfidf_682</th>\n",
       "      <th>tfidf_683</th>\n",
       "      <th>tfidf_684</th>\n",
       "      <th>tfidf_685</th>\n",
       "      <th>tfidf_686</th>\n",
       "      <th>tfidf_687</th>\n",
       "      <th>tfidf_688</th>\n",
       "      <th>tfidf_689</th>\n",
       "      <th>tfidf_690</th>\n",
       "      <th>tfidf_691</th>\n",
       "      <th>tfidf_692</th>\n",
       "      <th>tfidf_693</th>\n",
       "      <th>tfidf_694</th>\n",
       "      <th>tfidf_695</th>\n",
       "      <th>tfidf_696</th>\n",
       "      <th>tfidf_697</th>\n",
       "      <th>tfidf_698</th>\n",
       "      <th>tfidf_699</th>\n",
       "      <th>tfidf_700</th>\n",
       "      <th>tfidf_701</th>\n",
       "      <th>tfidf_702</th>\n",
       "      <th>tfidf_703</th>\n",
       "      <th>tfidf_704</th>\n",
       "      <th>tfidf_705</th>\n",
       "      <th>tfidf_706</th>\n",
       "      <th>tfidf_707</th>\n",
       "      <th>tfidf_708</th>\n",
       "      <th>tfidf_709</th>\n",
       "      <th>tfidf_710</th>\n",
       "      <th>tfidf_711</th>\n",
       "      <th>tfidf_712</th>\n",
       "      <th>tfidf_713</th>\n",
       "      <th>tfidf_714</th>\n",
       "      <th>tfidf_715</th>\n",
       "      <th>tfidf_716</th>\n",
       "      <th>tfidf_717</th>\n",
       "      <th>tfidf_718</th>\n",
       "      <th>tfidf_719</th>\n",
       "      <th>tfidf_720</th>\n",
       "      <th>tfidf_721</th>\n",
       "      <th>tfidf_722</th>\n",
       "      <th>tfidf_723</th>\n",
       "      <th>tfidf_724</th>\n",
       "      <th>tfidf_725</th>\n",
       "      <th>tfidf_726</th>\n",
       "      <th>tfidf_727</th>\n",
       "      <th>tfidf_728</th>\n",
       "      <th>tfidf_729</th>\n",
       "      <th>tfidf_730</th>\n",
       "      <th>tfidf_731</th>\n",
       "      <th>tfidf_732</th>\n",
       "      <th>tfidf_733</th>\n",
       "      <th>tfidf_734</th>\n",
       "      <th>tfidf_735</th>\n",
       "      <th>tfidf_736</th>\n",
       "      <th>tfidf_737</th>\n",
       "      <th>tfidf_738</th>\n",
       "      <th>tfidf_739</th>\n",
       "      <th>tfidf_740</th>\n",
       "      <th>tfidf_741</th>\n",
       "      <th>tfidf_742</th>\n",
       "      <th>tfidf_743</th>\n",
       "      <th>tfidf_744</th>\n",
       "      <th>tfidf_745</th>\n",
       "      <th>tfidf_746</th>\n",
       "      <th>tfidf_747</th>\n",
       "      <th>tfidf_748</th>\n",
       "      <th>tfidf_749</th>\n",
       "      <th>tfidf_750</th>\n",
       "      <th>tfidf_751</th>\n",
       "      <th>tfidf_752</th>\n",
       "      <th>tfidf_753</th>\n",
       "      <th>tfidf_754</th>\n",
       "      <th>tfidf_755</th>\n",
       "      <th>tfidf_756</th>\n",
       "      <th>tfidf_757</th>\n",
       "      <th>tfidf_758</th>\n",
       "      <th>tfidf_759</th>\n",
       "      <th>tfidf_760</th>\n",
       "      <th>tfidf_761</th>\n",
       "      <th>tfidf_762</th>\n",
       "      <th>tfidf_763</th>\n",
       "      <th>tfidf_764</th>\n",
       "      <th>tfidf_765</th>\n",
       "      <th>tfidf_766</th>\n",
       "      <th>tfidf_767</th>\n",
       "      <th>tfidf_768</th>\n",
       "      <th>tfidf_769</th>\n",
       "      <th>tfidf_770</th>\n",
       "      <th>tfidf_771</th>\n",
       "      <th>tfidf_772</th>\n",
       "      <th>tfidf_773</th>\n",
       "      <th>tfidf_774</th>\n",
       "      <th>tfidf_775</th>\n",
       "      <th>tfidf_776</th>\n",
       "      <th>tfidf_777</th>\n",
       "      <th>tfidf_778</th>\n",
       "      <th>tfidf_779</th>\n",
       "      <th>tfidf_780</th>\n",
       "      <th>tfidf_781</th>\n",
       "      <th>tfidf_782</th>\n",
       "      <th>tfidf_783</th>\n",
       "      <th>tfidf_784</th>\n",
       "      <th>tfidf_785</th>\n",
       "      <th>tfidf_786</th>\n",
       "      <th>tfidf_787</th>\n",
       "      <th>tfidf_788</th>\n",
       "      <th>tfidf_789</th>\n",
       "      <th>tfidf_790</th>\n",
       "      <th>tfidf_791</th>\n",
       "      <th>tfidf_792</th>\n",
       "      <th>tfidf_793</th>\n",
       "      <th>tfidf_794</th>\n",
       "      <th>tfidf_795</th>\n",
       "      <th>tfidf_796</th>\n",
       "      <th>tfidf_797</th>\n",
       "      <th>tfidf_798</th>\n",
       "      <th>tfidf_799</th>\n",
       "      <th>tfidf_800</th>\n",
       "      <th>tfidf_801</th>\n",
       "      <th>tfidf_802</th>\n",
       "      <th>tfidf_803</th>\n",
       "      <th>tfidf_804</th>\n",
       "      <th>tfidf_805</th>\n",
       "      <th>tfidf_806</th>\n",
       "      <th>tfidf_807</th>\n",
       "      <th>tfidf_808</th>\n",
       "      <th>tfidf_809</th>\n",
       "      <th>tfidf_810</th>\n",
       "      <th>tfidf_811</th>\n",
       "      <th>tfidf_812</th>\n",
       "      <th>tfidf_813</th>\n",
       "      <th>tfidf_814</th>\n",
       "      <th>tfidf_815</th>\n",
       "      <th>tfidf_816</th>\n",
       "      <th>tfidf_817</th>\n",
       "      <th>tfidf_818</th>\n",
       "      <th>tfidf_819</th>\n",
       "      <th>tfidf_820</th>\n",
       "      <th>tfidf_821</th>\n",
       "      <th>tfidf_822</th>\n",
       "      <th>tfidf_823</th>\n",
       "      <th>tfidf_824</th>\n",
       "      <th>tfidf_825</th>\n",
       "      <th>tfidf_826</th>\n",
       "      <th>tfidf_827</th>\n",
       "      <th>tfidf_828</th>\n",
       "      <th>tfidf_829</th>\n",
       "      <th>tfidf_830</th>\n",
       "      <th>tfidf_831</th>\n",
       "      <th>tfidf_832</th>\n",
       "      <th>tfidf_833</th>\n",
       "      <th>tfidf_834</th>\n",
       "      <th>tfidf_835</th>\n",
       "      <th>tfidf_836</th>\n",
       "      <th>tfidf_837</th>\n",
       "      <th>tfidf_838</th>\n",
       "      <th>tfidf_839</th>\n",
       "      <th>tfidf_840</th>\n",
       "      <th>tfidf_841</th>\n",
       "      <th>tfidf_842</th>\n",
       "      <th>tfidf_843</th>\n",
       "      <th>tfidf_844</th>\n",
       "      <th>tfidf_845</th>\n",
       "      <th>tfidf_846</th>\n",
       "      <th>tfidf_847</th>\n",
       "      <th>tfidf_848</th>\n",
       "      <th>tfidf_849</th>\n",
       "      <th>tfidf_850</th>\n",
       "      <th>tfidf_851</th>\n",
       "      <th>tfidf_852</th>\n",
       "      <th>tfidf_853</th>\n",
       "      <th>tfidf_854</th>\n",
       "      <th>tfidf_855</th>\n",
       "      <th>tfidf_856</th>\n",
       "      <th>tfidf_857</th>\n",
       "      <th>tfidf_858</th>\n",
       "      <th>tfidf_859</th>\n",
       "      <th>tfidf_860</th>\n",
       "      <th>tfidf_861</th>\n",
       "      <th>tfidf_862</th>\n",
       "      <th>tfidf_863</th>\n",
       "      <th>tfidf_864</th>\n",
       "      <th>tfidf_865</th>\n",
       "      <th>tfidf_866</th>\n",
       "      <th>tfidf_867</th>\n",
       "      <th>tfidf_868</th>\n",
       "      <th>tfidf_869</th>\n",
       "      <th>tfidf_870</th>\n",
       "      <th>tfidf_871</th>\n",
       "      <th>tfidf_872</th>\n",
       "      <th>tfidf_873</th>\n",
       "      <th>tfidf_874</th>\n",
       "      <th>tfidf_875</th>\n",
       "      <th>tfidf_876</th>\n",
       "      <th>tfidf_877</th>\n",
       "      <th>tfidf_878</th>\n",
       "      <th>tfidf_879</th>\n",
       "      <th>tfidf_880</th>\n",
       "      <th>tfidf_881</th>\n",
       "      <th>tfidf_882</th>\n",
       "      <th>tfidf_883</th>\n",
       "      <th>tfidf_884</th>\n",
       "      <th>tfidf_885</th>\n",
       "      <th>tfidf_886</th>\n",
       "      <th>tfidf_887</th>\n",
       "      <th>tfidf_888</th>\n",
       "      <th>tfidf_889</th>\n",
       "      <th>tfidf_890</th>\n",
       "      <th>tfidf_891</th>\n",
       "      <th>tfidf_892</th>\n",
       "      <th>tfidf_893</th>\n",
       "      <th>tfidf_894</th>\n",
       "      <th>tfidf_895</th>\n",
       "      <th>tfidf_896</th>\n",
       "      <th>tfidf_897</th>\n",
       "      <th>tfidf_898</th>\n",
       "      <th>tfidf_899</th>\n",
       "      <th>tfidf_900</th>\n",
       "      <th>tfidf_901</th>\n",
       "      <th>tfidf_902</th>\n",
       "      <th>tfidf_903</th>\n",
       "      <th>tfidf_904</th>\n",
       "      <th>tfidf_905</th>\n",
       "      <th>tfidf_906</th>\n",
       "      <th>tfidf_907</th>\n",
       "      <th>tfidf_908</th>\n",
       "      <th>tfidf_909</th>\n",
       "      <th>tfidf_910</th>\n",
       "      <th>tfidf_911</th>\n",
       "      <th>tfidf_912</th>\n",
       "      <th>tfidf_913</th>\n",
       "      <th>tfidf_914</th>\n",
       "      <th>tfidf_915</th>\n",
       "      <th>tfidf_916</th>\n",
       "      <th>tfidf_917</th>\n",
       "      <th>tfidf_918</th>\n",
       "      <th>tfidf_919</th>\n",
       "      <th>tfidf_920</th>\n",
       "      <th>tfidf_921</th>\n",
       "      <th>tfidf_922</th>\n",
       "      <th>tfidf_923</th>\n",
       "      <th>tfidf_924</th>\n",
       "      <th>tfidf_925</th>\n",
       "      <th>tfidf_926</th>\n",
       "      <th>tfidf_927</th>\n",
       "      <th>tfidf_928</th>\n",
       "      <th>tfidf_929</th>\n",
       "      <th>tfidf_930</th>\n",
       "      <th>tfidf_931</th>\n",
       "      <th>tfidf_932</th>\n",
       "      <th>tfidf_933</th>\n",
       "      <th>tfidf_934</th>\n",
       "      <th>tfidf_935</th>\n",
       "      <th>tfidf_936</th>\n",
       "      <th>tfidf_937</th>\n",
       "      <th>tfidf_938</th>\n",
       "      <th>tfidf_939</th>\n",
       "      <th>tfidf_940</th>\n",
       "      <th>tfidf_941</th>\n",
       "      <th>tfidf_942</th>\n",
       "      <th>tfidf_943</th>\n",
       "      <th>tfidf_944</th>\n",
       "      <th>tfidf_945</th>\n",
       "      <th>tfidf_946</th>\n",
       "      <th>tfidf_947</th>\n",
       "      <th>tfidf_948</th>\n",
       "      <th>tfidf_949</th>\n",
       "      <th>tfidf_950</th>\n",
       "      <th>tfidf_951</th>\n",
       "      <th>tfidf_952</th>\n",
       "      <th>tfidf_953</th>\n",
       "      <th>tfidf_954</th>\n",
       "      <th>tfidf_955</th>\n",
       "      <th>tfidf_956</th>\n",
       "      <th>tfidf_957</th>\n",
       "      <th>tfidf_958</th>\n",
       "      <th>tfidf_959</th>\n",
       "      <th>tfidf_960</th>\n",
       "      <th>tfidf_961</th>\n",
       "      <th>tfidf_962</th>\n",
       "      <th>tfidf_963</th>\n",
       "      <th>tfidf_964</th>\n",
       "      <th>tfidf_965</th>\n",
       "      <th>tfidf_966</th>\n",
       "      <th>tfidf_967</th>\n",
       "      <th>tfidf_968</th>\n",
       "      <th>tfidf_969</th>\n",
       "      <th>tfidf_970</th>\n",
       "      <th>tfidf_971</th>\n",
       "      <th>tfidf_972</th>\n",
       "      <th>tfidf_973</th>\n",
       "      <th>tfidf_974</th>\n",
       "      <th>tfidf_975</th>\n",
       "      <th>tfidf_976</th>\n",
       "      <th>tfidf_977</th>\n",
       "      <th>tfidf_978</th>\n",
       "      <th>tfidf_979</th>\n",
       "      <th>tfidf_980</th>\n",
       "      <th>tfidf_981</th>\n",
       "      <th>tfidf_982</th>\n",
       "      <th>tfidf_983</th>\n",
       "      <th>tfidf_984</th>\n",
       "      <th>tfidf_985</th>\n",
       "      <th>tfidf_986</th>\n",
       "      <th>tfidf_987</th>\n",
       "      <th>tfidf_988</th>\n",
       "      <th>tfidf_989</th>\n",
       "      <th>tfidf_990</th>\n",
       "      <th>tfidf_991</th>\n",
       "      <th>tfidf_992</th>\n",
       "      <th>tfidf_993</th>\n",
       "      <th>tfidf_994</th>\n",
       "      <th>tfidf_995</th>\n",
       "      <th>tfidf_996</th>\n",
       "      <th>tfidf_997</th>\n",
       "      <th>tfidf_998</th>\n",
       "      <th>tfidf_999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.29178</td>\n",
       "      <td>-6.25792</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>4.82</td>\n",
       "      <td>0.897737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013425</td>\n",
       "      <td>0.005049</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>0.003457</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.002238</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>0.004704</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>0.002912</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.004872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.006517</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>0.003072</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.016402</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.010011</td>\n",
       "      <td>0.016258</td>\n",
       "      <td>0.003165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004420</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>0.022071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003657</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.011882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.004814</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>0.010814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006603</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014734</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.007383</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009956</td>\n",
       "      <td>0.004094</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.013329</td>\n",
       "      <td>0.012432</td>\n",
       "      <td>0.008753</td>\n",
       "      <td>0.003837</td>\n",
       "      <td>0.003063</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.006932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.002167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>0.060857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022948</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028956</td>\n",
       "      <td>0.003746</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.005262</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004062</td>\n",
       "      <td>0.003247</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.006759</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001555</td>\n",
       "      <td>0.013455</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.011238</td>\n",
       "      <td>0.004067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.002081</td>\n",
       "      <td>0.030837</td>\n",
       "      <td>0.023336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.012383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003739</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.007625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033270</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004897</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.003758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.008844</td>\n",
       "      <td>0.003269</td>\n",
       "      <td>0.002980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006023</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>0.018784</td>\n",
       "      <td>0.050836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007505</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>0.006565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015276</td>\n",
       "      <td>0.002588</td>\n",
       "      <td>0.004099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015368</td>\n",
       "      <td>0.014190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013218</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007579</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>0.004326</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009057</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.004383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.002661</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045739</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>0.014455</td>\n",
       "      <td>0.002066</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022449</td>\n",
       "      <td>0.008465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.010275</td>\n",
       "      <td>0.003782</td>\n",
       "      <td>0.007187</td>\n",
       "      <td>0.014550</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>0.018861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007798</td>\n",
       "      <td>0.009384</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>0.004488</td>\n",
       "      <td>0.006777</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>0.015330</td>\n",
       "      <td>0.004805</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.002423</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.008980</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024878</td>\n",
       "      <td>0.006956</td>\n",
       "      <td>0.006308</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020928</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.002823</td>\n",
       "      <td>0.013584</td>\n",
       "      <td>0.007579</td>\n",
       "      <td>0.003171</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012472</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>0.027198</td>\n",
       "      <td>0.002826</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.003727</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014780</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014602</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.019796</td>\n",
       "      <td>0.013531</td>\n",
       "      <td>0.004797</td>\n",
       "      <td>0.004706</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.016097</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.007084</td>\n",
       "      <td>0.024283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>0.007553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.039577</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>0.011937</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002699</td>\n",
       "      <td>0.002875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007170</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.003059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016530</td>\n",
       "      <td>0.012187</td>\n",
       "      <td>0.041349</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>0.003625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002270</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.018022</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.034321</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005008</td>\n",
       "      <td>0.006044</td>\n",
       "      <td>0.007928</td>\n",
       "      <td>0.019031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005269</td>\n",
       "      <td>0.003707</td>\n",
       "      <td>0.048370</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.038059</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.057165</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.000927</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.003410</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>0.005379</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>0.015707</td>\n",
       "      <td>0.004448</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013329</td>\n",
       "      <td>0.015892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.005677</td>\n",
       "      <td>0.004037</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>0.002375</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003264</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005194</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.012091</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006958</td>\n",
       "      <td>0.003197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010212</td>\n",
       "      <td>0.008217</td>\n",
       "      <td>0.014524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004059</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.007971</td>\n",
       "      <td>0.007994</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.008607</td>\n",
       "      <td>0.005935</td>\n",
       "      <td>0.049631</td>\n",
       "      <td>0.013614</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003280</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.008311</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002141</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>0.006526</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.002905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002453</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.006251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008658</td>\n",
       "      <td>0.013068</td>\n",
       "      <td>0.001557</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.002775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004888</td>\n",
       "      <td>0.013279</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006504</td>\n",
       "      <td>0.006284</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.003755</td>\n",
       "      <td>0.009764</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.017679</td>\n",
       "      <td>0.006740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006857</td>\n",
       "      <td>0.003079</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033932</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.004823</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>0.005026</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003217</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.002466</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.008474</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002298</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004855</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.020704</td>\n",
       "      <td>0.016437</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>0.003912</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033349</td>\n",
       "      <td>0.016669</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>0.004099</td>\n",
       "      <td>0.007821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.003375</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005351</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>0.003577</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.003719</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>0.002876</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.004675</td>\n",
       "      <td>0.005335</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.016694</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.030057</td>\n",
       "      <td>0.003647</td>\n",
       "      <td>0.008707</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006140</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.007170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.013278</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010054</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002871</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.003225</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.015696</td>\n",
       "      <td>0.006073</td>\n",
       "      <td>0.005233</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.005806</td>\n",
       "      <td>0.004432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.004927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>0.006845</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003353</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.00190</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>0.005425</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.002319</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.003440</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002511</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004821</td>\n",
       "      <td>0.005322</td>\n",
       "      <td>0.064112</td>\n",
       "      <td>0.012349</td>\n",
       "      <td>0.017277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.015273</td>\n",
       "      <td>0.001463</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001099</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005399</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.006593</td>\n",
       "      <td>0.003987</td>\n",
       "      <td>0.001791</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.291092</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.006105</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.014907</td>\n",
       "      <td>0.012240</td>\n",
       "      <td>0.001894</td>\n",
       "      <td>0.007907</td>\n",
       "      <td>0.001463</td>\n",
       "      <td>0.004927</td>\n",
       "      <td>0.004748</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>0.002571</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.005818</td>\n",
       "      <td>0.023205</td>\n",
       "      <td>0.001266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005441</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.002530</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003817</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.009277</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.004520</td>\n",
       "      <td>0.025234</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.011556</td>\n",
       "      <td>0.013497</td>\n",
       "      <td>0.008879</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.002890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.016679</td>\n",
       "      <td>0.003881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061635</td>\n",
       "      <td>0.004123</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.007920</td>\n",
       "      <td>0.008252</td>\n",
       "      <td>0.005780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009351</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>0.023716</td>\n",
       "      <td>0.005212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>0.014946</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.013462</td>\n",
       "      <td>0.009104</td>\n",
       "      <td>0.036078</td>\n",
       "      <td>0.015594</td>\n",
       "      <td>0.005851</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>0.004692</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>0.032090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005281</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.001360</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.034269</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>85156</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.29209</td>\n",
       "      <td>-6.25624</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>4.78</td>\n",
       "      <td>0.909530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009196</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011966</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.002680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>0.014494</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.006672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.008810</td>\n",
       "      <td>0.005144</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.004843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.001766</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.004161</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004914</td>\n",
       "      <td>0.004611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.024434</td>\n",
       "      <td>0.002868</td>\n",
       "      <td>0.010553</td>\n",
       "      <td>0.015432</td>\n",
       "      <td>0.003694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>0.003894</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017514</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.014192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.006111</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.006205</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013399</td>\n",
       "      <td>0.006057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009199</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018644</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.018556</td>\n",
       "      <td>0.003093</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.002702</td>\n",
       "      <td>0.006741</td>\n",
       "      <td>0.008545</td>\n",
       "      <td>0.003851</td>\n",
       "      <td>0.005404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.003075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.004201</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025745</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.003395</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.004768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.004257</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018899</td>\n",
       "      <td>0.002762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018038</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>0.005548</td>\n",
       "      <td>0.030645</td>\n",
       "      <td>0.019623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.011950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.002737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007658</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.029543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010136</td>\n",
       "      <td>0.002678</td>\n",
       "      <td>0.002447</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.002095</td>\n",
       "      <td>0.002120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.008278</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012425</td>\n",
       "      <td>0.051303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.011622</td>\n",
       "      <td>0.002569</td>\n",
       "      <td>0.007013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.004302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008938</td>\n",
       "      <td>0.008556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>0.005551</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004872</td>\n",
       "      <td>0.002775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>0.005582</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049229</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008450</td>\n",
       "      <td>0.021379</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002761</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.008990</td>\n",
       "      <td>0.003543</td>\n",
       "      <td>0.019376</td>\n",
       "      <td>0.004998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006644</td>\n",
       "      <td>0.009688</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>0.010734</td>\n",
       "      <td>0.005519</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.019966</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>0.002230</td>\n",
       "      <td>0.007872</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.008023</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.005186</td>\n",
       "      <td>0.001915</td>\n",
       "      <td>0.019440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003053</td>\n",
       "      <td>0.004875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017043</td>\n",
       "      <td>0.004957</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>0.013540</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012314</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003751</td>\n",
       "      <td>0.002706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>0.029735</td>\n",
       "      <td>0.003531</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007103</td>\n",
       "      <td>0.021087</td>\n",
       "      <td>0.009630</td>\n",
       "      <td>0.006723</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.003111</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>0.018303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006393</td>\n",
       "      <td>0.036245</td>\n",
       "      <td>0.003026</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.010150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>0.004628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015178</td>\n",
       "      <td>0.006727</td>\n",
       "      <td>0.032526</td>\n",
       "      <td>0.003503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022932</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>0.054977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004465</td>\n",
       "      <td>0.009016</td>\n",
       "      <td>0.009865</td>\n",
       "      <td>0.013540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060536</td>\n",
       "      <td>0.004415</td>\n",
       "      <td>0.003259</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042990</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.003474</td>\n",
       "      <td>0.061654</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004195</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.004957</td>\n",
       "      <td>0.004043</td>\n",
       "      <td>0.010654</td>\n",
       "      <td>0.009180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.021090</td>\n",
       "      <td>0.007814</td>\n",
       "      <td>0.003461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022496</td>\n",
       "      <td>0.004789</td>\n",
       "      <td>0.007308</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>0.005190</td>\n",
       "      <td>0.003481</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.006767</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003468</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.012745</td>\n",
       "      <td>0.002461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012807</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012985</td>\n",
       "      <td>0.007251</td>\n",
       "      <td>0.018036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.002403</td>\n",
       "      <td>0.009342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004720</td>\n",
       "      <td>0.004513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>0.014440</td>\n",
       "      <td>0.070858</td>\n",
       "      <td>0.015088</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012521</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.007350</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008577</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>0.009126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>0.006228</td>\n",
       "      <td>0.007355</td>\n",
       "      <td>0.003692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013498</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009561</td>\n",
       "      <td>0.005705</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.004454</td>\n",
       "      <td>0.015773</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.012508</td>\n",
       "      <td>0.005125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.004816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.008930</td>\n",
       "      <td>0.003924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005233</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033228</td>\n",
       "      <td>0.011650</td>\n",
       "      <td>0.004521</td>\n",
       "      <td>0.005443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034553</td>\n",
       "      <td>0.008325</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>0.011425</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.005766</td>\n",
       "      <td>0.003544</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.002385</td>\n",
       "      <td>0.002657</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.001590</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002687</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>0.014221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>0.014994</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.015675</td>\n",
       "      <td>0.008857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003907</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.028585</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025960</td>\n",
       "      <td>0.009175</td>\n",
       "      <td>0.007191</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>0.004766</td>\n",
       "      <td>0.005619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002327</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.004605</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.002931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005160</td>\n",
       "      <td>0.007848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022774</td>\n",
       "      <td>0.009339</td>\n",
       "      <td>0.001194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002874</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002948</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001651</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.015407</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.007016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002026</td>\n",
       "      <td>0.002569</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>0.004202</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.004129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.001780</td>\n",
       "      <td>0.007803</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.004974</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.003569</td>\n",
       "      <td>0.004998</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006392</td>\n",
       "      <td>0.005019</td>\n",
       "      <td>0.056329</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>0.030676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.007257</td>\n",
       "      <td>0.004371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.004781</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.006493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>0.010521</td>\n",
       "      <td>0.004624</td>\n",
       "      <td>0.001995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008224</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005872</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.009768</td>\n",
       "      <td>0.003398</td>\n",
       "      <td>0.003739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.009666</td>\n",
       "      <td>0.011557</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.008705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.006478</td>\n",
       "      <td>0.004931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010803</td>\n",
       "      <td>0.024796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.010826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.003969</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007825</td>\n",
       "      <td>0.024361</td>\n",
       "      <td>0.005520</td>\n",
       "      <td>0.024763</td>\n",
       "      <td>0.008667</td>\n",
       "      <td>0.004176</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003325</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014876</td>\n",
       "      <td>0.004533</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.005930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043724</td>\n",
       "      <td>0.002515</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.003991</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.003062</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.003009</td>\n",
       "      <td>0.002543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.019081</td>\n",
       "      <td>0.008378</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>0.010516</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017083</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005120</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.004128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005860</td>\n",
       "      <td>0.004777</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>0.012232</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>0.026712</td>\n",
       "      <td>0.014666</td>\n",
       "      <td>0.009722</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.004846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.031174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030111</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>159889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.39027</td>\n",
       "      <td>-6.23547</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>355.0</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.816467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.009270</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.005810</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.004541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>0.012805</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008909</td>\n",
       "      <td>0.005292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>0.006277</td>\n",
       "      <td>0.055078</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>0.012237</td>\n",
       "      <td>0.004215</td>\n",
       "      <td>0.018928</td>\n",
       "      <td>0.010253</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>0.002718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006846</td>\n",
       "      <td>0.004529</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.004180</td>\n",
       "      <td>0.003111</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>0.005331</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.009154</td>\n",
       "      <td>0.002243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013229</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.008171</td>\n",
       "      <td>0.00194</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021142</td>\n",
       "      <td>0.006063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>0.011039</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.003481</td>\n",
       "      <td>0.005464</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.007354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053989</td>\n",
       "      <td>0.006129</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003831</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>0.003093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.002851</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.004705</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025246</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025474</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>0.004645</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.045882</td>\n",
       "      <td>0.029104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.025199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009075</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.004383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.015055</td>\n",
       "      <td>0.005286</td>\n",
       "      <td>0.004207</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>0.007703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009403</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.012699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.011967</td>\n",
       "      <td>0.004438</td>\n",
       "      <td>0.003596</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009101</td>\n",
       "      <td>0.013441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.001918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>0.003305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.003273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028199</td>\n",
       "      <td>0.002656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004367</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.005545</td>\n",
       "      <td>0.018427</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004892</td>\n",
       "      <td>0.002047</td>\n",
       "      <td>0.015246</td>\n",
       "      <td>0.011618</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.002718</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.001977</td>\n",
       "      <td>0.010095</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.007369</td>\n",
       "      <td>0.003927</td>\n",
       "      <td>0.004822</td>\n",
       "      <td>0.014007</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>0.009641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002650</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.004794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>0.002295</td>\n",
       "      <td>0.003091</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.001613</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001884</td>\n",
       "      <td>0.019377</td>\n",
       "      <td>0.003198</td>\n",
       "      <td>0.010383</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.020797</td>\n",
       "      <td>0.002095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014195</td>\n",
       "      <td>0.002730</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>0.007180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010820</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001884</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003730</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>0.004031</td>\n",
       "      <td>0.045057</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.003619</td>\n",
       "      <td>0.015956</td>\n",
       "      <td>0.004987</td>\n",
       "      <td>0.006775</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.002970</td>\n",
       "      <td>0.012559</td>\n",
       "      <td>0.001822</td>\n",
       "      <td>0.005647</td>\n",
       "      <td>0.036420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003211</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.035503</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003277</td>\n",
       "      <td>0.006948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>0.006854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.003224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018147</td>\n",
       "      <td>0.008281</td>\n",
       "      <td>0.030449</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.003145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012475</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.035910</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.011842</td>\n",
       "      <td>0.013160</td>\n",
       "      <td>0.027903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004254</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>0.055173</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.040066</td>\n",
       "      <td>0.004022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.032210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.002066</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007361</td>\n",
       "      <td>0.012205</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004549</td>\n",
       "      <td>0.001550</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.002546</td>\n",
       "      <td>0.003972</td>\n",
       "      <td>0.002965</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.004820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.018914</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.007224</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004619</td>\n",
       "      <td>0.008440</td>\n",
       "      <td>0.021925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>0.005305</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.003898</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>0.027811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.002678</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.014875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012661</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.26798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.002082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.002986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007660</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.006616</td>\n",
       "      <td>0.018640</td>\n",
       "      <td>0.003531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.005501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010893</td>\n",
       "      <td>0.009648</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004337</td>\n",
       "      <td>0.011451</td>\n",
       "      <td>0.003553</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006916</td>\n",
       "      <td>0.010704</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.011283</td>\n",
       "      <td>0.006528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003658</td>\n",
       "      <td>0.005469</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005876</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.006394</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.003582</td>\n",
       "      <td>0.00275</td>\n",
       "      <td>0.004882</td>\n",
       "      <td>0.006716</td>\n",
       "      <td>0.003456</td>\n",
       "      <td>0.009248</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.013655</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.004238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.022576</td>\n",
       "      <td>0.016215</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.002243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038881</td>\n",
       "      <td>0.004799</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>0.011598</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.003702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004679</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.003452</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.007291</td>\n",
       "      <td>0.004935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.005959</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018778</td>\n",
       "      <td>0.010084</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>0.003927</td>\n",
       "      <td>0.030582</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.028981</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.008541</td>\n",
       "      <td>0.00116</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002913</td>\n",
       "      <td>0.002102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>0.008723</td>\n",
       "      <td>0.005541</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055527</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005566</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.002375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>0.005864</td>\n",
       "      <td>0.011827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004197</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.002699</td>\n",
       "      <td>0.011113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001171</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.003574</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.003914</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003131</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.003881</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.002898</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>0.006317</td>\n",
       "      <td>0.052137</td>\n",
       "      <td>0.006997</td>\n",
       "      <td>0.027610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016392</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.012260</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002566</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007256</td>\n",
       "      <td>0.008501</td>\n",
       "      <td>0.001457</td>\n",
       "      <td>0.004614</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.006006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018084</td>\n",
       "      <td>0.015569</td>\n",
       "      <td>0.004811</td>\n",
       "      <td>0.005433</td>\n",
       "      <td>0.003823</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.008240</td>\n",
       "      <td>0.019176</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.006036</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.004541</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013050</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008084</td>\n",
       "      <td>0.008864</td>\n",
       "      <td>0.004308</td>\n",
       "      <td>0.004230</td>\n",
       "      <td>0.002028</td>\n",
       "      <td>0.004267</td>\n",
       "      <td>0.003104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006091</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>0.002501</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.005957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001977</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.002066</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.015211</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010171</td>\n",
       "      <td>0.003969</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.004554</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.003385</td>\n",
       "      <td>0.002180</td>\n",
       "      <td>0.017446</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.007559</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.011769</td>\n",
       "      <td>0.007323</td>\n",
       "      <td>0.022259</td>\n",
       "      <td>0.011646</td>\n",
       "      <td>0.007884</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005845</td>\n",
       "      <td>0.009351</td>\n",
       "      <td>0.001944</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>0.005029</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.016983</td>\n",
       "      <td>0.003245</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.022115</td>\n",
       "      <td>0.001099</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>162809</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.28326</td>\n",
       "      <td>-6.19188</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>4.85</td>\n",
       "      <td>0.862075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>0.004321</td>\n",
       "      <td>0.003028</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006123</td>\n",
       "      <td>0.006213</td>\n",
       "      <td>0.004293</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.016335</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.004656</td>\n",
       "      <td>0.002148</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001633</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.00135</td>\n",
       "      <td>0.003522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.006288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.010978</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.002340</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>0.015971</td>\n",
       "      <td>0.002510</td>\n",
       "      <td>0.009514</td>\n",
       "      <td>0.017059</td>\n",
       "      <td>0.003363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005772</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.006301</td>\n",
       "      <td>0.002214</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.014848</td>\n",
       "      <td>0.003983</td>\n",
       "      <td>0.009967</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.008614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.004992</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.003813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007765</td>\n",
       "      <td>0.010342</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>0.015766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.003377</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.008665</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010328</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.010328</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>0.013069</td>\n",
       "      <td>0.008965</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>0.013104</td>\n",
       "      <td>0.003640</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.003254</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00137</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.003674</td>\n",
       "      <td>0.006938</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059762</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.006845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028786</td>\n",
       "      <td>0.005835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.006749</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012141</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007892</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.003279</td>\n",
       "      <td>0.009680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004461</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.006581</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>0.017942</td>\n",
       "      <td>0.032161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.010162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.003458</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.007046</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.034318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003896</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004385</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003671</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.004227</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.005278</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011661</td>\n",
       "      <td>0.002318</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012030</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.006509</td>\n",
       "      <td>0.004058</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.006812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>0.004876</td>\n",
       "      <td>0.004921</td>\n",
       "      <td>0.002678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.002852</td>\n",
       "      <td>0.010548</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003974</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>0.003577</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006604</td>\n",
       "      <td>0.015173</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.012758</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.003792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.003749</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.006512</td>\n",
       "      <td>0.016739</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.006457</td>\n",
       "      <td>0.010174</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.014881</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.006314</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.003850</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>0.002892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013371</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.001923</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.012416</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004553</td>\n",
       "      <td>0.013383</td>\n",
       "      <td>0.004153</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013280</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.009003</td>\n",
       "      <td>0.012091</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>0.009294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001872</td>\n",
       "      <td>0.002081</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004068</td>\n",
       "      <td>0.004030</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.005335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.005034</td>\n",
       "      <td>0.027451</td>\n",
       "      <td>0.005822</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.009124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.005822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013950</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>0.021254</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>0.004297</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.011097</td>\n",
       "      <td>0.005151</td>\n",
       "      <td>0.004732</td>\n",
       "      <td>0.037640</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.006840</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074520</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002876</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008441</td>\n",
       "      <td>0.007312</td>\n",
       "      <td>0.041704</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003352</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.017922</td>\n",
       "      <td>0.004134</td>\n",
       "      <td>0.038085</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005227</td>\n",
       "      <td>0.014970</td>\n",
       "      <td>0.018456</td>\n",
       "      <td>0.079204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>0.002840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003063</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048434</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.001556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006027</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.002290</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.002209</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.011270</td>\n",
       "      <td>0.004584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001944</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012047</td>\n",
       "      <td>0.005399</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.002102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.003053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019836</td>\n",
       "      <td>0.002768</td>\n",
       "      <td>0.008690</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.007993</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>0.008224</td>\n",
       "      <td>0.005912</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.002874</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>0.003554</td>\n",
       "      <td>0.004231</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002026</td>\n",
       "      <td>0.002649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.013442</td>\n",
       "      <td>0.008651</td>\n",
       "      <td>0.024777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.004181</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>0.003594</td>\n",
       "      <td>0.005468</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.006804</td>\n",
       "      <td>0.007556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.003634</td>\n",
       "      <td>0.024853</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005744</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.002684</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.003111</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.005562</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007260</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.006902</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.006874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008999</td>\n",
       "      <td>0.008619</td>\n",
       "      <td>0.007124</td>\n",
       "      <td>0.005642</td>\n",
       "      <td>0.006564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002029</td>\n",
       "      <td>0.003306</td>\n",
       "      <td>0.008390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.002194</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.009023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.012235</td>\n",
       "      <td>0.005113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.002122</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.009521</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>0.005210</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.003420</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.012873</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.010004</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.045174</td>\n",
       "      <td>0.010622</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.006627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010829</td>\n",
       "      <td>0.006512</td>\n",
       "      <td>0.002423</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.004436</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>0.011554</td>\n",
       "      <td>0.003608</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.005831</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.001883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.004648</td>\n",
       "      <td>0.003948</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.025014</td>\n",
       "      <td>0.006446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.004516</td>\n",
       "      <td>0.004963</td>\n",
       "      <td>0.035499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.028608</td>\n",
       "      <td>0.005615</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.001907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.005093</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.002731</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>0.006301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003422</td>\n",
       "      <td>0.002949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015139</td>\n",
       "      <td>0.016847</td>\n",
       "      <td>0.002847</td>\n",
       "      <td>0.003410</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007536</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006073</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.009269</td>\n",
       "      <td>0.001948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>0.002875</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002748</td>\n",
       "      <td>0.004496</td>\n",
       "      <td>0.005445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.009561</td>\n",
       "      <td>0.003172</td>\n",
       "      <td>0.004916</td>\n",
       "      <td>0.005108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001884</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>0.004779</td>\n",
       "      <td>0.005187</td>\n",
       "      <td>0.00156</td>\n",
       "      <td>0.006087</td>\n",
       "      <td>0.016118</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.002615</td>\n",
       "      <td>0.004924</td>\n",
       "      <td>0.004632</td>\n",
       "      <td>0.006493</td>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>0.001985</td>\n",
       "      <td>0.049230</td>\n",
       "      <td>0.012206</td>\n",
       "      <td>0.015735</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009831</td>\n",
       "      <td>0.004847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005453</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.010164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008855</td>\n",
       "      <td>0.014233</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.009058</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004416</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.010173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.009635</td>\n",
       "      <td>0.018502</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.005540</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.003143</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.009445</td>\n",
       "      <td>0.002726</td>\n",
       "      <td>0.001614</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.024214</td>\n",
       "      <td>0.002749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011136</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002876</td>\n",
       "      <td>0.004663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.006168</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.011240</td>\n",
       "      <td>0.007224</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>0.001166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008840</td>\n",
       "      <td>0.003440</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.002478</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.004007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061380</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012896</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>0.002712</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.005084</td>\n",
       "      <td>0.013440</td>\n",
       "      <td>0.006379</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005720</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014660</td>\n",
       "      <td>0.002843</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.014664</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.017638</td>\n",
       "      <td>0.006153</td>\n",
       "      <td>0.027577</td>\n",
       "      <td>0.022179</td>\n",
       "      <td>0.014944</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>0.004466</td>\n",
       "      <td>0.013296</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.003351</td>\n",
       "      <td>0.022918</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>0.002328</td>\n",
       "      <td>0.003315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>165828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.33744</td>\n",
       "      <td>-6.32363</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0.790500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.002374</td>\n",
       "      <td>0.015537</td>\n",
       "      <td>0.017138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019202</td>\n",
       "      <td>0.018945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008278</td>\n",
       "      <td>0.004407</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017708</td>\n",
       "      <td>0.014127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002762</td>\n",
       "      <td>0.007630</td>\n",
       "      <td>0.005434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014911</td>\n",
       "      <td>0.005080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.087187</td>\n",
       "      <td>0.008349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>0.023473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016124</td>\n",
       "      <td>0.010598</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>0.004838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011209</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005092</td>\n",
       "      <td>0.021348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007038</td>\n",
       "      <td>0.007508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.003791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.016545</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012040</td>\n",
       "      <td>0.011771</td>\n",
       "      <td>0.018679</td>\n",
       "      <td>0.011541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009404</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>0.004054</td>\n",
       "      <td>0.008138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004422</td>\n",
       "      <td>0.042632</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005364</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005305</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004675</td>\n",
       "      <td>0.004843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008297</td>\n",
       "      <td>0.002798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005660</td>\n",
       "      <td>0.009207</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036254</td>\n",
       "      <td>0.033689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006391</td>\n",
       "      <td>0.003103</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.008737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007872</td>\n",
       "      <td>0.006916</td>\n",
       "      <td>0.004221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020962</td>\n",
       "      <td>0.004275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004252</td>\n",
       "      <td>0.002725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025083</td>\n",
       "      <td>0.004681</td>\n",
       "      <td>0.009493</td>\n",
       "      <td>0.007344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013667</td>\n",
       "      <td>0.005137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010267</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003858</td>\n",
       "      <td>0.009281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004815</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004802</td>\n",
       "      <td>0.002904</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.004163</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>0.018084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012701</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017272</td>\n",
       "      <td>0.005880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006811</td>\n",
       "      <td>0.050076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003423</td>\n",
       "      <td>0.004857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018191</td>\n",
       "      <td>0.021754</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001962</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004590</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.002255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016397</td>\n",
       "      <td>0.013002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006899</td>\n",
       "      <td>0.018376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006457</td>\n",
       "      <td>0.008659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022447</td>\n",
       "      <td>0.002811</td>\n",
       "      <td>0.022057</td>\n",
       "      <td>0.009101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008919</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004151</td>\n",
       "      <td>0.005504</td>\n",
       "      <td>0.004344</td>\n",
       "      <td>0.016122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016326</td>\n",
       "      <td>0.008430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>0.012363</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013062</td>\n",
       "      <td>0.014117</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.010173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006980</td>\n",
       "      <td>0.006848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004176</td>\n",
       "      <td>0.006021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002780</td>\n",
       "      <td>0.006407</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023049</td>\n",
       "      <td>0.006752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031901</td>\n",
       "      <td>0.009682</td>\n",
       "      <td>0.005633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006852</td>\n",
       "      <td>0.004410</td>\n",
       "      <td>0.004243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>0.004503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013246</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006555</td>\n",
       "      <td>0.016727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006614</td>\n",
       "      <td>0.007228</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002428</td>\n",
       "      <td>0.014432</td>\n",
       "      <td>0.008720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006448</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>0.008558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>0.002061</td>\n",
       "      <td>0.006572</td>\n",
       "      <td>0.002737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017244</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.011637</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009495</td>\n",
       "      <td>0.002855</td>\n",
       "      <td>0.004459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>0.005249</td>\n",
       "      <td>0.009632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004807</td>\n",
       "      <td>0.002597</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.007790</td>\n",
       "      <td>0.003496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005778</td>\n",
       "      <td>0.009560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003983</td>\n",
       "      <td>0.005187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008951</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006123</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.013464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004723</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.004672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005551</td>\n",
       "      <td>0.014045</td>\n",
       "      <td>0.010617</td>\n",
       "      <td>0.046717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>0.012499</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.008225</td>\n",
       "      <td>0.007049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004370</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006503</td>\n",
       "      <td>0.004210</td>\n",
       "      <td>0.026371</td>\n",
       "      <td>0.005539</td>\n",
       "      <td>0.014350</td>\n",
       "      <td>0.003254</td>\n",
       "      <td>0.007773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003477</td>\n",
       "      <td>0.007253</td>\n",
       "      <td>0.002851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.019666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004782</td>\n",
       "      <td>0.004589</td>\n",
       "      <td>0.014959</td>\n",
       "      <td>0.015281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010766</td>\n",
       "      <td>0.013034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.007503</td>\n",
       "      <td>0.006091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047989</td>\n",
       "      <td>0.003889</td>\n",
       "      <td>0.005261</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.011055</td>\n",
       "      <td>0.006943</td>\n",
       "      <td>0.005173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010312</td>\n",
       "      <td>0.004448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005768</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012533</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003032</td>\n",
       "      <td>0.003043</td>\n",
       "      <td>0.015214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>0.044561</td>\n",
       "      <td>0.003743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004441</td>\n",
       "      <td>0.004532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044199</td>\n",
       "      <td>0.014270</td>\n",
       "      <td>0.007144</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004362</td>\n",
       "      <td>0.007320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012044</td>\n",
       "      <td>0.006913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008109</td>\n",
       "      <td>0.017894</td>\n",
       "      <td>0.014834</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005180</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>0.006174</td>\n",
       "      <td>0.013302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003478</td>\n",
       "      <td>0.036332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021223</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.005678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>0.013093</td>\n",
       "      <td>0.020530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008718</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010044</td>\n",
       "      <td>0.009046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006472</td>\n",
       "      <td>0.009566</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005603</td>\n",
       "      <td>0.004864</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.018659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005331</td>\n",
       "      <td>0.009823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006118</td>\n",
       "      <td>0.004690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007460</td>\n",
       "      <td>0.005262</td>\n",
       "      <td>0.005514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.02091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007288</td>\n",
       "      <td>0.003110</td>\n",
       "      <td>0.003578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003619</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.013337</td>\n",
       "      <td>0.018735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>0.009467</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005054</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014406</td>\n",
       "      <td>0.065594</td>\n",
       "      <td>0.013178</td>\n",
       "      <td>0.022305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>0.007425</td>\n",
       "      <td>0.004449</td>\n",
       "      <td>0.005189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008648</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.011580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003753</td>\n",
       "      <td>0.011053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.011969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.004833</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016665</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>0.002855</td>\n",
       "      <td>0.006084</td>\n",
       "      <td>0.010777</td>\n",
       "      <td>0.005203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005548</td>\n",
       "      <td>0.003324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006009</td>\n",
       "      <td>0.006986</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015581</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.003913</td>\n",
       "      <td>0.004628</td>\n",
       "      <td>0.051715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028286</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>0.006569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.007617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004916</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009471</td>\n",
       "      <td>0.011707</td>\n",
       "      <td>0.003909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022484</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>0.007964</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.006826</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005310</td>\n",
       "      <td>0.021729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004519</td>\n",
       "      <td>0.011562</td>\n",
       "      <td>0.011512</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006047</td>\n",
       "      <td>0.002063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005729</td>\n",
       "      <td>0.054534</td>\n",
       "      <td>0.009122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011086</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006099</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.023489</td>\n",
       "      <td>0.003491</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>0.019001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008543</td>\n",
       "      <td>0.049671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002725</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0_x      id  host_is_superhost  latitude  longitude  \\\n",
       "0           0             0   44077                1.0  53.29178   -6.25792   \n",
       "1           1             1   85156                1.0  53.29209   -6.25624   \n",
       "2           2             2  159889                0.0  53.39027   -6.23547   \n",
       "3           3             3  162809                1.0  53.28326   -6.19188   \n",
       "4           4             4  165828                0.0  53.33744   -6.32363   \n",
       "\n",
       "   accommodates  bathrooms_text  bedrooms  beds  amenities  price  \\\n",
       "0           2.0             1.5       1.0   2.0      113.0   70.0   \n",
       "1           2.0             1.5       1.0   1.0       72.0   67.0   \n",
       "2           1.0             2.0       1.0   1.0       56.0   45.0   \n",
       "3           6.0             1.5       2.0   4.0      102.0   80.0   \n",
       "4           5.0             1.0       1.0   3.0       53.0  251.0   \n",
       "\n",
       "   minimum_nights  number_of_reviews  review_scores_value  sentiment  \\\n",
       "0             3.0              258.0                 4.82   0.897737   \n",
       "1             3.0              210.0                 4.78   0.909530   \n",
       "2             2.0              355.0                 4.74   0.816467   \n",
       "3             1.0              326.0                 4.85   0.862075   \n",
       "4             3.0               64.0                 4.55   0.790500   \n",
       "\n",
       "   room_type  property_type  Unnamed: 0_y   tfidf_0   tfidf_1   tfidf_2  \\\n",
       "0        0.0            2.0             0  0.008526  0.000000  0.013425   \n",
       "1        0.0            2.0             1  0.009196  0.000000  0.011966   \n",
       "2        0.0            2.0             2  0.010858  0.000604  0.008065   \n",
       "3        0.0            2.0             3  0.008333  0.003754  0.001756   \n",
       "4        1.0            2.0             4  0.018000  0.002374  0.015537   \n",
       "\n",
       "    tfidf_3   tfidf_4   tfidf_5   tfidf_6   tfidf_7   tfidf_8   tfidf_9  \\\n",
       "0  0.005049  0.003768  0.003457  0.001066  0.002238  0.001012  0.006799   \n",
       "1  0.003139  0.001363  0.002680  0.000000  0.001506  0.002799  0.014494   \n",
       "2  0.009270  0.002215  0.005810  0.001743  0.004541  0.000000  0.004765   \n",
       "3  0.004321  0.003028  0.004356  0.001482  0.000999  0.000000  0.006123   \n",
       "4  0.017138  0.000000  0.000000  0.000000  0.005149  0.000000  0.000000   \n",
       "\n",
       "   tfidf_10  tfidf_11  tfidf_12  tfidf_13  tfidf_14  tfidf_15  tfidf_16  \\\n",
       "0  0.004704  0.006454  0.000000  0.001093  0.003776  0.002912  0.003878   \n",
       "1  0.000692  0.006672  0.000000  0.001788  0.008810  0.005144  0.002593   \n",
       "2  0.012805  0.002518  0.000845  0.000000  0.008909  0.005292  0.000000   \n",
       "3  0.006213  0.004293  0.001588  0.002039  0.007812  0.016335  0.000895   \n",
       "4  0.019202  0.018945  0.000000  0.000000  0.008454  0.000000  0.000000   \n",
       "\n",
       "   tfidf_17  tfidf_18  tfidf_19  tfidf_20  tfidf_21  tfidf_22  tfidf_23  \\\n",
       "0  0.001084  0.000000  0.000786       0.0  0.003593  0.000000   0.00000   \n",
       "1  0.004843  0.000000  0.000000       0.0  0.002524  0.001766   0.00000   \n",
       "2  0.003540  0.000000  0.006013       0.0  0.000000  0.000887   0.00000   \n",
       "3  0.004656  0.002148  0.000476       0.0  0.001633  0.000909   0.00135   \n",
       "4  0.008278  0.004407  0.003372       0.0  0.000000  0.000000   0.00000   \n",
       "\n",
       "   tfidf_24  tfidf_25  tfidf_26  tfidf_27  tfidf_28  tfidf_29  tfidf_30  \\\n",
       "0  0.003106  0.004872       0.0       0.0  0.000000  0.009547  0.000000   \n",
       "1  0.004161  0.000972       0.0       0.0  0.000000  0.013895  0.000000   \n",
       "2  0.000879  0.000438       0.0       0.0  0.001429  0.004625  0.001336   \n",
       "3  0.003522  0.000000       0.0       0.0  0.000969  0.006288  0.000000   \n",
       "4  0.000000  0.000000       0.0       0.0  0.000000  0.000000  0.000000   \n",
       "\n",
       "   tfidf_31  tfidf_32  tfidf_33  tfidf_34  tfidf_35  tfidf_36  tfidf_37  \\\n",
       "0  0.002100  0.006517  0.003636  0.003072  0.000906  0.000490  0.002253   \n",
       "1  0.000000  0.004914  0.004611  0.000000  0.001610  0.000000  0.001146   \n",
       "2  0.002302  0.006277  0.055078  0.000328  0.001450  0.000000  0.002035   \n",
       "3  0.002154  0.010978  0.003333  0.002340  0.000566  0.000738  0.003054   \n",
       "4  0.000000  0.017708  0.014127  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "   tfidf_38  tfidf_39  tfidf_40  tfidf_41  tfidf_42  tfidf_43  tfidf_44  \\\n",
       "0  0.016402  0.001984  0.010011  0.016258  0.003165       0.0       0.0   \n",
       "1  0.024434  0.002868  0.010553  0.015432  0.003694       0.0       0.0   \n",
       "2  0.012237  0.004215  0.018928  0.010253  0.001372       0.0       0.0   \n",
       "3  0.015971  0.002510  0.009514  0.017059  0.003363       0.0       0.0   \n",
       "4  0.023924  0.000000  0.002762  0.007630  0.005434       0.0       0.0   \n",
       "\n",
       "   tfidf_45  tfidf_46  tfidf_47  tfidf_48  tfidf_49  tfidf_50  tfidf_51  \\\n",
       "0       0.0  0.004420  0.000717  0.001264  0.009963  0.004866  0.002686   \n",
       "1       0.0  0.003515  0.000000  0.000000  0.010832  0.003894  0.001354   \n",
       "2       0.0  0.001785  0.002718  0.000000  0.006846  0.004529  0.000529   \n",
       "3       0.0  0.005772  0.001487  0.000892  0.006301  0.002214  0.003416   \n",
       "4       0.0  0.000000  0.000000  0.014911  0.005080  0.000000  0.008512   \n",
       "\n",
       "   tfidf_52  tfidf_53  tfidf_54  tfidf_55  tfidf_56  tfidf_57  tfidf_58  \\\n",
       "0       0.0  0.000868  0.000000  0.000000  0.000000  0.004265  0.000000   \n",
       "1       0.0  0.000000  0.000000  0.000000  0.000000  0.005078  0.000000   \n",
       "2       0.0  0.000453  0.000000  0.000000  0.000000  0.001709  0.000000   \n",
       "3       0.0  0.006989  0.000000  0.001908  0.000000  0.001379  0.000782   \n",
       "4       0.0  0.087187  0.008349  0.000000  0.021607  0.000000  0.004212   \n",
       "\n",
       "   tfidf_59  tfidf_60  tfidf_61  tfidf_62  tfidf_63  tfidf_64  tfidf_65  \\\n",
       "0  0.017213  0.002142  0.022071  0.000000  0.003657  0.000920  0.011882   \n",
       "1  0.017514  0.001296  0.014192  0.000000  0.001425  0.000969  0.006111   \n",
       "2  0.010179  0.000000  0.006069  0.000000  0.001238  0.000468  0.003451   \n",
       "3  0.014848  0.003983  0.009967  0.002326  0.008614  0.000000  0.010980   \n",
       "4  0.023473  0.000000  0.016124  0.010598  0.001401  0.000000  0.004881   \n",
       "\n",
       "   tfidf_66  tfidf_67  tfidf_68  tfidf_69  tfidf_70  tfidf_71  tfidf_72  \\\n",
       "0  0.000000  0.001423  0.004814  0.001527  0.002172  0.000000  0.001416   \n",
       "1  0.001546  0.001118  0.006205  0.004721  0.000000  0.000000  0.000818   \n",
       "2  0.001330  0.004180  0.003111  0.001442  0.001979  0.005331  0.000638   \n",
       "3  0.000000  0.002776  0.000754  0.004992  0.002256  0.003813  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.002642  0.004838  0.000000  0.011209   \n",
       "\n",
       "   tfidf_73  tfidf_74  tfidf_75  tfidf_76  tfidf_77  tfidf_78  tfidf_79  \\\n",
       "0  0.004070  0.010814  0.000000  0.006603  0.001466  0.000000  0.014734   \n",
       "1  0.000000  0.001227  0.000000  0.013399  0.006057  0.000000  0.017860   \n",
       "2  0.001317  0.004090  0.001451  0.009154  0.002243  0.000000  0.013229   \n",
       "3  0.000000  0.006405  0.000000  0.007765  0.010342  0.002226  0.015766   \n",
       "4  0.000000  0.000000  0.005092  0.021348  0.000000  0.000000  0.007038   \n",
       "\n",
       "   tfidf_80  tfidf_81  tfidf_82  tfidf_83  tfidf_84  tfidf_85  tfidf_86  \\\n",
       "0  0.001074  0.000566       0.0  0.000725  0.001665  0.001327  0.000000   \n",
       "1  0.000000  0.000000       0.0  0.000855  0.000000  0.000000  0.001213   \n",
       "2  0.001788  0.000000       0.0  0.001250  0.000845  0.001088  0.000734   \n",
       "3  0.000000  0.000000       0.0  0.000000  0.000650  0.003377  0.000991   \n",
       "4  0.007508  0.000000       0.0  0.001687  0.003791  0.000000  0.000000   \n",
       "\n",
       "   tfidf_87  tfidf_88  tfidf_89  tfidf_90  tfidf_91  tfidf_92  tfidf_93  \\\n",
       "0  0.001400  0.007383   0.00000  0.000000       0.0  0.013435  0.000000   \n",
       "1  0.000000  0.009199   0.00000  0.000000       0.0  0.018644  0.000827   \n",
       "2  0.000717  0.008171   0.00194  0.000298       0.0  0.004499  0.000000   \n",
       "3  0.000881  0.008665   0.00000  0.000000       0.0  0.010328  0.002696   \n",
       "4  0.002055  0.016545   0.00000  0.000000       0.0  0.012040  0.011771   \n",
       "\n",
       "   tfidf_94  tfidf_95  tfidf_96  tfidf_97  tfidf_98  tfidf_99  tfidf_100  \\\n",
       "0  0.009956  0.004094  0.000656  0.013329  0.012432  0.008753   0.003837   \n",
       "1  0.018556  0.003093  0.001389  0.002702  0.006741  0.008545   0.003851   \n",
       "2  0.021142  0.006063  0.000000  0.001140  0.011039  0.002521   0.003481   \n",
       "3  0.010328  0.002985  0.004172  0.013069  0.008965  0.002265   0.013104   \n",
       "4  0.018679  0.011541  0.000000  0.009404  0.005639  0.003120   0.004054   \n",
       "\n",
       "   tfidf_101  tfidf_102  tfidf_103  tfidf_104  tfidf_105  tfidf_106  \\\n",
       "0   0.003063   0.000704   0.006932   0.000000        0.0   0.002973   \n",
       "1   0.005404   0.000000   0.005975   0.000000        0.0   0.000000   \n",
       "2   0.005464   0.000898   0.007354   0.000000        0.0   0.002197   \n",
       "3   0.003640   0.001343   0.003254   0.000834        0.0   0.000000   \n",
       "4   0.008138   0.000000   0.035723   0.000000        0.0   0.000000   \n",
       "\n",
       "   tfidf_107  tfidf_108  tfidf_109  tfidf_110  tfidf_111  tfidf_112  \\\n",
       "0    0.00000   0.000000   0.002484   0.002897   0.002167   0.000000   \n",
       "1    0.00000   0.002219   0.003075   0.000000   0.001217   0.000000   \n",
       "2    0.00000   0.000650   0.001634   0.000653   0.000529   0.000000   \n",
       "3    0.00137   0.001192   0.003674   0.006938   0.004214   0.000000   \n",
       "4    0.00000   0.000000   0.005569   0.000000   0.002344   0.002193   \n",
       "\n",
       "   tfidf_113  tfidf_114  tfidf_115  tfidf_116  tfidf_117  tfidf_118  \\\n",
       "0   0.000000   0.000000   0.004487   0.060857   0.000000   0.002484   \n",
       "1   0.000000   0.000000   0.000000   0.054860   0.000000   0.004595   \n",
       "2   0.000000   0.000000   0.000000   0.055137   0.000000   0.000854   \n",
       "3   0.000000   0.000913   0.000000   0.059762   0.001889   0.002307   \n",
       "4   0.002376   0.000000   0.000000   0.053880   0.000000   0.004300   \n",
       "\n",
       "   tfidf_119  tfidf_120  tfidf_121  tfidf_122  tfidf_123  tfidf_124  \\\n",
       "0        0.0   0.022948        0.0        0.0   0.000000   0.001103   \n",
       "1        0.0   0.041443        0.0        0.0   0.002499   0.004201   \n",
       "2        0.0   0.018354        0.0        0.0   0.002452   0.000000   \n",
       "3        0.0   0.032533        0.0        0.0   0.001455   0.006845   \n",
       "4        0.0   0.001229        0.0        0.0   0.004120   0.000000   \n",
       "\n",
       "   tfidf_125  tfidf_126  tfidf_127  tfidf_128  tfidf_129  tfidf_130  \\\n",
       "0   0.000000   0.000000   0.028956   0.003746   0.000668   0.001855   \n",
       "1   0.001159   0.000000   0.025745   0.000649   0.000000   0.001478   \n",
       "2   0.000000   0.000000   0.053989   0.006129   0.000800   0.000000   \n",
       "3   0.000000   0.000000   0.028786   0.005835   0.000000   0.000676   \n",
       "4   0.000000   0.004422   0.042632   0.006061   0.000000   0.000000   \n",
       "\n",
       "   tfidf_131  tfidf_132  tfidf_133  tfidf_134  tfidf_135  tfidf_136  \\\n",
       "0   0.002551     0.0000   0.001909        0.0        0.0   0.001502   \n",
       "1   0.003395     0.0014   0.000918        0.0        0.0   0.000000   \n",
       "2   0.003831     0.0000   0.001459        0.0        0.0   0.000000   \n",
       "3   0.001414     0.0000   0.000000        0.0        0.0   0.000000   \n",
       "4   0.005364     0.0000   0.005305        0.0        0.0   0.000000   \n",
       "\n",
       "   tfidf_137  tfidf_138  tfidf_139  tfidf_140  tfidf_141  tfidf_142  \\\n",
       "0   0.000000   0.001071   0.005262   0.003416        0.0   0.004062   \n",
       "1   0.001809   0.000956   0.004768   0.000000        0.0   0.000000   \n",
       "2   0.001046   0.002068   0.003529   0.003093        0.0   0.001314   \n",
       "3   0.001315   0.000000   0.003380   0.000000        0.0   0.000000   \n",
       "4   0.004675   0.004843   0.000000   0.010866        0.0   0.000000   \n",
       "\n",
       "   tfidf_143  tfidf_144  tfidf_145  tfidf_146  tfidf_147  tfidf_148  \\\n",
       "0   0.003247   0.002909   0.001747   0.006759   0.006818        0.0   \n",
       "1   0.000884   0.004257   0.002312   0.000709   0.003738        0.0   \n",
       "2   0.002851   0.003988   0.001484   0.004705   0.003156        0.0   \n",
       "3   0.001163   0.006749   0.000451   0.002260   0.000687        0.0   \n",
       "4   0.000000   0.008297   0.002798   0.000000   0.000000        0.0   \n",
       "\n",
       "   tfidf_149  tfidf_150  tfidf_151  tfidf_152  tfidf_153  tfidf_154  \\\n",
       "0   0.000000        0.0        0.0   0.003058        0.0   0.025498   \n",
       "1   0.000000        0.0        0.0   0.000000        0.0   0.018899   \n",
       "2   0.000699        0.0        0.0   0.001718        0.0   0.025246   \n",
       "3   0.000000        0.0        0.0   0.001648        0.0   0.012141   \n",
       "4   0.005039        0.0        0.0   0.000000        0.0   0.033193   \n",
       "\n",
       "   tfidf_155  tfidf_156  tfidf_157  tfidf_158  tfidf_159  tfidf_160  \\\n",
       "0   0.000000   0.001555   0.013455   0.001425   0.001691   0.002105   \n",
       "1   0.002762   0.000000   0.018038   0.000841   0.001089   0.001725   \n",
       "2   0.001573   0.000000   0.025474   0.000715   0.005454   0.003342   \n",
       "3   0.000798   0.000000   0.007892   0.001336   0.002220   0.002524   \n",
       "4   0.000000   0.000000   0.014700   0.000000   0.000000   0.000000   \n",
       "\n",
       "   tfidf_161  tfidf_162  tfidf_163  tfidf_164  tfidf_165  tfidf_166  \\\n",
       "0   0.011238   0.004067   0.000000   0.000817   0.001137        0.0   \n",
       "1   0.002140   0.006061   0.000000   0.000939   0.000000        0.0   \n",
       "2   0.001255   0.004645   0.001881   0.002894   0.000000        0.0   \n",
       "3   0.003279   0.009680   0.000000   0.004461   0.001568        0.0   \n",
       "4   0.000000   0.000000   0.005660   0.009207   0.003201        0.0   \n",
       "\n",
       "   tfidf_167  tfidf_168  tfidf_169  tfidf_170  tfidf_171  tfidf_172  \\\n",
       "0   0.000000   0.001967   0.001523   0.002081   0.030837   0.023336   \n",
       "1   0.000000   0.000000   0.003463   0.005548   0.030645   0.019623   \n",
       "2   0.000000   0.000715   0.002957   0.001696   0.045882   0.029104   \n",
       "3   0.001928   0.001812   0.006581   0.001538   0.017942   0.032161   \n",
       "4   0.000000   0.003001   0.000000   0.000000   0.036254   0.033689   \n",
       "\n",
       "   tfidf_173  tfidf_174  tfidf_175  tfidf_176  tfidf_177  tfidf_178  \\\n",
       "0        0.0   0.000000   0.000000   0.000626   0.012383        0.0   \n",
       "1        0.0   0.000000   0.001700   0.001112   0.011950        0.0   \n",
       "2        0.0   0.000000   0.001399   0.000000   0.022085        0.0   \n",
       "3        0.0   0.000000   0.001110   0.000828   0.010162        0.0   \n",
       "4        0.0   0.005007   0.000000   0.000000   0.016108        0.0   \n",
       "\n",
       "   tfidf_179  tfidf_180  tfidf_181  tfidf_182  tfidf_183  tfidf_184  \\\n",
       "0   0.000000   0.003739   0.000328   0.001094   0.007625   0.000000   \n",
       "1   0.000980   0.000919   0.002737   0.000000   0.007658   0.002366   \n",
       "2   0.000000   0.006889   0.000000   0.000000   0.008400   0.001007   \n",
       "3   0.000483   0.003458   0.001317   0.000471   0.007046   0.001115   \n",
       "4   0.000000   0.006391   0.003103   0.001909   0.008737   0.000000   \n",
       "\n",
       "   tfidf_185  tfidf_186  tfidf_187  tfidf_188  tfidf_189  tfidf_190  \\\n",
       "0   0.033270        0.0   0.004897   0.001601   0.000000   0.000000   \n",
       "1   0.029543        0.0   0.010136   0.002678   0.002447   0.003473   \n",
       "2   0.025199        0.0   0.009075   0.002248   0.000000   0.000592   \n",
       "3   0.034318        0.0   0.003896   0.001475   0.000000   0.000000   \n",
       "4   0.021274        0.0   0.007872   0.006916   0.004221   0.000000   \n",
       "\n",
       "   tfidf_191  tfidf_192  tfidf_193  tfidf_194  tfidf_195  tfidf_196  \\\n",
       "0   0.000000   0.001183   0.003758   0.000000   0.000000   0.001238   \n",
       "1   0.000000   0.000000   0.004449   0.000000   0.000940   0.000000   \n",
       "2   0.000000   0.000000   0.002129   0.002346   0.000613   0.000000   \n",
       "3   0.002221   0.000000   0.008038   0.000000   0.002362   0.000000   \n",
       "4   0.000000   0.000000   0.020962   0.004275   0.000000   0.000000   \n",
       "\n",
       "   tfidf_197  tfidf_198  tfidf_199  tfidf_200  tfidf_201  tfidf_202  \\\n",
       "0   0.008844   0.003269   0.002980        0.0        0.0   0.000000   \n",
       "1   0.001247   0.002095   0.002120        0.0        0.0   0.000920   \n",
       "2   0.000981   0.000548   0.004383        0.0        0.0   0.000276   \n",
       "3   0.000000   0.004385   0.002707        0.0        0.0   0.003572   \n",
       "4   0.000000   0.000000   0.002142        0.0        0.0   0.000000   \n",
       "\n",
       "   tfidf_203  tfidf_204  tfidf_205  tfidf_206  tfidf_207  tfidf_208  \\\n",
       "0   0.000000   0.006208   0.000000   0.006023   0.000676   0.000000   \n",
       "1   0.001475   0.008278   0.000000   0.004207   0.000000   0.000000   \n",
       "2   0.002296   0.015055   0.005286   0.004207   0.000686   0.001138   \n",
       "3   0.000000   0.003671   0.000464   0.004227   0.002279   0.000599   \n",
       "4   0.004252   0.002725   0.000000   0.000000   0.001978   0.007026   \n",
       "\n",
       "   tfidf_209  tfidf_210  tfidf_211  tfidf_212  tfidf_213  tfidf_214  \\\n",
       "0   0.000000   0.001759   0.018784   0.050836   0.000000   0.007505   \n",
       "1   0.001222   0.000000   0.012425   0.051303   0.000000   0.010631   \n",
       "2   0.000000   0.001895   0.007703   0.000000   0.000000   0.009403   \n",
       "3   0.003298   0.001067   0.001653   0.000000   0.000000   0.007235   \n",
       "4   0.000000   0.000000   0.025083   0.004681   0.009493   0.007344   \n",
       "\n",
       "   tfidf_215  tfidf_216  tfidf_217  tfidf_218  tfidf_219  tfidf_220  \\\n",
       "0   0.003565   0.006565        0.0   0.000000   0.015276   0.002588   \n",
       "1   0.000000   0.010783        0.0   0.001317   0.011622   0.002569   \n",
       "2   0.002757   0.012699        0.0   0.000770   0.011967   0.004438   \n",
       "3   0.000000   0.002164        0.0   0.000513   0.005278   0.001252   \n",
       "4   0.000000   0.007353        0.0   0.000000   0.008397   0.000000   \n",
       "\n",
       "   tfidf_221  tfidf_222  tfidf_223  tfidf_224  tfidf_225  tfidf_226  \\\n",
       "0   0.004099   0.000000        0.0        0.0   0.000000   0.005973   \n",
       "1   0.007013   0.000000        0.0        0.0   0.000878   0.004302   \n",
       "2   0.003596   0.000000        0.0        0.0   0.000000   0.001663   \n",
       "3   0.000000   0.000494        0.0        0.0   0.000826   0.000000   \n",
       "4   0.003727   0.000000        0.0        0.0   0.002149   0.000000   \n",
       "\n",
       "   tfidf_227  tfidf_228  tfidf_229  tfidf_230  tfidf_231  tfidf_232  \\\n",
       "0        0.0   0.015368   0.014190   0.000000        0.0   0.000884   \n",
       "1        0.0   0.008938   0.008556   0.000000        0.0   0.000000   \n",
       "2        0.0   0.009101   0.013441   0.000000        0.0   0.000000   \n",
       "3        0.0   0.011661   0.002318   0.001237        0.0   0.000000   \n",
       "4        0.0   0.013667   0.005137   0.000000        0.0   0.000000   \n",
       "\n",
       "   tfidf_233  tfidf_234  tfidf_235  tfidf_236  tfidf_237  tfidf_238  \\\n",
       "0   0.001941        0.0   0.013218   0.002106   0.001329   0.003078   \n",
       "1   0.000752        0.0   0.015484   0.000000   0.004940   0.005551   \n",
       "2   0.000611        0.0   0.013523   0.000000   0.000496   0.000905   \n",
       "3   0.001615        0.0   0.012030   0.001410   0.006509   0.004058   \n",
       "4   0.005196        0.0   0.010267   0.000000   0.000000   0.000000   \n",
       "\n",
       "   tfidf_239  tfidf_240  tfidf_241  tfidf_242  tfidf_243  tfidf_244  \\\n",
       "0   0.000000   0.007579   0.002469   0.000605   0.000671   0.000000   \n",
       "1   0.000642   0.002302   0.001203   0.003883   0.000000   0.005724   \n",
       "2   0.000000   0.001988   0.001390   0.000709   0.000000   0.000000   \n",
       "3   0.001826   0.006812   0.000000   0.000964   0.002417   0.004876   \n",
       "4   0.000000   0.003858   0.009281   0.000000   0.004815   0.004601   \n",
       "\n",
       "   tfidf_245  tfidf_246  tfidf_247  tfidf_248  tfidf_249  tfidf_250  \\\n",
       "0   0.002142   0.004326   0.002087   0.000000   0.000000   0.009057   \n",
       "1   0.000000   0.004872   0.002775   0.000000   0.001432   0.005582   \n",
       "2   0.001216   0.001918   0.000000   0.000762   0.000933   0.003307   \n",
       "3   0.004921   0.002678   0.000000   0.001025   0.002852   0.010548   \n",
       "4   0.004802   0.002904   0.007026   0.004163   0.001956   0.018084   \n",
       "\n",
       "   tfidf_251  tfidf_252  tfidf_253  tfidf_254  tfidf_255  tfidf_256  \\\n",
       "0   0.002223   0.000000   0.000817   0.004383        0.0   0.001305   \n",
       "1   0.001329   0.000000   0.000000   0.006663        0.0   0.000000   \n",
       "2   0.003305   0.000000   0.000000   0.004418        0.0   0.000000   \n",
       "3   0.000369   0.000000   0.001537   0.001200        0.0   0.000000   \n",
       "4   0.000000   0.003582   0.000000   0.002740        0.0   0.000000   \n",
       "\n",
       "   tfidf_257  tfidf_258  tfidf_259  tfidf_260  tfidf_261  tfidf_262  \\\n",
       "0        0.0        0.0   0.000689   0.002694   0.000000   0.000671   \n",
       "1        0.0        0.0   0.000612   0.001520   0.000000   0.000000   \n",
       "2        0.0        0.0   0.000000   0.001969   0.000000   0.000000   \n",
       "3        0.0        0.0   0.002942   0.000820   0.000767   0.000000   \n",
       "4        0.0        0.0   0.000000   0.012701   0.004485   0.000000   \n",
       "\n",
       "   tfidf_263  tfidf_264  tfidf_265  tfidf_266  tfidf_267  tfidf_268  \\\n",
       "0   0.000907   0.000000   0.002835   0.000000   0.000704   0.002661   \n",
       "1   0.005713   0.000000   0.003895   0.001668   0.000000   0.000943   \n",
       "2   0.000000   0.000579   0.003273   0.000000   0.000000   0.000701   \n",
       "3   0.001103   0.000000   0.003974   0.001546   0.003577   0.001484   \n",
       "4   0.004791   0.000000   0.017272   0.005880   0.000000   0.004195   \n",
       "\n",
       "   tfidf_269  tfidf_270  tfidf_271  tfidf_272  tfidf_273  tfidf_274  \\\n",
       "0   0.000890   0.000000   0.045739   0.000824        0.0   0.000000   \n",
       "1   0.000000   0.000000   0.049229   0.002351        0.0   0.000000   \n",
       "2   0.000627   0.000000   0.028199   0.002656        0.0   0.000000   \n",
       "3   0.000000   0.000000   0.030306   0.000000        0.0   0.000000   \n",
       "4   0.000000   0.006811   0.050076   0.000000        0.0   0.003423   \n",
       "\n",
       "   tfidf_275  tfidf_276  tfidf_277  tfidf_278  tfidf_279  tfidf_280  \\\n",
       "0   0.000451   0.000000   0.003744   0.014455   0.002066   0.000000   \n",
       "1   0.002491   0.000000   0.008450   0.021379   0.001469   0.000000   \n",
       "2   0.004367   0.000796   0.005545   0.018427   0.002756   0.000972   \n",
       "3   0.001133   0.000000   0.006604   0.015173   0.003399   0.000000   \n",
       "4   0.004857   0.000000   0.018191   0.021754   0.001654   0.000000   \n",
       "\n",
       "   tfidf_281  tfidf_282  tfidf_283  tfidf_284  tfidf_285  tfidf_286  \\\n",
       "0   0.000000        0.0   0.000359        0.0   0.000521   0.000806   \n",
       "1   0.000000        0.0   0.000000        0.0   0.000000   0.000000   \n",
       "2   0.000000        0.0   0.001382        0.0   0.002812   0.000000   \n",
       "3   0.000543        0.0   0.000000        0.0   0.003500   0.001001   \n",
       "4   0.000000        0.0   0.000000        0.0   0.010968   0.000000   \n",
       "\n",
       "   tfidf_287  tfidf_288  tfidf_289  tfidf_290  tfidf_291  tfidf_292  \\\n",
       "0   0.000999   0.000000   0.005920   0.000000   0.022449   0.008465   \n",
       "1   0.002761   0.001519   0.008990   0.003543   0.019376   0.004998   \n",
       "2   0.000933   0.000000   0.004892   0.002047   0.015246   0.011618   \n",
       "3   0.000591   0.000931   0.004765   0.003126   0.012758   0.010607   \n",
       "4   0.001962   0.000000   0.000000   0.004590   0.005366   0.002255   \n",
       "\n",
       "   tfidf_293  tfidf_294  tfidf_295  tfidf_296  tfidf_297  tfidf_298  \\\n",
       "0   0.000000   0.000379   0.000000        0.0   0.000609   0.002635   \n",
       "1   0.000000   0.000000   0.000000        0.0   0.001737   0.002902   \n",
       "2   0.001671   0.000000   0.001149        0.0   0.000600   0.002718   \n",
       "3   0.000000   0.000845   0.003792        0.0   0.002389   0.003749   \n",
       "4   0.000000   0.000000   0.000000        0.0   0.016397   0.013002   \n",
       "\n",
       "   tfidf_299  tfidf_300  tfidf_301  tfidf_302  tfidf_303  tfidf_304  \\\n",
       "0   0.000000   0.003431   0.010275   0.003782   0.007187   0.014550   \n",
       "1   0.000000   0.006644   0.009688   0.004744   0.010734   0.005519   \n",
       "2   0.000838   0.001977   0.010095   0.001609   0.007369   0.003927   \n",
       "3   0.000821   0.006512   0.016739   0.002691   0.006457   0.010174   \n",
       "4   0.000000   0.006899   0.018376   0.000000   0.006457   0.008659   \n",
       "\n",
       "   tfidf_305  tfidf_306  tfidf_307  tfidf_308  tfidf_309  tfidf_310  \\\n",
       "0   0.001562   0.018861   0.000000   0.007798   0.009384   0.001891   \n",
       "1   0.001419   0.019966   0.001196   0.002230   0.007872   0.001014   \n",
       "2   0.004822   0.014007   0.000569   0.002665   0.009641   0.000000   \n",
       "3   0.000410   0.014881   0.000509   0.006314   0.020264   0.002555   \n",
       "4   0.000000   0.022447   0.002811   0.022057   0.009101   0.000000   \n",
       "\n",
       "   tfidf_311  tfidf_312  tfidf_313  tfidf_314  tfidf_315  tfidf_316  \\\n",
       "0   0.003232   0.004488   0.006777   0.001874   0.015330   0.004805   \n",
       "1   0.008023   0.001023   0.005186   0.001915   0.019440   0.000000   \n",
       "2   0.002650   0.002010   0.004794   0.000000   0.011615   0.000000   \n",
       "3   0.003850   0.002652   0.002892   0.000000   0.013371   0.000594   \n",
       "4   0.000000   0.000000   0.003526   0.000000   0.015205   0.000000   \n",
       "\n",
       "   tfidf_317  tfidf_318  tfidf_319  tfidf_320  tfidf_321  tfidf_322  \\\n",
       "0   0.001079   0.002423   0.000826   0.008980   0.000364   0.001035   \n",
       "1   0.003053   0.004875   0.000000   0.008880   0.000000   0.000000   \n",
       "2   0.000471   0.004131   0.002295   0.003091   0.001321   0.001613   \n",
       "3   0.002286   0.001923   0.001464   0.012416   0.002817   0.000000   \n",
       "4   0.000000   0.000000   0.008919   0.007381   0.000000   0.000000   \n",
       "\n",
       "   tfidf_323  tfidf_324  tfidf_325  tfidf_326  tfidf_327  tfidf_328  \\\n",
       "0   0.004100   0.000000   0.000000   0.024878   0.006956   0.006308   \n",
       "1   0.001530   0.000000   0.000000   0.017043   0.004957   0.005330   \n",
       "2   0.001647   0.000000   0.001884   0.019377   0.003198   0.010383   \n",
       "3   0.004793   0.000000   0.000000   0.004553   0.013383   0.004153   \n",
       "4   0.004151   0.005504   0.004344   0.016122   0.000000   0.016326   \n",
       "\n",
       "   tfidf_329  tfidf_330  tfidf_331  tfidf_332  tfidf_333  tfidf_334  \\\n",
       "0   0.000727   0.000000   0.000000   0.020928   0.001625   0.002823   \n",
       "1   0.001415   0.001279   0.001511   0.013540   0.001097   0.000000   \n",
       "2   0.002518   0.000734   0.000370   0.020797   0.002095   0.000000   \n",
       "3   0.002044   0.000000   0.000000   0.013280   0.000676   0.000666   \n",
       "4   0.008430   0.000000   0.004940   0.012363   0.000000   0.000000   \n",
       "\n",
       "   tfidf_335  tfidf_336  tfidf_337  tfidf_338  tfidf_339  tfidf_340  \\\n",
       "0   0.013584   0.007579   0.003171   0.001208        0.0   0.012472   \n",
       "1   0.012314   0.006185   0.002411   0.003628        0.0   0.008444   \n",
       "2   0.014195   0.002730   0.002987   0.007180        0.0   0.010820   \n",
       "3   0.009003   0.012091   0.003547   0.009294        0.0   0.007343   \n",
       "4   0.013062   0.014117   0.002111   0.010173        0.0   0.009384   \n",
       "\n",
       "   tfidf_341  tfidf_342  tfidf_343  tfidf_344  tfidf_345  tfidf_346  \\\n",
       "0   0.000783   0.000000   0.000000   0.005277   0.000547   0.000000   \n",
       "1   0.000000   0.000000   0.001715   0.002673   0.000875   0.001043   \n",
       "2   0.002348   0.000000   0.000000   0.001884   0.001705   0.000620   \n",
       "3   0.000000   0.001872   0.002081   0.002014   0.000000   0.004068   \n",
       "4   0.000000   0.000000   0.034460   0.000000   0.000000   0.006980   \n",
       "\n",
       "   tfidf_347  tfidf_348  tfidf_349  tfidf_350  tfidf_351  tfidf_352  \\\n",
       "0   0.002253   0.000516   0.001893   0.003106   0.001365        0.0   \n",
       "1   0.002056   0.001088   0.001422   0.001452   0.001818        0.0   \n",
       "2   0.003444   0.000000   0.003730   0.004027   0.000000        0.0   \n",
       "3   0.004030   0.000535   0.002347   0.005335   0.000000        0.0   \n",
       "4   0.006848   0.000000   0.004176   0.006021   0.000000        0.0   \n",
       "\n",
       "   tfidf_353  tfidf_354  tfidf_355  tfidf_356  tfidf_357  tfidf_358  \\\n",
       "0   0.002756   0.001625   0.000000   0.003002   0.027198   0.002826   \n",
       "1   0.003751   0.002706   0.000000   0.005467   0.029735   0.003531   \n",
       "2   0.000410   0.000000   0.001087   0.004031   0.045057   0.001570   \n",
       "3   0.002245   0.000618   0.000672   0.005034   0.027451   0.005822   \n",
       "4   0.002346   0.000000   0.000000   0.000000   0.002780   0.006407   \n",
       "\n",
       "   tfidf_359  tfidf_360  tfidf_361  tfidf_362  tfidf_363  tfidf_364  \\\n",
       "0   0.002773   0.000904   0.003727   0.000756   0.001796        0.0   \n",
       "1   0.002281   0.000790   0.004623   0.000000   0.003673        0.0   \n",
       "2   0.000969   0.000000   0.001570   0.000471   0.001645        0.0   \n",
       "3   0.001969   0.003942   0.009124   0.000000   0.004199        0.0   \n",
       "4   0.004960   0.000000   0.013160   0.000000   0.000000        0.0   \n",
       "\n",
       "   tfidf_365  tfidf_366  tfidf_367  tfidf_368  tfidf_369  tfidf_370  \\\n",
       "0   0.001259        0.0   0.000000   0.014780        0.0   0.014602   \n",
       "1   0.000000        0.0   0.001389   0.011601        0.0   0.020890   \n",
       "2   0.000000        0.0   0.001356   0.000484        0.0   0.004627   \n",
       "3   0.000626        0.0   0.001860   0.005822        0.0   0.013950   \n",
       "4   0.000000        0.0   0.000000   0.000000        0.0   0.000000   \n",
       "\n",
       "   tfidf_371  tfidf_372  tfidf_373  tfidf_374  tfidf_375  tfidf_376  \\\n",
       "0   0.000941   0.000619   0.001115   0.019796   0.013531   0.004797   \n",
       "1   0.000000   0.000000   0.007103   0.021087   0.009630   0.006723   \n",
       "2   0.000000   0.000705   0.003619   0.015956   0.004987   0.006775   \n",
       "3   0.000561   0.001579   0.010832   0.021254   0.005372   0.004297   \n",
       "4   0.000000   0.000000   0.000000   0.023049   0.006752   0.000000   \n",
       "\n",
       "   tfidf_377  tfidf_378  tfidf_379  tfidf_380  tfidf_381  tfidf_382  \\\n",
       "0   0.004706   0.001925   0.004835   0.016097   0.000881   0.007084   \n",
       "1   0.002393   0.003111   0.001006   0.018303   0.000000   0.006393   \n",
       "2   0.000442   0.000601   0.002970   0.012559   0.001822   0.005647   \n",
       "3   0.002154   0.000000   0.001129   0.011097   0.005151   0.004732   \n",
       "4   0.000000   0.000000   0.000000   0.003097   0.000000   0.000000   \n",
       "\n",
       "   tfidf_383  tfidf_384  tfidf_385  tfidf_386  tfidf_387  tfidf_388  \\\n",
       "0   0.024283   0.000000   0.001890   0.007553        0.0   0.000948   \n",
       "1   0.036245   0.003026   0.007259   0.002494        0.0   0.000000   \n",
       "2   0.036420   0.000000   0.003211   0.001634        0.0   0.000369   \n",
       "3   0.037640   0.000944   0.006840   0.004032        0.0   0.000000   \n",
       "4   0.031901   0.009682   0.005633   0.000000        0.0   0.002192   \n",
       "\n",
       "   tfidf_389  tfidf_390  tfidf_391  tfidf_392  tfidf_393  tfidf_394  \\\n",
       "0   0.039577   0.000828   0.001327   0.000000   0.000806   0.000000   \n",
       "1   0.041488   0.000000   0.002129   0.000000   0.000000   0.000000   \n",
       "2   0.035503   0.000614   0.000997   0.000477   0.002945   0.000000   \n",
       "3   0.074520   0.001373   0.003333   0.000000   0.000000   0.000000   \n",
       "4   0.020400   0.000000   0.000000   0.006852   0.004410   0.004243   \n",
       "\n",
       "   tfidf_395  tfidf_396  tfidf_397  tfidf_398  tfidf_399  tfidf_400  \\\n",
       "0   0.002936   0.011937   0.000947   0.000000   0.000000   0.000363   \n",
       "1   0.000907   0.010150   0.000000   0.000802   0.000000   0.002718   \n",
       "2   0.003277   0.006948   0.000000   0.000000   0.002483   0.006854   \n",
       "3   0.002876   0.005588   0.001470   0.000000   0.012449   0.000000   \n",
       "4   0.000000   0.000000   0.002177   0.002279   0.000000   0.000000   \n",
       "\n",
       "   tfidf_401  tfidf_402  tfidf_403  tfidf_404  tfidf_405  tfidf_406  \\\n",
       "0        0.0        0.0   0.002699   0.002875   0.000000   0.007170   \n",
       "1        0.0        0.0   0.000000   0.003179   0.001535   0.004628   \n",
       "2        0.0        0.0   0.000975   0.002521   0.000000   0.002885   \n",
       "3        0.0        0.0   0.000582   0.000831   0.000000   0.002412   \n",
       "4        0.0        0.0   0.000000   0.015723   0.000000   0.003763   \n",
       "\n",
       "   tfidf_407  tfidf_408  tfidf_409  tfidf_410  tfidf_411  tfidf_412  \\\n",
       "0   0.000904   0.003059   0.000000   0.002532   0.000000   0.000792   \n",
       "1   0.000000   0.000000   0.001199   0.001973   0.002623   0.000939   \n",
       "2   0.001133   0.000564   0.000000   0.001354   0.000626   0.003224   \n",
       "3   0.001031   0.000000   0.002058   0.000654   0.000000   0.000000   \n",
       "4   0.004503   0.000000   0.002531   0.000000   0.000000   0.008584   \n",
       "\n",
       "   tfidf_413  tfidf_414  tfidf_415  tfidf_416  tfidf_417  tfidf_418  \\\n",
       "0        0.0   0.016530   0.012187   0.041349   0.002648   0.003625   \n",
       "1        0.0   0.015178   0.006727   0.032526   0.003503   0.000000   \n",
       "2        0.0   0.018147   0.008281   0.030449   0.002554   0.000000   \n",
       "3        0.0   0.008441   0.007312   0.041704   0.000668   0.000468   \n",
       "4        0.0   0.008717   0.000000   0.016062   0.000000   0.000000   \n",
       "\n",
       "   tfidf_419  tfidf_420  tfidf_421  tfidf_422  tfidf_423  tfidf_424  \\\n",
       "0   0.000000   0.002270   0.002286   0.018022   0.000938   0.034321   \n",
       "1   0.000643   0.000000   0.000000   0.022932   0.004623   0.054977   \n",
       "2   0.000877   0.003145   0.000000   0.012475   0.000886   0.035910   \n",
       "3   0.000000   0.003352   0.000590   0.017922   0.004134   0.038085   \n",
       "4   0.000000   0.004357   0.000000   0.027203   0.000000   0.013246   \n",
       "\n",
       "   tfidf_425  tfidf_426  tfidf_427  tfidf_428  tfidf_429  tfidf_430  \\\n",
       "0   0.000925        0.0   0.005008   0.006044   0.007928   0.019031   \n",
       "1   0.000000        0.0   0.004465   0.009016   0.009865   0.013540   \n",
       "2   0.001618        0.0   0.002999   0.011842   0.013160   0.027903   \n",
       "3   0.000479        0.0   0.005227   0.014970   0.018456   0.079204   \n",
       "4   0.000000        0.0   0.000000   0.000000   0.006555   0.016727   \n",
       "\n",
       "   tfidf_431  tfidf_432  tfidf_433  tfidf_434  tfidf_435  tfidf_436  \\\n",
       "0        0.0   0.005269   0.003707   0.048370   0.004751   0.002219   \n",
       "1        0.0   0.002527   0.000000   0.060536   0.004415   0.003259   \n",
       "2        0.0   0.004254   0.001798   0.055173   0.001250   0.000000   \n",
       "3        0.0   0.001162   0.002840   0.000000   0.003063   0.000605   \n",
       "4        0.0   0.000000   0.000000   0.006614   0.007228   0.001737   \n",
       "\n",
       "   tfidf_437  tfidf_438  tfidf_439  tfidf_440  tfidf_441  tfidf_442  \\\n",
       "0   0.000774   0.001757   0.038059   0.001059   0.000000   0.000772   \n",
       "1   0.000904   0.000000   0.042990   0.001235   0.005204   0.003474   \n",
       "2   0.001446   0.000641   0.040066   0.004022   0.000000   0.000669   \n",
       "3   0.001139   0.000000   0.048434   0.001234   0.000374   0.001556   \n",
       "4   0.000000   0.002428   0.014432   0.008720   0.000000   0.006448   \n",
       "\n",
       "   tfidf_443  tfidf_444  tfidf_445  tfidf_446  tfidf_447  tfidf_448  \\\n",
       "0   0.057165   0.001895        0.0   0.000000   0.001792   0.000927   \n",
       "1   0.061654   0.001509        0.0   0.001592   0.000000   0.001567   \n",
       "2   0.032210   0.000000        0.0   0.006548   0.000000   0.000000   \n",
       "3   0.000000   0.000691        0.0   0.006027   0.001811   0.002290   \n",
       "4   0.001717   0.000000        0.0   0.000000   0.000000   0.000000   \n",
       "\n",
       "   tfidf_449  tfidf_450  tfidf_451  tfidf_452  tfidf_453  tfidf_454  \\\n",
       "0   0.000961   0.003410   0.003055   0.001578   0.001841   0.005379   \n",
       "1   0.002056   0.000000   0.004195   0.000767   0.002002   0.004957   \n",
       "2   0.000780   0.003142   0.000000   0.000000   0.000000   0.002987   \n",
       "3   0.001908   0.000443   0.000000   0.001600   0.004977   0.002209   \n",
       "4   0.003754   0.008558   0.000000   0.000000   0.000000   0.003714   \n",
       "\n",
       "   tfidf_455  tfidf_456  tfidf_457  tfidf_458  tfidf_459  tfidf_460  \\\n",
       "0   0.003832   0.015707   0.004448   0.000851   0.000000   0.005311   \n",
       "1   0.004043   0.010654   0.009180   0.000000   0.001873   0.000000   \n",
       "2   0.000000   0.001686   0.002066   0.000000   0.001010   0.001269   \n",
       "3   0.001440   0.011270   0.004584   0.000000   0.001944   0.018061   \n",
       "4   0.002061   0.006572   0.002737   0.000000   0.006635   0.007800   \n",
       "\n",
       "   tfidf_461  tfidf_462  tfidf_463  tfidf_464  tfidf_465  tfidf_466  \\\n",
       "0   0.000000   0.013329   0.015892   0.000000   0.000000   0.000000   \n",
       "1   0.001021   0.021090   0.007814   0.003461   0.000000   0.002099   \n",
       "2   0.000000   0.007361   0.012205   0.001568   0.000000   0.000000   \n",
       "3   0.000000   0.012047   0.005399   0.001491   0.001559   0.001705   \n",
       "4   0.000000   0.017244   0.007407   0.011637   0.002647   0.002976   \n",
       "\n",
       "   tfidf_467  tfidf_468  tfidf_469  tfidf_470  tfidf_471  tfidf_472  \\\n",
       "0        0.0        0.0        0.0        0.0   0.000000   0.000000   \n",
       "1        0.0        0.0        0.0        0.0   0.000000   0.000000   \n",
       "2        0.0        0.0        0.0        0.0   0.000000   0.000000   \n",
       "3        0.0        0.0        0.0        0.0   0.000000   0.000599   \n",
       "4        0.0        0.0        0.0        0.0   0.004627   0.000000   \n",
       "\n",
       "   tfidf_473  tfidf_474  tfidf_475  tfidf_476  tfidf_477  tfidf_478  \\\n",
       "0   0.001498   0.000000        0.0        0.0   0.000819   0.000000   \n",
       "1   0.000810   0.001392        0.0        0.0   0.000000   0.003794   \n",
       "2   0.000000   0.000763        0.0        0.0   0.000000   0.001813   \n",
       "3   0.000887   0.002102        0.0        0.0   0.000000   0.002236   \n",
       "4   0.000000   0.000000        0.0        0.0   0.000000   0.005565   \n",
       "\n",
       "   tfidf_479  tfidf_480  tfidf_481  tfidf_482  tfidf_483  tfidf_484  \\\n",
       "0        0.0   0.001543        0.0        0.0   0.000660   0.000000   \n",
       "1        0.0   0.003802        0.0        0.0   0.001085   0.001007   \n",
       "2        0.0   0.001393        0.0        0.0   0.003244   0.000000   \n",
       "3        0.0   0.002086        0.0        0.0   0.000399   0.000402   \n",
       "4        0.0   0.000000        0.0        0.0   0.009495   0.002855   \n",
       "\n",
       "   tfidf_485  tfidf_486  tfidf_487  tfidf_488  tfidf_489  tfidf_490  \\\n",
       "0   0.000000        0.0   0.019116   0.000000   0.008994   0.005677   \n",
       "1   0.000000        0.0   0.022496   0.004789   0.007308   0.001956   \n",
       "2   0.000661        0.0   0.020446   0.000000   0.004549   0.001550   \n",
       "3   0.003053        0.0   0.019836   0.002768   0.008690   0.001295   \n",
       "4   0.004459        0.0   0.004388   0.005249   0.009632   0.000000   \n",
       "\n",
       "   tfidf_491  tfidf_492  tfidf_493  tfidf_494  tfidf_495  tfidf_496  \\\n",
       "0   0.004037   0.002353   0.002375   0.001992   0.003497   0.000000   \n",
       "1   0.005190   0.003481   0.000881   0.001237   0.006767   0.001356   \n",
       "2   0.003643   0.000000   0.001050   0.002546   0.003972   0.002965   \n",
       "3   0.007993   0.003276   0.008224   0.005912   0.005882   0.001074   \n",
       "4   0.003343   0.000000   0.003887   0.000000   0.004807   0.002597   \n",
       "\n",
       "   tfidf_497  tfidf_498  tfidf_499  tfidf_500  tfidf_501  tfidf_502  \\\n",
       "0   0.000760   0.000000   0.003264   0.000689   0.000000   0.005194   \n",
       "1   0.000000   0.000000   0.003468   0.001049   0.001991   0.001672   \n",
       "2   0.002506   0.001585   0.000000   0.000000   0.001025   0.001483   \n",
       "3   0.002874   0.001059   0.001973   0.001843   0.005164   0.003554   \n",
       "4   0.002113   0.000000   0.002919   0.000000   0.002248   0.007790   \n",
       "\n",
       "   tfidf_503  tfidf_504  tfidf_505  tfidf_506  tfidf_507  tfidf_508  \\\n",
       "0   0.003310   0.000000    0.00000        0.0   0.002645        0.0   \n",
       "1   0.002075   0.000000    0.00000        0.0   0.002146        0.0   \n",
       "2   0.004820   0.000000    0.00000        0.0   0.005672        0.0   \n",
       "3   0.004231   0.002114    0.00000        0.0   0.002215        0.0   \n",
       "4   0.003496   0.000000    0.00464        0.0   0.000000        0.0   \n",
       "\n",
       "   tfidf_509  tfidf_510  tfidf_511  tfidf_512  tfidf_513  tfidf_514  \\\n",
       "0   0.000570   0.012091   0.001999        0.0   0.000000   0.003145   \n",
       "1   0.000977   0.012745   0.002461        0.0   0.000000   0.012767   \n",
       "2   0.001057   0.018914   0.006411        0.0   0.002464   0.000576   \n",
       "3   0.001072   0.012161   0.001010        0.0   0.002026   0.002649   \n",
       "4   0.005778   0.009560   0.000000        0.0   0.003983   0.005187   \n",
       "\n",
       "   tfidf_515  tfidf_516  tfidf_517  tfidf_518  tfidf_519  tfidf_520  \\\n",
       "0   0.000000   0.000922   0.000000   0.006958   0.003197        0.0   \n",
       "1   0.000000   0.001435   0.000000   0.012807   0.002158        0.0   \n",
       "2   0.000902   0.001686   0.000567   0.007224   0.003827        0.0   \n",
       "3   0.000000   0.001634   0.000000   0.001438   0.000000        0.0   \n",
       "4   0.000000   0.000000   0.000000   0.008951   0.002550        0.0   \n",
       "\n",
       "   tfidf_521  tfidf_522  tfidf_523  tfidf_524  tfidf_525  tfidf_526  \\\n",
       "0   0.000000   0.000000   0.010212   0.008217   0.014524        0.0   \n",
       "1   0.001666   0.000000   0.012985   0.007251   0.018036        0.0   \n",
       "2   0.002203   0.000000   0.004619   0.008440   0.021925        0.0   \n",
       "3   0.000742   0.002857   0.013442   0.008651   0.024777        0.0   \n",
       "4   0.008450   0.000000   0.006123   0.015038   0.013464        0.0   \n",
       "\n",
       "   tfidf_527  tfidf_528  tfidf_529  tfidf_530  tfidf_531  tfidf_532  \\\n",
       "0   0.000000   0.004059   0.002481   0.002439   0.000815   0.003057   \n",
       "1   0.000000   0.001062   0.000531   0.000685   0.002403   0.009342   \n",
       "2   0.000552   0.004131   0.000000   0.002481   0.005305   0.002636   \n",
       "3   0.001137   0.004181   0.002010   0.003501   0.003594   0.005468   \n",
       "4   0.004723   0.003482   0.001813   0.003832   0.003298   0.004672   \n",
       "\n",
       "   tfidf_533  tfidf_534  tfidf_535  tfidf_536  tfidf_537  tfidf_538  \\\n",
       "0   0.000682   0.007971   0.007994   0.000507   0.008607   0.005935   \n",
       "1   0.000000   0.004720   0.004513   0.000000   0.008432   0.014440   \n",
       "2   0.000000   0.005195   0.003232   0.000589   0.003898   0.002709   \n",
       "3   0.000620   0.006804   0.007556   0.000000   0.000409   0.003634   \n",
       "4   0.000000   0.003639   0.000000   0.000000   0.005551   0.014045   \n",
       "\n",
       "   tfidf_539  tfidf_540  tfidf_541  tfidf_542  tfidf_543  tfidf_544  \\\n",
       "0   0.049631   0.013614   0.000612   0.002760   0.000000   0.019423   \n",
       "1   0.070858   0.015088   0.000970   0.000000   0.000000   0.017089   \n",
       "2   0.027811   0.000000   0.000740   0.002678   0.001040   0.014875   \n",
       "3   0.024853   0.000396   0.000798   0.002058   0.000000   0.015487   \n",
       "4   0.010617   0.046717   0.000000   0.000000   0.002483   0.012499   \n",
       "\n",
       "   tfidf_545  tfidf_546  tfidf_547  tfidf_548  tfidf_549  tfidf_550  \\\n",
       "0        0.0   0.003280   0.000996   0.000776   0.008311   0.001332   \n",
       "1        0.0   0.000000   0.000000   0.000000   0.012521   0.002799   \n",
       "2        0.0   0.000000   0.000000   0.000000   0.012661   0.000690   \n",
       "3        0.0   0.006248   0.000000   0.000802   0.009703   0.001295   \n",
       "4        0.0   0.003079   0.000000   0.000000   0.006500   0.008225   \n",
       "\n",
       "   tfidf_551  tfidf_552  tfidf_553  tfidf_554  tfidf_555  tfidf_556  \\\n",
       "0   0.002997        0.0   0.002141   0.005055   0.006526    0.00000   \n",
       "1   0.004634        0.0   0.002186   0.002195   0.007350    0.00000   \n",
       "2   0.002396        0.0   0.002979   0.000000   0.000000    0.26798   \n",
       "3   0.000000        0.0   0.010385   0.000000   0.000000    0.00000   \n",
       "4   0.007049        0.0   0.007440   0.000000   0.002040    0.00000   \n",
       "\n",
       "   tfidf_557  tfidf_558  tfidf_559  tfidf_560  tfidf_561  tfidf_562  \\\n",
       "0        0.0        0.0   0.000000   0.001215   0.001479   0.002905   \n",
       "1        0.0        0.0   0.000000   0.000000   0.000000   0.001142   \n",
       "2        0.0        0.0   0.001219   0.000000   0.001374   0.002082   \n",
       "3        0.0        0.0   0.000000   0.000000   0.000000   0.000857   \n",
       "4        0.0        0.0   0.000000   0.000000   0.004370   0.002014   \n",
       "\n",
       "   tfidf_563  tfidf_564  tfidf_565  tfidf_566  tfidf_567  tfidf_568  \\\n",
       "0        0.0   0.000000   0.001422   0.002979        0.0   0.003166   \n",
       "1        0.0   0.000000   0.008577   0.001213        0.0   0.004989   \n",
       "2        0.0   0.003444   0.002878   0.001458        0.0   0.002585   \n",
       "3        0.0   0.000000   0.005744   0.001805        0.0   0.005277   \n",
       "4        0.0   0.000000   0.010672   0.000000        0.0   0.000000   \n",
       "\n",
       "   tfidf_569  tfidf_570  tfidf_571  tfidf_572  tfidf_573  tfidf_574  \\\n",
       "0        0.0        0.0        0.0   0.000000   0.002453   0.000468   \n",
       "1        0.0        0.0        0.0   0.000000   0.000891   0.001869   \n",
       "2        0.0        0.0        0.0   0.000659   0.002986   0.000000   \n",
       "3        0.0        0.0        0.0   0.001086   0.002684   0.001466   \n",
       "4        0.0        0.0        0.0   0.000000   0.006503   0.004210   \n",
       "\n",
       "   tfidf_575  tfidf_576  tfidf_577  tfidf_578  tfidf_579  tfidf_580  \\\n",
       "0   0.006251   0.000000   0.000000   0.008658   0.013068   0.001557   \n",
       "1   0.009126   0.000000   0.001824   0.006228   0.007355   0.003692   \n",
       "2   0.007660   0.000693   0.002126   0.006616   0.018640   0.003531   \n",
       "3   0.003111   0.002039   0.004634   0.005562   0.002520   0.000000   \n",
       "4   0.026371   0.005539   0.014350   0.003254   0.007773   0.000000   \n",
       "\n",
       "   tfidf_581  tfidf_582  tfidf_583  tfidf_584  tfidf_585  tfidf_586  \\\n",
       "0   0.001480   0.002775   0.000000   0.000000   0.004888   0.013279   \n",
       "1   0.000000   0.000948   0.001679   0.000000   0.013498   0.009697   \n",
       "2   0.000000   0.001224   0.005501   0.000000   0.010893   0.009648   \n",
       "3   0.007260   0.001294   0.001646   0.000947   0.007706   0.006902   \n",
       "4   0.003477   0.007253   0.002851   0.000000   0.001451   0.019666   \n",
       "\n",
       "   tfidf_587  tfidf_588  tfidf_589  tfidf_590  tfidf_591  tfidf_592  \\\n",
       "0   0.001661   0.002316   0.000000   0.006504   0.006284   0.002127   \n",
       "1   0.000000   0.000000   0.000000   0.009561   0.005705   0.002818   \n",
       "2   0.000559   0.000000   0.004337   0.011451   0.003553   0.001372   \n",
       "3   0.000411   0.000000   0.001542   0.003639   0.006874   0.000000   \n",
       "4   0.000000   0.004782   0.004589   0.014959   0.015281   0.000000   \n",
       "\n",
       "   tfidf_593  tfidf_594  tfidf_595  tfidf_596  tfidf_597  tfidf_598  \\\n",
       "0   0.003583   0.003755   0.009764   0.001425   0.017679   0.006740   \n",
       "1   0.001823   0.004454   0.015773   0.000762   0.012508   0.005125   \n",
       "2   0.000000   0.006916   0.010704   0.000975   0.011283   0.006528   \n",
       "3   0.000000   0.008999   0.008619   0.007124   0.005642   0.006564   \n",
       "4   0.000000   0.010766   0.013034   0.000000   0.007493   0.000000   \n",
       "\n",
       "   tfidf_599  tfidf_600  tfidf_601  tfidf_602  tfidf_603  tfidf_604  \\\n",
       "0        0.0   0.006857   0.003079   0.001902        0.0   0.033932   \n",
       "1        0.0   0.001218   0.005090   0.004816        0.0   0.045140   \n",
       "2        0.0   0.003658   0.005469   0.003784        0.0   0.061734   \n",
       "3        0.0   0.002029   0.003306   0.008390        0.0   0.037967   \n",
       "4        0.0   0.001897   0.007503   0.006091        0.0   0.047989   \n",
       "\n",
       "   tfidf_605  tfidf_606  tfidf_607  tfidf_608  tfidf_609  tfidf_610  \\\n",
       "0   0.001526   0.004823   0.001926   0.005026   0.001230   0.000000   \n",
       "1   0.000000   0.006786   0.000000   0.008136   0.000000   0.002990   \n",
       "2   0.000000   0.005876   0.000531   0.006394   0.000795   0.000842   \n",
       "3   0.000000   0.010928   0.000000   0.007639   0.000000   0.000000   \n",
       "4   0.003889   0.005261   0.002347   0.011055   0.006943   0.005173   \n",
       "\n",
       "   tfidf_611  tfidf_612  tfidf_613  tfidf_614  tfidf_615  tfidf_616  \\\n",
       "0   0.001252   0.000000   0.003217    0.00100   0.002466   0.002833   \n",
       "1   0.000000   0.000000   0.001118    0.00000   0.001393   0.003236   \n",
       "2   0.001111   0.000665   0.003582    0.00275   0.004882   0.006716   \n",
       "3   0.000537   0.002194   0.004082    0.00000   0.001379   0.009023   \n",
       "4   0.000000   0.000000   0.000000    0.00000   0.001925   0.000000   \n",
       "\n",
       "   tfidf_617  tfidf_618  tfidf_619  tfidf_620  tfidf_621  tfidf_622  \\\n",
       "0   0.000688   0.000000   0.000000   0.001506   0.008474   0.003454   \n",
       "1   0.001321   0.000000   0.000000   0.000906   0.008930   0.003924   \n",
       "2   0.003456   0.009248   0.002205   0.001281   0.013655   0.002853   \n",
       "3   0.000000   0.002049   0.000000   0.003346   0.012235   0.005113   \n",
       "4   0.000000   0.007538   0.000000   0.000000   0.010312   0.004448   \n",
       "\n",
       "   tfidf_623  tfidf_624  tfidf_625  tfidf_626  tfidf_627  tfidf_628  \\\n",
       "0   0.000000   0.002298   0.001449   0.000885   0.000000   0.004855   \n",
       "1   0.000000   0.005233   0.003298   0.001919   0.000000   0.007743   \n",
       "2   0.001228   0.001850   0.001913   0.004238   0.000000   0.000635   \n",
       "3   0.000000   0.004751   0.000473   0.002122   0.001054   0.002274   \n",
       "4   0.000000   0.005768   0.009963   0.000000   0.000000   0.004946   \n",
       "\n",
       "   tfidf_629  tfidf_630  tfidf_631  tfidf_632  tfidf_633  tfidf_634  \\\n",
       "0   0.000715   0.002536   0.000000   0.000000   0.000000   0.000668   \n",
       "1   0.000000   0.006804   0.000000   0.000000   0.000713   0.000000   \n",
       "2   0.000000   0.007353   0.000000   0.000000   0.000000   0.000915   \n",
       "3   0.001868   0.009521   0.001133   0.005210   0.001290   0.002790   \n",
       "4   0.000000   0.012533   0.000000   0.003032   0.003043   0.015214   \n",
       "\n",
       "   tfidf_635  tfidf_636  tfidf_637  tfidf_638  tfidf_639  tfidf_640  \\\n",
       "0   0.002336   0.000000   0.000000   0.000000        0.0        0.0   \n",
       "1   0.003108   0.000000   0.003647   0.000000        0.0        0.0   \n",
       "2   0.005204   0.000000   0.001839   0.000000        0.0        0.0   \n",
       "3   0.003420   0.001182   0.000000   0.000486        0.0        0.0   \n",
       "4   0.000000   0.000000   0.000000   0.000000        0.0        0.0   \n",
       "\n",
       "   tfidf_641  tfidf_642  tfidf_643  tfidf_644  tfidf_645  tfidf_646  \\\n",
       "0        0.0   0.000717        0.0   0.000000   0.005267   0.020704   \n",
       "1        0.0   0.000000        0.0   0.000000   0.000000   0.033228   \n",
       "2        0.0   0.000000        0.0   0.000775   0.000783   0.022576   \n",
       "3        0.0   0.000887        0.0   0.002199   0.003807   0.001181   \n",
       "4        0.0   0.000000        0.0   0.005230   0.000000   0.008782   \n",
       "\n",
       "   tfidf_647  tfidf_648  tfidf_649  tfidf_650  tfidf_651  tfidf_652  \\\n",
       "0   0.016437   0.002212   0.003912   0.000654   0.000000        0.0   \n",
       "1   0.011650   0.004521   0.005443   0.000000   0.000000        0.0   \n",
       "2   0.016215   0.001868   0.009500   0.002243   0.000000        0.0   \n",
       "3   0.012873   0.004545   0.010004   0.001015   0.000000        0.0   \n",
       "4   0.044561   0.003743   0.000000   0.000000   0.003941        0.0   \n",
       "\n",
       "   tfidf_653  tfidf_654  tfidf_655  tfidf_656  tfidf_657  tfidf_658  \\\n",
       "0   0.000000   0.000000   0.002342   0.001512   0.001212   0.001412   \n",
       "1   0.000000   0.000000   0.000926   0.000000   0.000000   0.001826   \n",
       "2   0.002317   0.000528   0.002396   0.000712   0.000000   0.000574   \n",
       "3   0.001526   0.001323   0.003189   0.000000   0.000000   0.000532   \n",
       "4   0.004441   0.004532   0.000000   0.000000   0.000000   0.007497   \n",
       "\n",
       "   tfidf_659  tfidf_660  tfidf_661  tfidf_662  tfidf_663  tfidf_664  \\\n",
       "0   0.000000   0.033349   0.016669   0.005588   0.004099   0.007821   \n",
       "1   0.000000   0.034553   0.008325   0.003742   0.002205   0.011425   \n",
       "2   0.000000   0.038881   0.004799   0.001788   0.001239   0.011598   \n",
       "3   0.000616   0.045174   0.010622   0.000825   0.000863   0.006627   \n",
       "4   0.000000   0.044199   0.014270   0.007144   0.007072   0.000000   \n",
       "\n",
       "   tfidf_665  tfidf_666  tfidf_667  tfidf_668  tfidf_669  tfidf_670  \\\n",
       "0   0.000000   0.000649   0.003375   0.001645   0.000000   0.000000   \n",
       "1   0.001334   0.005766   0.003544   0.002174   0.001433   0.000000   \n",
       "2   0.000621   0.000717   0.001132   0.003702   0.000000   0.002045   \n",
       "3   0.000000   0.010829   0.006512   0.002423   0.001014   0.001578   \n",
       "4   0.002220   0.000000   0.021047   0.000000   0.004674   0.000000   \n",
       "\n",
       "   tfidf_671  tfidf_672  tfidf_673  tfidf_674  tfidf_675  tfidf_676  \\\n",
       "0   0.000000   0.002679   0.000000   0.004063   0.000000   0.000000   \n",
       "1   0.000000   0.002185   0.000000   0.000000   0.000000   0.000000   \n",
       "2   0.000748   0.001375   0.000000   0.001315   0.001257   0.000000   \n",
       "3   0.001359   0.004436   0.001468   0.000000   0.004950   0.001067   \n",
       "4   0.000000   0.000000   0.000000   0.000000   0.000000   0.004362   \n",
       "\n",
       "   tfidf_677  tfidf_678  tfidf_679  tfidf_680  tfidf_681  tfidf_682  \\\n",
       "0   0.002681   0.000000   0.000000   0.003643   0.001505   0.000000   \n",
       "1   0.000000   0.000791   0.002385   0.002657   0.001547   0.000000   \n",
       "2   0.004679   0.001813   0.000000   0.000536   0.001016   0.003452   \n",
       "3   0.000714   0.000589   0.001024   0.001772   0.001786   0.001510   \n",
       "4   0.007320   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "   tfidf_683  tfidf_684  tfidf_685  tfidf_686  tfidf_687  tfidf_688  \\\n",
       "0   0.005351   0.000904   0.000000   0.000000   0.000000   0.001538   \n",
       "1   0.001811   0.000937   0.001590   0.001701   0.000000   0.002687   \n",
       "2   0.001836   0.003139   0.000000   0.000000   0.000000   0.000558   \n",
       "3   0.001542   0.000000   0.000915   0.000000   0.001369   0.004287   \n",
       "4   0.005517   0.000000   0.011183   0.000000   0.000000   0.000000   \n",
       "\n",
       "   tfidf_689  tfidf_690  tfidf_691  tfidf_692  tfidf_693  tfidf_694  \\\n",
       "0   0.003577   0.008130   0.001434   0.003719   0.000853   0.000000   \n",
       "1   0.001061   0.014221   0.000000   0.007539   0.000000   0.001690   \n",
       "2   0.001185   0.004634   0.000882   0.000579   0.000586   0.003185   \n",
       "3   0.002421   0.011554   0.003608   0.001212   0.000000   0.001030   \n",
       "4   0.000000   0.012044   0.006913   0.000000   0.000000   0.008109   \n",
       "\n",
       "   tfidf_695  tfidf_696  tfidf_697  tfidf_698  tfidf_699  tfidf_700  \\\n",
       "0   0.012712   0.002520   0.003720   0.002876   0.000902   0.000719   \n",
       "1   0.014994   0.000000   0.001362   0.001581   0.000000   0.000000   \n",
       "2   0.007291   0.004935   0.000000   0.000000   0.000000   0.000697   \n",
       "3   0.005831   0.000994   0.001883   0.000000   0.000000   0.002451   \n",
       "4   0.017894   0.014834   0.001984   0.000000   0.000000   0.000000   \n",
       "\n",
       "   tfidf_701  tfidf_702  tfidf_703  tfidf_704  tfidf_705  tfidf_706  \\\n",
       "0   0.003099   0.004675   0.005335   0.000530   0.016694   0.003115   \n",
       "1   0.000000   0.002043   0.001984   0.000805   0.015675   0.008857   \n",
       "2   0.000606   0.005959   0.001978   0.000000   0.018778   0.010084   \n",
       "3   0.001009   0.004648   0.003948   0.003070   0.025014   0.006446   \n",
       "4   0.000000   0.005180   0.001353   0.003327   0.006174   0.013302   \n",
       "\n",
       "   tfidf_707  tfidf_708  tfidf_709  tfidf_710  tfidf_711  tfidf_712  \\\n",
       "0   0.002073   0.002365   0.000000   0.000000   0.035358   0.000000   \n",
       "1   0.000000   0.003907   0.000726   0.001859   0.028585   0.001599   \n",
       "2   0.000700   0.005126   0.003483   0.003927   0.030582   0.003560   \n",
       "3   0.000000   0.000745   0.004516   0.004963   0.035499   0.000000   \n",
       "4   0.000000   0.006184   0.000000   0.000000   0.025875   0.000000   \n",
       "\n",
       "   tfidf_713  tfidf_714  tfidf_715  tfidf_716  tfidf_717  tfidf_718  \\\n",
       "0   0.001527   0.030057   0.003647   0.008707    0.00000   0.000651   \n",
       "1   0.000000   0.025960   0.009175   0.007191    0.00000   0.001361   \n",
       "2   0.000480   0.028981   0.000625   0.008541    0.00116   0.001509   \n",
       "3   0.002239   0.028608   0.005615   0.008033    0.00000   0.001907   \n",
       "4   0.003478   0.036332   0.000000   0.021223    0.00000   0.002370   \n",
       "\n",
       "   tfidf_719  tfidf_720  tfidf_721  tfidf_722  tfidf_723  tfidf_724  \\\n",
       "0   0.000695   0.002123   0.001964   0.002076   0.001071   0.003976   \n",
       "1   0.002964   0.004766   0.005619   0.000000   0.000000   0.000000   \n",
       "2   0.000000   0.001616   0.000549   0.000807   0.000000   0.001286   \n",
       "3   0.000000   0.001771   0.003480   0.000000   0.001059   0.000000   \n",
       "4   0.000000   0.000000   0.000000   0.000000   0.006347   0.000000   \n",
       "\n",
       "   tfidf_725  tfidf_726  tfidf_727  tfidf_728  tfidf_729  tfidf_730  \\\n",
       "0   0.000825   0.000953   0.000000   0.000000        0.0   0.000000   \n",
       "1   0.003557   0.000000   0.000000   0.000000        0.0   0.002942   \n",
       "2   0.002755   0.001207   0.000000   0.000000        0.0   0.000886   \n",
       "3   0.000694   0.005093   0.001154   0.000572        0.0   0.000000   \n",
       "4   0.000000   0.000000   0.000000   0.000000        0.0   0.003260   \n",
       "\n",
       "   tfidf_731  tfidf_732  tfidf_733  tfidf_734  tfidf_735  tfidf_736  \\\n",
       "0   0.000000   0.003949   0.000000   0.006140   0.004500   0.000000   \n",
       "1   0.000000   0.002327   0.001065   0.004605   0.001968   0.002931   \n",
       "2   0.001392   0.002635   0.000000   0.002913   0.002102   0.000000   \n",
       "3   0.000843   0.002731   0.004147   0.000818   0.003564   0.000000   \n",
       "4   0.000000   0.002001   0.000000   0.001237   0.005678   0.000000   \n",
       "\n",
       "   tfidf_737  tfidf_738  tfidf_739  tfidf_740  tfidf_741  tfidf_742  \\\n",
       "0   0.000000   0.004764   0.007170        0.0   0.001556   0.000000   \n",
       "1   0.000000   0.005160   0.007848        0.0   0.003759   0.000000   \n",
       "2   0.002142   0.008723   0.005541        0.0   0.006195   0.000000   \n",
       "3   0.001029   0.003191   0.006301        0.0   0.003422   0.002949   \n",
       "4   0.002294   0.013093   0.020530        0.0   0.010694   0.000000   \n",
       "\n",
       "   tfidf_743  tfidf_744  tfidf_745  tfidf_746  tfidf_747  tfidf_748  \\\n",
       "0   0.001041        0.0   0.031250   0.013278   0.001368   0.000000   \n",
       "1   0.000000        0.0   0.022774   0.009339   0.001194   0.000000   \n",
       "2   0.000000        0.0   0.055527   0.005288   0.001798   0.001664   \n",
       "3   0.000000        0.0   0.015139   0.016847   0.002847   0.003410   \n",
       "4   0.000000        0.0   0.008718   0.005697   0.000000   0.000000   \n",
       "\n",
       "   tfidf_749  tfidf_750  tfidf_751  tfidf_752  tfidf_753  tfidf_754  \\\n",
       "0   0.003231   0.000000   0.008022   0.000000        0.0   0.001315   \n",
       "1   0.002874   0.001207   0.002705   0.000000        0.0   0.002948   \n",
       "2   0.002320   0.000588   0.005285   0.002697        0.0   0.004673   \n",
       "3   0.001684   0.000000   0.006635   0.002554        0.0   0.007536   \n",
       "4   0.000000   0.000000   0.010044   0.009046        0.0   0.003735   \n",
       "\n",
       "   tfidf_755  tfidf_756  tfidf_757  tfidf_758  tfidf_759  tfidf_760  \\\n",
       "0   0.000000        0.0   0.000621   0.000000   0.010054   0.001734   \n",
       "1   0.002629        0.0   0.001292   0.000000   0.003021   0.001523   \n",
       "2   0.000000        0.0   0.003366   0.000000   0.005566   0.001583   \n",
       "3   0.001245        0.0   0.006073   0.001671   0.009269   0.001948   \n",
       "4   0.000000        0.0   0.000000   0.006472   0.009566   0.004394   \n",
       "\n",
       "   tfidf_761  tfidf_762  tfidf_763  tfidf_764  tfidf_765  tfidf_766  \\\n",
       "0   0.000000   0.000000   0.002422   0.000000   0.002871   0.000812   \n",
       "1   0.000000   0.003861   0.000000   0.000000   0.002779   0.001689   \n",
       "2   0.000835   0.000000   0.000000   0.000000   0.002057   0.000797   \n",
       "3   0.000000   0.000915   0.003427   0.000765   0.002522   0.002875   \n",
       "4   0.000000   0.005603   0.004864   0.002195   0.018659   0.000000   \n",
       "\n",
       "   tfidf_767  tfidf_768  tfidf_769  tfidf_770  tfidf_771  tfidf_772  \\\n",
       "0   0.001468   0.000826        0.0   0.001951   0.003225   0.002171   \n",
       "1   0.000000   0.000000        0.0   0.002625   0.000832   0.000000   \n",
       "2   0.002375   0.000000        0.0   0.000897   0.005027   0.001976   \n",
       "3   0.004721   0.000000        0.0   0.002748   0.004496   0.005445   \n",
       "4   0.014841   0.000000        0.0   0.000000   0.000000   0.000000   \n",
       "\n",
       "   tfidf_773  tfidf_774  tfidf_775  tfidf_776  tfidf_777  tfidf_778  \\\n",
       "0        0.0   0.001565   0.001000   0.015696   0.006073   0.005233   \n",
       "1        0.0   0.001651   0.000944   0.015407   0.004056   0.007016   \n",
       "2        0.0   0.000000   0.000000   0.001986   0.005864   0.011827   \n",
       "3        0.0   0.001807   0.000000   0.000768   0.000429   0.009561   \n",
       "4        0.0   0.000000   0.004393   0.000000   0.000000   0.000000   \n",
       "\n",
       "   tfidf_779  tfidf_780  tfidf_781  tfidf_782  tfidf_783  tfidf_784  \\\n",
       "0   0.000601   0.005806   0.004432   0.000000   0.000955   0.001333   \n",
       "1   0.000000   0.002026   0.002569   0.003337   0.000000   0.001277   \n",
       "2   0.000000   0.001632   0.002676   0.000000   0.000000   0.000000   \n",
       "3   0.003172   0.004916   0.005108   0.000000   0.001019   0.001418   \n",
       "4   0.000000   0.000000   0.001459   0.000000   0.005331   0.009823   \n",
       "\n",
       "   tfidf_785  tfidf_786  tfidf_787  tfidf_788  tfidf_789  tfidf_790  \\\n",
       "0   0.001462   0.004927   0.000000   0.003790   0.006845   0.000984   \n",
       "1   0.000717   0.001263   0.001667   0.000000   0.001499   0.000000   \n",
       "2   0.000000   0.000000   0.004197   0.000675   0.002699   0.011113   \n",
       "3   0.000000   0.000580   0.001913   0.000000   0.001261   0.000747   \n",
       "4   0.000000   0.000000   0.005262   0.000000   0.006118   0.004690   \n",
       "\n",
       "   tfidf_791  tfidf_792  tfidf_793  tfidf_794  tfidf_795  tfidf_796  \\\n",
       "0        0.0   0.003211   0.000000   0.000000   0.000000   0.003353   \n",
       "1        0.0   0.000000   0.002335   0.000000   0.003232   0.004202   \n",
       "2        0.0   0.000000   0.001060   0.000000   0.000000   0.001171   \n",
       "3        0.0   0.002087   0.000655   0.000758   0.000000   0.002706   \n",
       "4        0.0   0.000000   0.000000   0.000000   0.009975   0.000000   \n",
       "\n",
       "   tfidf_797  tfidf_798  tfidf_799  tfidf_800  tfidf_801  tfidf_802  \\\n",
       "0   0.001168   0.004120   0.000000   0.004211   0.004651   0.000000   \n",
       "1   0.000973   0.000000   0.000000   0.001523   0.004129   0.000000   \n",
       "2   0.003800   0.003574   0.001809   0.000483   0.022461   0.001749   \n",
       "3   0.000000   0.011204   0.000000   0.003900   0.000000   0.000000   \n",
       "4   0.000000   0.007460   0.005262   0.005514   0.000000   0.000000   \n",
       "\n",
       "   tfidf_803  tfidf_804  tfidf_805  tfidf_806  tfidf_807  tfidf_808  \\\n",
       "0   0.000000   0.000629    0.00000   0.000000   0.001315   0.001196   \n",
       "1   0.000000   0.000000    0.00000   0.000000   0.000000   0.004497   \n",
       "2   0.002859   0.000631    0.00000   0.003914   0.002014   0.001067   \n",
       "3   0.001884   0.001400    0.00000   0.000000   0.002260   0.002068   \n",
       "4   0.004638   0.000000    0.02091   0.000000   0.007288   0.003110   \n",
       "\n",
       "   tfidf_809  tfidf_810  tfidf_811  tfidf_812  tfidf_813  tfidf_814  \\\n",
       "0   0.002262   0.002885   0.002629    0.00190   0.004142   0.005425   \n",
       "1   0.000822   0.001780   0.007803    0.00000   0.004974   0.001334   \n",
       "2   0.000000   0.000000   0.003131    0.00000   0.003881   0.001997   \n",
       "3   0.001982   0.004779   0.005187    0.00156   0.006087   0.016118   \n",
       "4   0.003578   0.000000   0.003619    0.00000   0.013337   0.018735   \n",
       "\n",
       "   tfidf_815  tfidf_816  tfidf_817  tfidf_818  tfidf_819  tfidf_820  \\\n",
       "0   0.002004   0.002319   0.001234   0.003440   0.005218   0.000000   \n",
       "1   0.000000   0.001346   0.003569   0.004998   0.002004   0.000000   \n",
       "2   0.000000   0.000825   0.001276   0.002470   0.002898   0.000566   \n",
       "3   0.000951   0.003057   0.002615   0.004924   0.004632   0.006493   \n",
       "4   0.000000   0.000000   0.000000   0.004070   0.009467   0.000000   \n",
       "\n",
       "   tfidf_821  tfidf_822  tfidf_823  tfidf_824  tfidf_825  tfidf_826  \\\n",
       "0   0.000000   0.002511   0.001409        0.0   0.000000   0.000000   \n",
       "1   0.000000   0.000000   0.000000        0.0   0.000000   0.001276   \n",
       "2   0.000000   0.000000   0.000000        0.0   0.000544   0.000000   \n",
       "3   0.003106   0.001203   0.001427        0.0   0.000000   0.000000   \n",
       "4   0.000000   0.000000   0.000000        0.0   0.005054   0.002123   \n",
       "\n",
       "   tfidf_827  tfidf_828  tfidf_829  tfidf_830  tfidf_831  tfidf_832  \\\n",
       "0   0.000000   0.004821   0.005322   0.064112   0.012349   0.017277   \n",
       "1   0.000000   0.006392   0.005019   0.056329   0.007941   0.030676   \n",
       "2   0.001291   0.002245   0.006317   0.052137   0.006997   0.027610   \n",
       "3   0.000000   0.001896   0.001985   0.049230   0.012206   0.015735   \n",
       "4   0.000000   0.000000   0.014406   0.065594   0.013178   0.022305   \n",
       "\n",
       "   tfidf_833  tfidf_834  tfidf_835  tfidf_836  tfidf_837  tfidf_838  \\\n",
       "0   0.000000   0.000905   0.000000   0.000553   0.000828   0.015273   \n",
       "1   0.000000   0.000000   0.000000   0.001325   0.000922   0.007257   \n",
       "2   0.000000   0.000000   0.000827   0.000000   0.000000   0.016392   \n",
       "3   0.000993   0.000667   0.000418   0.004464   0.000000   0.009831   \n",
       "4   0.000000   0.000000   0.000000   0.000000   0.003483   0.007425   \n",
       "\n",
       "   tfidf_839  tfidf_840  tfidf_841  tfidf_842  tfidf_843  tfidf_844  \\\n",
       "0   0.001463   0.001343        0.0   0.001099   0.001161   0.000584   \n",
       "1   0.004371   0.000000        0.0   0.001253   0.004781   0.001362   \n",
       "2   0.001042   0.004810        0.0   0.000467   0.002382   0.004164   \n",
       "3   0.004847   0.000000        0.0   0.000000   0.000000   0.001344   \n",
       "4   0.004449   0.005189        0.0   0.008648   0.002653   0.011580   \n",
       "\n",
       "   tfidf_845  tfidf_846  tfidf_847  tfidf_848  tfidf_849  tfidf_850  \\\n",
       "0   0.000000   0.000000        0.0   0.001322   0.000000   0.000962   \n",
       "1   0.000000   0.000000        0.0   0.002238   0.000000   0.001344   \n",
       "2   0.000533   0.002547        0.0   0.000803   0.000000   0.000793   \n",
       "3   0.000000   0.005453        0.0   0.000622   0.000816   0.002488   \n",
       "4   0.000000   0.000000        0.0   0.000000   0.000000   0.000000   \n",
       "\n",
       "   tfidf_851  tfidf_852  tfidf_853  tfidf_854  tfidf_855  tfidf_856  \\\n",
       "0   0.000998   0.000000   0.004498        0.0   0.000000   0.008120   \n",
       "1   0.006493   0.000000   0.005070        0.0   0.003327   0.010521   \n",
       "2   0.000000   0.003494   0.001665        0.0   0.001461   0.008267   \n",
       "3   0.000000   0.000521   0.010164        0.0   0.008855   0.014233   \n",
       "4   0.003753   0.011053   0.000000        0.0   0.000000   0.006234   \n",
       "\n",
       "   tfidf_857  tfidf_858  tfidf_859  tfidf_860  tfidf_861  tfidf_862  \\\n",
       "0   0.000000   0.001525   0.000000   0.000000   0.006841   0.000000   \n",
       "1   0.004624   0.001995   0.000000   0.000000   0.008224   0.001459   \n",
       "2   0.001139   0.012260   0.001142   0.000000   0.009580   0.000000   \n",
       "3   0.002583   0.000000   0.000000   0.001679   0.009058   0.001262   \n",
       "4   0.000000   0.002460   0.000000   0.000000   0.003022   0.000000   \n",
       "\n",
       "   tfidf_863  tfidf_864  tfidf_865  tfidf_866  tfidf_867  tfidf_868  \\\n",
       "0        0.0   0.005399   0.000715   0.000780   0.006593   0.003987   \n",
       "1        0.0   0.005872   0.003337   0.000000   0.005738   0.009768   \n",
       "2        0.0   0.002566   0.000609   0.000000   0.007256   0.008501   \n",
       "3        0.0   0.004416   0.000797   0.002889   0.002247   0.001257   \n",
       "4        0.0   0.004211   0.011969   0.000000   0.013254   0.000000   \n",
       "\n",
       "   tfidf_869  tfidf_870  tfidf_871  tfidf_872  tfidf_873  tfidf_874  \\\n",
       "0   0.001791   0.002079   0.000542   0.000000   0.008240   0.000000   \n",
       "1   0.003398   0.003739   0.000000   0.000000   0.005789   0.000000   \n",
       "2   0.001457   0.004614   0.002953   0.001103   0.006006   0.000000   \n",
       "3   0.000491   0.008904   0.000702   0.001467   0.010173   0.000000   \n",
       "4   0.000000   0.000000   0.003675   0.000000   0.003675   0.004833   \n",
       "\n",
       "   tfidf_875  tfidf_876  tfidf_877  tfidf_878  tfidf_879  tfidf_880  \\\n",
       "0   0.000234   0.291092   0.000748   0.006105   0.001187   0.014907   \n",
       "1   0.000000   0.284980   0.000000   0.000000   0.001033   0.009666   \n",
       "2   0.000000   0.000000   0.000000   0.000945   0.000000   0.018084   \n",
       "3   0.000000   0.000000   0.000000   0.004348   0.000471   0.009635   \n",
       "4   0.001741   0.000000   0.000000   0.000000   0.016665   0.013930   \n",
       "\n",
       "   tfidf_881  tfidf_882  tfidf_883  tfidf_884  tfidf_885  tfidf_886  \\\n",
       "0   0.012240   0.001894   0.007907   0.001463   0.004927   0.004748   \n",
       "1   0.011557   0.002143   0.008705   0.000000   0.002338   0.002034   \n",
       "2   0.015569   0.004811   0.005433   0.003823   0.000939   0.002507   \n",
       "3   0.018502   0.001307   0.005540   0.001286   0.003143   0.004295   \n",
       "4   0.002855   0.006084   0.010777   0.005203   0.000000   0.003495   \n",
       "\n",
       "   tfidf_887  tfidf_888  tfidf_889  tfidf_890  tfidf_891  tfidf_892  \\\n",
       "0   0.001267   0.002571   0.000537   0.000530   0.005818   0.023205   \n",
       "1   0.000828   0.006478   0.004931   0.000000   0.010803   0.024796   \n",
       "2   0.000427   0.003319   0.001276   0.001074   0.008240   0.019176   \n",
       "3   0.000553   0.009445   0.002726   0.001614   0.007882   0.024214   \n",
       "4   0.000000   0.000000   0.015658   0.000000   0.000000   0.020856   \n",
       "\n",
       "   tfidf_893  tfidf_894  tfidf_895  tfidf_896  tfidf_897  tfidf_898  \\\n",
       "0   0.001266   0.000000   0.018922   0.000000   0.000000        0.0   \n",
       "1   0.000000   0.001349   0.010826   0.000000   0.001483        0.0   \n",
       "2   0.001554   0.001042   0.006036   0.002100   0.000000        0.0   \n",
       "3   0.002749   0.000000   0.011136   0.001115   0.000684        0.0   \n",
       "4   0.000000   0.005548   0.003324   0.000000   0.000000        0.0   \n",
       "\n",
       "   tfidf_899  tfidf_900  tfidf_901  tfidf_902  tfidf_903  tfidf_904  \\\n",
       "0   0.000863        0.0   0.001074        0.0   0.005441   0.002158   \n",
       "1   0.002513        0.0   0.000000        0.0   0.000000   0.003445   \n",
       "2   0.000000        0.0   0.000000        0.0   0.004060   0.000300   \n",
       "3   0.001281        0.0   0.000000        0.0   0.002876   0.004663   \n",
       "4   0.003308        0.0   0.000000        0.0   0.006009   0.006986   \n",
       "\n",
       "   tfidf_905  tfidf_906  tfidf_907  tfidf_908  tfidf_909  tfidf_910  \\\n",
       "0   0.002530   0.002370   0.000000   0.003817   0.000000   0.003125   \n",
       "1   0.000000   0.000762   0.003969   0.001540   0.000523   0.000000   \n",
       "2   0.004541   0.001436   0.000000   0.000000   0.000000   0.000000   \n",
       "3   0.000000   0.000000   0.000516   0.000992   0.000000   0.001201   \n",
       "4   0.002776   0.000000   0.000000   0.008990   0.000000   0.015581   \n",
       "\n",
       "   tfidf_911  tfidf_912  tfidf_913  tfidf_914  tfidf_915  tfidf_916  \\\n",
       "0   0.009277   0.000827   0.004520   0.025234   0.002273   0.011556   \n",
       "1   0.012365   0.000000   0.007825   0.024361   0.005520   0.024763   \n",
       "2   0.013050   0.001065   0.000000   0.000000   0.000000   0.008084   \n",
       "3   0.006168   0.001648   0.000761   0.000000   0.000425   0.011240   \n",
       "4   0.001381   0.003913   0.004628   0.051715   0.000000   0.028286   \n",
       "\n",
       "   tfidf_917  tfidf_918  tfidf_919  tfidf_920  tfidf_921  tfidf_922  \\\n",
       "0   0.013497   0.008879   0.000868   0.001327   0.002890   0.000000   \n",
       "1   0.008667   0.004176   0.001520   0.000000   0.003325   0.001820   \n",
       "2   0.008864   0.004308   0.004230   0.002028   0.004267   0.003104   \n",
       "3   0.007224   0.000801   0.000954   0.001505   0.002337   0.001166   \n",
       "4   0.006772   0.006569   0.000000   0.002094   0.007617   0.000000   \n",
       "\n",
       "   tfidf_923  tfidf_924  tfidf_925  tfidf_926  tfidf_927  tfidf_928  \\\n",
       "0   0.000324   0.016679   0.003881   0.000000   0.001575   0.007682   \n",
       "1   0.000000   0.014876   0.004533   0.000000   0.001329   0.002857   \n",
       "2   0.000000   0.006091   0.000311   0.004534   0.002057   0.002501   \n",
       "3   0.000000   0.008840   0.003440   0.001024   0.002478   0.003392   \n",
       "4   0.000000   0.025252   0.000000   0.000000   0.004916   0.000000   \n",
       "\n",
       "   tfidf_929  tfidf_930  tfidf_931  tfidf_932  tfidf_933  tfidf_934  \\\n",
       "0   0.001136   0.000000   0.003877   0.000000   0.000880   0.000000   \n",
       "1   0.000000   0.000535   0.005930   0.000000   0.000000   0.000777   \n",
       "2   0.001300   0.000611   0.005957   0.000000   0.000000   0.000583   \n",
       "3   0.000706   0.000431   0.004007   0.000000   0.000573   0.001477   \n",
       "4   0.004774   0.000000   0.009471   0.011707   0.003909   0.000000   \n",
       "\n",
       "   tfidf_935  tfidf_936  tfidf_937  tfidf_938  tfidf_939  tfidf_940  \\\n",
       "0        0.0   0.000529        0.0   0.061635   0.004123   0.001621   \n",
       "1        0.0   0.002639        0.0   0.043724   0.002515   0.001352   \n",
       "2        0.0   0.000319        0.0   0.001977   0.004883   0.002066   \n",
       "3        0.0   0.003554        0.0   0.061380   0.003235   0.001778   \n",
       "4        0.0   0.000000        0.0   0.022484   0.004640   0.007964   \n",
       "\n",
       "   tfidf_941  tfidf_942  tfidf_943  tfidf_944  tfidf_945  tfidf_946  \\\n",
       "0   0.002351   0.000653   0.000000   0.001227   0.001891   0.000780   \n",
       "1   0.003991   0.001128   0.003062   0.001465   0.003009   0.002543   \n",
       "2   0.000000   0.003258   0.000313   0.015211   0.000688   0.001808   \n",
       "3   0.002614   0.000677   0.000000   0.012896   0.001851   0.002712   \n",
       "4   0.004615   0.004073   0.000000   0.004274   0.006826   0.001726   \n",
       "\n",
       "   tfidf_947  tfidf_948  tfidf_949  tfidf_950  tfidf_951  tfidf_952  \\\n",
       "0   0.001911   0.000753   0.007920   0.008252   0.005780   0.000000   \n",
       "1   0.000000   0.000972   0.019081   0.008378   0.001817   0.000000   \n",
       "2   0.000000   0.000000   0.010171   0.003969   0.002386   0.000000   \n",
       "3   0.001543   0.005084   0.013440   0.006379   0.000780   0.001203   \n",
       "4   0.000000   0.000000   0.010461   0.000000   0.000000   0.005310   \n",
       "\n",
       "   tfidf_953  tfidf_954  tfidf_955  tfidf_956  tfidf_957  tfidf_958  \\\n",
       "0   0.009351   0.000653   0.000000   0.004993   0.000000   0.000000   \n",
       "1   0.011569   0.000000   0.000000   0.009110   0.000000   0.005041   \n",
       "2   0.008069   0.000000   0.001585   0.003843   0.000000   0.000680   \n",
       "3   0.004640   0.000000   0.000487   0.000427   0.000573   0.000000   \n",
       "4   0.021729   0.000000   0.000000   0.004063   0.000000   0.000000   \n",
       "\n",
       "   tfidf_959  tfidf_960  tfidf_961  tfidf_962  tfidf_963  tfidf_964  \\\n",
       "0   0.009349   0.000000   0.001251   0.003044   0.023716   0.005212   \n",
       "1   0.010516   0.001927   0.000000   0.000000   0.017083   0.001163   \n",
       "2   0.004554   0.001795   0.003385   0.002180   0.017446   0.005324   \n",
       "3   0.005720   0.002398   0.001690   0.000000   0.014660   0.002843   \n",
       "4   0.001448   0.000000   0.000000   0.000000   0.011620   0.000000   \n",
       "\n",
       "   tfidf_965  tfidf_966  tfidf_967  tfidf_968  tfidf_969  tfidf_970  \\\n",
       "0   0.000000   0.000000   0.002964   0.014946   0.001427   0.003319   \n",
       "1   0.000000   0.000000   0.005120   0.004832   0.004128   0.000000   \n",
       "2   0.000000   0.001106   0.001163   0.007559   0.001542   0.002389   \n",
       "3   0.000447   0.000000   0.001074   0.014664   0.000427   0.002801   \n",
       "4   0.004519   0.011562   0.011512   0.001593   0.000000   0.004973   \n",
       "\n",
       "   tfidf_971  tfidf_972  tfidf_973  tfidf_974  tfidf_975  tfidf_976  \\\n",
       "0   0.002155   0.002179   0.002304   0.013462   0.009104   0.036078   \n",
       "1   0.005860   0.004777   0.002136   0.012232   0.004157   0.026712   \n",
       "2   0.005618   0.000000   0.000657   0.011769   0.007323   0.022259   \n",
       "3   0.000000   0.000000   0.000871   0.017638   0.006153   0.027577   \n",
       "4   0.000000   0.006047   0.002063   0.000000   0.000000   0.005729   \n",
       "\n",
       "   tfidf_977  tfidf_978  tfidf_979  tfidf_980  tfidf_981  tfidf_982  \\\n",
       "0   0.015594   0.005851   0.000994   0.001146   0.002556   0.002865   \n",
       "1   0.014666   0.009722   0.001300   0.004846   0.000000   0.000000   \n",
       "2   0.011646   0.007884   0.001048   0.002746   0.000000   0.005845   \n",
       "3   0.022179   0.014944   0.001621   0.001010   0.001158   0.000000   \n",
       "4   0.054534   0.009122   0.000000   0.011086   0.002809   0.005103   \n",
       "\n",
       "   tfidf_983  tfidf_984  tfidf_985  tfidf_986  tfidf_987  tfidf_988  \\\n",
       "0   0.003258   0.000950   0.000000   0.000000   0.001571   0.004692   \n",
       "1   0.002263   0.000000   0.001992   0.001289   0.000000   0.004167   \n",
       "2   0.009351   0.001944   0.000000   0.000583   0.002498   0.005029   \n",
       "3   0.002999   0.000000   0.002137   0.000422   0.001532   0.001337   \n",
       "4   0.000000   0.000000   0.000000   0.000000   0.000000   0.006099   \n",
       "\n",
       "   tfidf_989  tfidf_990  tfidf_991  tfidf_992  tfidf_993  tfidf_994  \\\n",
       "0   0.001697   0.032090   0.000000   0.005281   0.001574   0.001360   \n",
       "1   0.002653   0.031174   0.000000   0.003782   0.000000   0.000000   \n",
       "2   0.001280   0.016983   0.003245   0.000505   0.000679   0.000626   \n",
       "3   0.004466   0.013296   0.000753   0.001142   0.002024   0.000454   \n",
       "4   0.003340   0.023489   0.003491   0.001937   0.019001   0.000000   \n",
       "\n",
       "   tfidf_995  tfidf_996  tfidf_997  tfidf_998  tfidf_999  \n",
       "0   0.001218   0.034269   0.000729   0.002473   0.000000  \n",
       "1   0.000000   0.030111   0.001060   0.002014   0.000000  \n",
       "2   0.000618   0.022115   0.001099   0.000748   0.000000  \n",
       "3   0.003351   0.022918   0.004587   0.002328   0.003315  \n",
       "4   0.008543   0.049671   0.000000   0.002725   0.000000  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15493779",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(['Unnamed: 0','Unnamed: 0_x','Unnamed: 0_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b182aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.assign(\n",
    "    rating_bins = pd.qcut(\n",
    "        X['review_scores_value'],\n",
    "        q=5,\n",
    "        labels=[0,1,2,3,4]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ebe8558",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a00fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler \n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "G = pd.DataFrame(scaler.fit_transform(X), columns=list(X.columns))\n",
    "\n",
    "vals = G.drop(labels=['rating_bins','review_scores_value'],axis=1)\n",
    "t_val = G['rating_bins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1966b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(vals, t_val, test_size=0.25, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b568811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn=KNeighborsRegressor(n_neighbors=43)\n",
    "knn.fit(X_train,y_train)\n",
    "pred=knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26787d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Listing Columns + Count_Vectors')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "ax1=sns.distplot(y_test,hist=False,color='black',label='actual value')\n",
    "ax2=sns.distplot(pred,hist=False,color='grey',label='predicted value')\n",
    "plt.title(\"Listing Columns + Count_Vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab020bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0768654681094676\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of the model \n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0ff3ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rmse_val = []\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "def count_vec_max_df(params, X_train, y_train):\n",
    "    knn=KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(X_train,y_train)\n",
    "    pred=knn.predict(X_test)\n",
    "    error=sqrt(mean_squared_error(y_test,pred)) \n",
    "    return error\n",
    "\n",
    "mylist=list(range(1,250))\n",
    "# mylist=list(filter(lambda x:x%2!=0,mylist))\n",
    "for k in mylist:\n",
    "    rmse_val.append(count_vec_max_df(k, X_train, y_train))\n",
    "   \n",
    "#plotting the rmse values against k values\n",
    "# curve = pd.DataFrame(rmse_val) #elbow curve \n",
    "# curve.plot(color='black', label='RMSE')\n",
    "# plt.title(\"Only Listing Columns\")\n",
    "# plt.xlabel(\"K neighbours values\")\n",
    "# plt.ylabel(\"RMSE Values\")\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(mylist, rmse_val, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Elbow Method for optimal K')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c84f32d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At k=43\n",
      "Mean Squared Error =  0.03580322694496539\n",
      "Root Mean Squared Error =  0.18921740655913608\n",
      "R2 Score =  0.0768654681094676\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "print(\"At k=43\")\n",
    "print(\"Mean Squared Error = \", (mean_squared_error(y_test, pred)))\n",
    "print(\"Root Mean Squared Error = \", np.sqrt(mean_squared_error(y_test, pred)))\n",
    "print(\"R2 Score = \", r2_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b2f73b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1294744044905215\n",
      "0.5496267333248683\n",
      "0.22961061081864692\n",
      "-0.359335733208344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "rr = Ridge(alpha=0.01)\n",
    "rr.fit(X_train, y_train) \n",
    "pred_train_rr= rr.predict(X_train)\n",
    "print(np.sqrt(mean_squared_error(y_train,pred_train_rr)))\n",
    "print(r2_score(y_train, pred_train_rr))\n",
    "\n",
    "pred_test_rr= rr.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(y_test,pred_test_rr))) \n",
    "print(r2_score(y_test, pred_test_rr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "619ee17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'alpha':[0.000001, 0.0001,0.001,0.01, 0.05, 0.1, 0.5, 0, 1, 2,3, 4, 5, 10, 15, 20, 25, 30, 50, 100]}\n",
    "# define the model/ estimator\n",
    "model = Ridge()\n",
    "# define the grid search\n",
    "Ridge_reg= GridSearchCV(model, parameters, scoring='neg_mean_squared_error',cv=5)\n",
    "#fit the grid search\n",
    "Ridge_reg.fit(X_train,y_train)\n",
    "# best estimator\n",
    "print(Ridge_reg.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bb60fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE train:  0.16139804607578503\n",
      "R2 Score Train:  0.30015590136774195\n",
      "RMSE test:  0.18492301259885666\n",
      "R2 Score test:  0.11829207254416685\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "rr = Ridge(alpha=15)\n",
    "rr.fit(X_train, y_train) \n",
    "pred_train_rr= rr.predict(X_train)\n",
    "print(\"RMSE train: \", np.sqrt(mean_squared_error(y_train,pred_train_rr)))\n",
    "print(\"R2 Score Train: \", r2_score(y_train, pred_train_rr))\n",
    "\n",
    "pred_test_rr= rr.predict(X_test)\n",
    "print(\"RMSE test: \", np.sqrt(mean_squared_error(y_test,pred_test_rr))) \n",
    "print(\"R2 Score test: \", r2_score(y_test, pred_test_rr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5927184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7e901d2940>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEXCAYAAACqIS9uAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABCtUlEQVR4nO3deXxU1fn48c+TkJCV7OxIUJRFWWQriGtFEWtRkSquxY1qlWpdq11Ea1v9aqtVfloRqaioKIuKVRZRxA0RVJAdRNCwZoFskG3m+f1xZ2LALJOQmTtJnvfrNa/M3Ln3nmfuZJ45c+6554iqYowxpuWIcDsAY4wxoWWJ3xhjWhhL/MYY08JY4jfGmBbGEr8xxrQwlviNMaaFscTfBInIKSKysYHbvisiv27smI6UiDwvIg+6HYcxLYEl/jAmIttEZMThy1X1I1XtEcD2k0TkpcO2HaWq0xszzirltRGRx0XkexEpEpEtvsfpwSivqRCRaN97sVlEin3v6zQRyQxyuaeLSFYA613qi0kOW95KRPaKyHnBLN+EniV+0yhEJBpYDBwPnAO0AU4CcoEhLoYWFL5fKOMDXH0WMBq4DEgC+gErgTODE129zQWSgdMOW34OoMD8UAcEzhePG+W2CKpqtzC9AduAEdUsPx3IqvL4bmAHUAhsxEko5wBlQDlQBKzyrbsEuM53fzzwMfAosA/4DhhVZb/dgKW+/b4H/D/gpRpivQ7YAyTU8np6+crfD6wFRld57nngwapxHbatAt2rrPsU8K7vtX0CtAce972ODcCJhx3HO4DVQD4wE4jxPZcOvO2LKQ/4CIgI4L15HhgfwHojgINAl1rW6Qi85St/C3B9dcelhve+2tcGxPvK9fqOURHQsZYYpgDTDlv2GvAv3/2hwKe+47QKOL3KeqnAf4GdvuP/Rk3lA61979NO3+1xoHXV14bz/7wbeLGh74/dar9Zjb+JE5EewM3AYFVNBEYC21R1PvB3YKaqJqhqvxp28TOcL4t04P+A56r85H8ZWA6kAZOAK2sJZQQwX1WLaogzCpgHLATaAhOBGb74G+Ji4E++uEuBz4AvfY9nAf+qZv1zcL7M+uJ8uQDcjpNsMoB2wL04XzKNZQSwXFV/qGWdV3wxdATGAn8Xkfr8GvjJa1PVYmAUsNP3/ieo6s5a9jEdGCsisQAikgT8EnhBRDoB/wMexEnydwCzRSTDt+2LQBzOr722wGO1lP9HnC+R/ji/fIbgvI9+7X1ldAUmEPz3p0WyxN/0eXBqUb1FJEpVt6nqt/XYfruqPquqHpwPfwegnYgcBQwG/qKqZar6MU6ttCZpwK5anh8KJAAP+fb3Pk5N7tJ6xFrVXFVdqaolOE0VJar6gu91zAROPGz9J1R1p6rm4XwB9fctL8d5zV1VtVyd8yeNmVhqPS4i0gU4GbhbVUtU9WtgKrV/yR6uptcWMFX9BOcX24W+RRcDm3zxXAG8o6rvqKpXVRcBK4BzRaQDToK/QVX3+Y7hh7UUdTnwgKruVdVs4H4Ofa1e4D5VLVXVgwT//WmRLPE3caq6BbgVp0a+V0ReFZGO9djF7ir7OuC7m4BT+8yrsgygtlprLs4HtCYdgR9U1Vtl2XagUz1irWpPlfsHq3mccNj6u6vcP1Dl+UdwmlcWishWEflDTQWKyGoR2S8i+3Ha65/yPxaRp2rYLJDjkqeqhVWW1fe41PTa6usF4Crf/StxKgLg1L5/VeW17sf5suoAdMGJf1+AZXTEeX1+233L/LJ9X+Z+Ab8/JnCW+JsBVX1ZVU/G+YAq8LD/qSPY7S4gVUTiqizrUsv67wEjRSS+hud3Al1EpOr/3FE45yYOV4zTdACAiLQPLOT6U9VCVb1dVY/Gadq4raZmFlXtq6rJqpqM0wz2W/9jVf1tDUW8BwwRkc41PL8T5zgnVllW9bgccixwmkICVd/3/wXgTBEZhvML7WXf8h+AF6u81mRVjVfVh3zPpYpIcoDl78T5P/U7yres2m3q8/6YwFniD39RIhJT5XZITwcR6SEiPxeR1kAJTm3X43t6D5B5WLINiKpux/k5P8nXHXEYzgevJi/iJIHZItJTRCJEJE1E7hWRc4HPcZLYXSISJSKn+/b3ajX7WgUcLyL9RSQG59dMUIjIeSLS3XdeowDn2Hnq2CxgqvoesAiYKyIDfV0kE0XkBhG5xtf2/ynwD9/72xe4Fpjh28XXOE0qqb4vwFvrUfweIM3XXh9IrNtxTva/AixSVf8viZeAX4rISBGJ9MV5uoh0VtVdOCfZnxKRFN97e2ot5b8C/ElEMnzdfP/i23+1gv3+tFSW+MPfOzjJ3H+bdNjzrYGHgBycn/xtcU6AAbzu+5srIl82oOzLgWE4zRUP4rSdl1a3oqqW4pzI3ICT6ApwTgynA5+rahlOl8ZRvlifAq5S1Q3V7GsT8ABObXkzTjIKlmN95RThnCB+SlWXNHIZY3Hex5k4PW/WAIN85YJzniMTp+Y7F6eNe5HvuRdxvgi34ZwYnxloob5j+wqw1ddEE0gT4HScGvkLVfbzA3A+zv9VNs4X/J38mD+uxGmL3wDsxfflVEP5D+JUKFYD3+CckK/twr1QvD8tjth5EhMoEZkJbFDV+9yOxRjTcFbjNzUSkcEicoyv2eYcnFrfGy6HZYw5Qpb4TW3a41xwVQQ8Adyoql+5GpFpMHHGaSqq5nZv3Vub5sSaeowxpoWxGr8xxrQwYTUIUnp6umZmZrodhjHGNBkrV67MUdWMutf8UVgl/szMTFasWOF2GMYY02SIyPa61zqUNfUYY0wLY4nfGGNaGEv8xhjTwoRVG391ysvLycrKoqSkpO6VTdiJiYmhc+fOREVFuR2KMcYn7BN/VlYWiYmJZGZmctiUoCbMqSq5ublkZWXRrVs3t8MxxviEfVNPSUkJaWlplvSbIBEhLS3Nfq0ZE2bCPvEDlvSbMHvvjAk/TSLxG2OMaTyW+BvZkiVL+PTTT49oHwkJDZ05Lzj7MQYgNzeXyZMn8+KLL1JcXOx2OOYIWOJvZI2R+I0JN6rKa6+9Rm5uLlu3buW9996reyMTtizxB+CCCy5g4MCBHH/88UyZMqVy+fz58xkwYAD9+vXjzDPPZNu2bfznP//hscceo3///nz00UeMHz+eWbNmVW7jr4UXFRVx5plnMmDAAPr06cObb75Zawx33303Tz3143zekyZN4p///GdA+1myZAnnnXde5eObb76Z559/HoCVK1dy2mmnMXDgQEaOHMmuXbsadIxM87Z161b27t3LBRdcwNChQ/n6668pKipyOyzTQGHfnbOqW2+9la+//rpR99m/f38ef/zxWteZNm0aqampHDx4kMGDB3PRRRfh9Xq5/vrrWbp0Kd26dSMvL4/U1FRuuOEGEhISuOOOOwB47rnnqt1nTEwMc+fOpU2bNuTk5DB06FBGjx5d48nQcePGceutt/Lb3zpzer/22mvMnz+/3vupqry8nIkTJ/Lmm2+SkZHBzJkz+eMf/8i0adPq3Na0LF999RVxcXEcf/zxdOjQgWXLlrF+/XoGDx7sdmimAZpU4nfLE088wdy5cwH44Ycf2Lx5M9nZ2Zx66qmV/dNTU1PrtU9V5d5772Xp0qVERESwY8cO9uzZQ/v27atd/8QTT2Tv3r3s3LmT7OxsUlJSOOqooygvL6/XfqrauHEja9as4ayzzgLA4/HQoUOHer0O0/x5PB62bNlC7969adWqFW3btiU9Pd0SfxPWpBJ/XTXzYFiyZAnvvfcen332GXFxcZx++umUlJSgqgHVqlu1aoXX6wWcZF9WVgbAjBkzyM7OZuXKlURFRZGZmVlnf/exY8cya9Ysdu/ezbhx4wLeT9UYgMrnVZXjjz+ezz77LPADYlqc77//ntLSUo477rjKZccccwwrV67E4/EQGRnpYnSmIayNvw75+fmkpKQQFxfHhg0bWLZsGQDDhg3jww8/5LvvvgMgLy8PgMTERAoLCyu3z8zMZOXKlQC8+eablJeXV+63bdu2REVF8cEHH7B9e90jq44bN45XX32VWbNmMXbs2ID307VrV9atW0dpaSn5+fksXrwYgB49epCdnV2Z+MvLy1m7dm2DjpNpvr777jtE5JCrr7t27UpFRQU7d+50MTLTUJb463DOOedQUVFB3759+fOf/8zQoUMByMjIYMqUKYwZM4Z+/fpxySWXAPDLX/6SuXPnVp7cvf766/nwww8ZMmQIn3/+OfHx8QBcfvnlrFixgkGDBjFjxgx69uxZZyzHH388hYWFdOrUqbJJJpD9dOnShYsvvpi+ffty+eWXc+KJJwIQHR3NrFmzuPvuu+nXrx/9+/e3HknmJ7KysmjXrh2tW7euXHbUUUcBzq8B0/SE1Zy7gwYN0sMnYlm/fj29evVyKSLTGOw9bLq8Xi8PP/wwffv25Re/+MUhz/373/+mU6dOlb8+jTtEZKWqDqrPNlbjN8bUKDs7m7KyMrp06fKT59q3b8/u3btdiMocKUv8xpga/fDDDwB07tz5J8+1b9+e3Nzcyg4LpumwxG+MqVFWVhZxcXGkpKT85Ll27doBsHfv3lCHZY6QJX5jTI1++OEHunTpUm3XZf+1Itbc0/RY4jfGVKukpIS8vDw6depU7fNJSUnExMRY4m+CLPEbY6q1Z88egBqvAhcR2rVrV7meaTos8YdY1QHT3nrrLR566KEa192/f/8hA7MFatKkSTz66KMNjrGx92OaJn9NvrbhP9q3b8+ePXsOuTLchD9L/I3E4/HUe5vRo0fzhz/8ocbnG5r4jWkMu3fvJi4urtZ5Hdq1a0d5eTn79+8PXWDmiAU18YtIsojMEpENIrJeRIYFs7xg2LZtGz179uTXv/41ffv2ZezYsRw4cABwhmN44IEHOPnkk3n99ddZuHAhw4YNY8CAAfzqV7+qHLZ2/vz59OzZk5NPPpk5c+ZU7vv555/n5ptvBpyf1RdeeCH9+vWjX79+fPrpp/zhD3/g22+/pX///tx5550APPLIIwwePJi+ffty3333Ve7rb3/7Gz169GDEiBFs3LjxJ68jPz+fzMzMyprZgQMH6NKlC+Xl5Tz77LMMHjyYfv36cdFFF1W+vqpOP/10/BfX5eTkkJmZCThfeHfeeWdlTM8888yRHnITJvyD/dU2JlV6ejrg/E+YpiPYg7T9G5ivqmNFJBqIO5KdzZ8/v9FPJLVv355zzjmn1nU2btzIc889x/Dhw7nmmmt46qmnKoddjomJ4eOPPyYnJ4cxY8bw3nvvER8fz8MPP8y//vUv7rrrLq6//nref/99unfvXjm0w+F+97vfcdpppzF37lw8Hg9FRUU89NBDrFmzpnIo6oULF7J582aWL1+OqjJ69GiWLl1KfHw8r776Kl999RUVFRUMGDCAgQMHHrL/pKQk+vXrx4cffsgZZ5zBvHnzGDlyJFFRUYwZM4brr78egD/96U8899xzTJw4MaDj99xzz5GUlMQXX3xBaWkpw4cP5+yzzz5kXBfT9Hi9Xvbu3cuQIUNqXc+f+LOzsw8ZxM2Et6DV+EWkDXAq8ByAqpap6v5glRdMXbp0Yfjw4QBcccUVfPzxx5XP+RP5smXLWLduHcOHD6d///5Mnz6d7du3s2HDBrp168axxx6LiHDFFVdUW8b777/PjTfeCEBkZCRJSUk/WWfhwoUsXLiQE088kQEDBrBhwwY2b97MRx99xIUXXkhcXBxt2rRh9OjR1ZZxySWXMHPmTABeffXVytjXrFnDKaecQp8+fZgxY0a9BmpbuHAhL7zwAv379+dnP/sZubm5bN68OeDtTXjKycnB4/HUObx3bGws8fHxVuNvYoJZ4z8ayAb+KyL9gJXALap6yGSdIjIBmAA/DvxUk7pq5sFy+E/dqo/9g66pKmeddRavvPLKIet+/fXXAQ3fHAhV5Z577uE3v/nNIcsff/zxgMoYPXo099xzD3l5eaxcuZKf//znAIwfP5433niDfv368fzzz7NkyZKfbFt1aOeqwz6rKk8++SQjR448gldmwk1dPXqqysjIsMTfxASzjb8VMAB4WlVPBIqBn5zJVNUpqjpIVQdlZGQEMZyG+/777yuHLn7llVc4+eSTf7LO0KFD+eSTT9iyZQvgtKFv2rSJnj178t133/Htt99Wbl+dM888k6effhpw2s0LCgp+MsTzyJEjmTZtWuW5gx07drB3715OPfVU5s6dy8GDByksLGTevHnVlpGQkMCQIUO45ZZbOO+88yrHUS8sLKRDhw6Ul5czY8aMaretOrx01akkR44cydNPP1053PSmTZtsIu5mYPfu3URGRpKWllbnuunp6eTk5BBOAz6a2gUz8WcBWar6ue/xLJwvgianV69eTJ8+nb59+5KXl1fZJFNVRkYGzz//PJdeeil9+/Zl6NChbNiwgZiYGKZMmcIvfvELTj75ZLp27VptGf/+97/54IMP6NOnDwMHDmTt2rWkpaUxfPhwTjjhBO68807OPvtsLrvsMoYNG0afPn0YO3YshYWFDBgwgEsuuYT+/ftz0UUXccopp9T4Wi655BJeeumlQ841/PWvf+VnP/sZZ511Vo3DQ99xxx08/fTTnHTSSYfU7q677jp69+7NgAEDOOGEE/jNb35DRUVFoIfWhKmcnBzS0tICmmQlPT2dkpIS+8JvQoI6LLOIfARcp6obRWQSEK+qd9a0fjgOy7xt2zbOO+881qxZ41oMTZ3b76GpvyeeeIKOHTsGNOTyt99+y0svvcRVV11lJ/VdEI7DMk8EZojIaqA/8Pcgl2eMOULl5eXs27evssdOXfxNtNbO33QEtTunqn4N1OubKNxkZmZabd+0KLm5ucCPCb0uiYmJREdHW+JvQprElbt20qjpsveu6cnOzgYIuMYvIpUneE3TEPaJPyYmhtzcXEsgTZCqkpubS0xMjNuhmHrIzs5GRALq0eOXnp5e+YVhwl+wr9w9Yp07dyYrK8v+qZqomJiYamdvMuErJyeHlJQUWrUKPD2kp6ezevVqSktLD5mU3YSnsE/8UVFR1lPAmBDKzs4OuH3fr+oJ3prG7zfhI+ybeowxoePxeMjLywu4fd/PBmtrWizxG2Mq7d+/H6/XW+/En5KSQkREhDXJNhGW+I0xlfLy8gBITU2t13aRkZGkpqZajb+JsMRvjKnU0MQPWJfOJsQSvzGmUl5eHlFRUZWjztZHeno6eXl5DZqNzoSWJX5jTKV9+/aRmpraoKHEMzIyUNXKXw0mfFniN8ZUysvLa1AzD1B5wZd/yAcTvizxG2MAZ7rF/fv3k5KS0qDt/Ynf2vnDnyV+YwwABQUFeDyeBtf4Y2JiiI+Ptxp/E2CJ3xgDHFmPHr+0tDRL/E2AJX5jDOCc2AVL/C2BJX5jDODU+CMjI2nTpk2D95GWlsaBAwc4ePBgI0ZmGpslfmMM4NT4k5OTG9SV088/1IPV+sObJX5jDAD5+fkkJycf0T6sS2fTYInfGAM4if9ImnnAGaxNRCzxhzlL/MYYysvLKS4uPuIaf2RkJCkpKZb4w5wlfmMMBQUFACQlJR3xvqxnT/izxG+MIT8/H2jcxG/zZIevoE69KCLbgELAA1So6qBglmeMaZjGTvwVFRUUFBQ0yv5M4wvFnLtnqKoN3mFMGPMn/iM9uQuH9uyxxB+erKnHGEN+fj6JiYlERkYe8b5ssLbwF+zEr8BCEVkpIhOCXJYxpoHy8/MbrXaemJhIVFSUjcsfxoLd1DNcVXeKSFtgkYhsUNWlVVfwfSFMADjqqKOCHI4xpjr5+fl07NixUfYlIqSkpFSO/WPCT1Br/Kq60/d3LzAXGFLNOlNUdZCqDsrIyAhmOMaYaqhqo1y8VVVqaqrV+MNY0BK/iMSLSKL/PnA2sCZY5RljGqa4uBiPx3PEF29V5a/xW5fO8BTMpp52wFzfgE+tgJdVdX4QyzPGNEBj9ujxS0lJwePxWJfOMBW0xK+qW4F+wdq/MaZx+K/abeymHnBG/LTEH36sO6cxLVxhYSHg9MZpLP55e62dPzxZ4jemhSsoKCAiIoL4+PhG22dycjIRERHWsydMWeI3poUrKioiMTHxiCZgOVxERARJSUmW+MOUJX5jWriCgoJGbebxsy6d4csSvzEtXGFhYaOe2PWzi7jClyV+Y1q4goICEhISGn2/KSkplJSU2MTrYcgSvzEtWGlpKWVlZUGp8fu7dFpzT/ixxG9MCxaMrpx+/i6d1twTfizxG9OC+RN/sNr4wWr84cgSvzEtmP+q3WDU+KOjo0lISLAafxiyxG9MCxbMph6wnj3hyhK/MS1YQUEBrVu3Jjo6Oij7t7784ckSvzEtWLD68PslJydTWFiIx+MJWhmm/izxG9OCFRYWBq2ZB6gc498/9LMJD5b4jWnBCgoKgl7jB9i/f3/QyjD1Z4nfmBbK6/VWDtAWLP7Ebyd4w4slfmNaqOLiYlQ1qIm/TZs2iIjV+MOMJX5jWqhgd+WEH4dntjb+8GKJ35gWKhhTLlYnOTnZmnrCjCV+Y1qoUNT4wUn81tQTXizxG9NCFRQUICKNOuVidZKTkykqKqK8vDyo5ZjAWeI3poXy9+GPiAhuGrC+/OHHEr8xLVSwL97y84/Sac094SPoiV9EIkXkKxF5O9hlGWMCF+yLt/zsIq7wE4oa/y3A+hCUY4yph2BNsn44f3OSJf7wEdTELyKdgV8AU4NZjjGmfoI55eLhRMR69oSZgBK/iMwWkV+ISH2/KB4H7gK8tex7goisEJEV2dnZ9dy9MaYhQtWV088Sf3gJNJE/DVwGbBaRh0SkZ10biMh5wF5VXVnbeqo6RVUHqeqgjIyMAMMxxhyJUF285WeJP7wElPhV9T1VvRwYAGwDFonIpyJytYhE1bDZcGC0iGwDXgV+LiIvNULMxpgjFMwpF6uTnJxMcXExZWVlISnP1C7gphsRSQPGA9cBXwH/xvkiWFTd+qp6j6p2VtVMYBzwvqpecaQBG2OOXDAnWa+O9eUPL4G28c8BPgLigF+q6mhVnamqE4GEYAZojGl8BQUFxMTEEBVV0w/2xmXDM4eXVgGuN1VV36m6QERaq2qpqg6qa2NVXQIsqX94xphgCPaUi4ezvvzhJdCmngerWfZZYwZijAmdUPXh90tISKBVq1aW+MNErTV+EWkPdAJiReREQHxPtcFp9jHGNEGFhYW0b98+ZOWJCElJSZb4w0RdTT0jcU7odgb+VWV5IXBvkGIyxgSRx+MJ+pSL1UlJSbHEHyZqTfyqOh2YLiIXqersEMVkjAmioqIiIHQ9evySkpLYsWNHSMs01aurqecKVX0JyBSR2w5/XlX/Vc1mxpgwFuo+/H7JyckcPHiQ0tJSWrduHdKyzaHqOrnrn6EhAUis5maMaWJC3Yffz4ZnDh91NfU84/t7f2jCMcYEW6iHa/Cr2qWzXbt2IS3bHCrQC7j+T0TaiEiUiCwWkRwRsatwjWmCCgsLiYyMJDY2NqTlWl/+8BFoP/6zVbUAOA/IAo4D7gxaVMaYoPH34ReRulduRHFxcURFRVniDwOBJn7/dd3nAq+oal6Q4jHGBFmor9r1s3H5w0egiX+eiGwABgGLRSQDKAleWMaYYAnVlIvVscQfHgIdlvkPwDBgkKqWA8XA+cEMzBjT+LxeL/n5+SQlJblSfnJysg3UFgYCHaQNoBdOf/6q27zQyPEYY4KoqKgIr9dbeaI11JKTkyktLaWkpISYmBhXYjABJn4ReRE4Bvga8PgWK5b4jWlS/M0sbiZ+cIZn7tChgysxmMBr/IOA3qqqwQzGGBNcbif+qhdxWeJ3T6And9cAoRvKzxgTFP7E72Ybf9U4jDsCrfGnA+tEZDlQ6l+oqqODEpUxJij2799PfHx8yGbeOlxMTAytW7e2xO+yQBP/pGAGYYwJjfz8fNeaecD68oeLgBK/qn4oIl2BY1X1PRGJAyKDG5oxprGFQ9u6del0X6Bj9VwPzAKe8S3qBLwRpJiMMUGgqq724ffzJ37rK+KeQE/u3gQMBwoAVHUz0DZYQRljGl9RUREej8fVph5wEn95eTkHDx50NY6WLNDEX6qqZf4Hvou47OvamCbE7a6cflX78ht3BJr4PxSRe3EmXT8LeB2YV9sGIhIjIstFZJWIrBURG9PfGBeFS+K3CVncF2ji/wOQDXwD/AZ4B/hTHduUAj9X1X5Af+AcERnawDiNMUcoL88ZVNftxG99+d0XaK8er4i8AbyhqtkBbqNAke9hlO9mzUPGuCQ3N5ekpCTX+vD7tW7dmtjYWEv8Lqq1xi+OSSKSA2wANopItoj8JZCdi0ikiHwN7AUWqern1awzQURWiMiK7OyAvlOMMQ2Qm5tLWlqa22EANjyz2+pq6rkVpzfPYFVNU9VU4GfAcBH5fV07V1WPqvYHOgNDROSEataZoqqDVHVQRkZGvV+AMaZuqkpOTo4lfgPUnfivAi5V1e/8C1R1K3CF77mAqOp+YAlwTv1DNMYcqeLiYsrKysIu8VtffnfUlfijVDXn8IW+dv5aGwpFJENEkn33Y4EROM1FxpgQy8lxPsbp6ekuR+JITk6moqKC4uJit0Npkeo6uVvWwOcAOgDTRSQS5wvmNVV9uz7BGWMaR25uLkBY1fjB6cufkJDgbjAtUF2Jv5+IFFSzXIBap89R1dXAiQ0NzBjTeHJzc2nVqpXrwzX4Ve3L36VLF5ejaXlqTfyqagOxGdMM5Obmkpqaioi4HQrw43wAdoLXHYFewGWMacJycnLCpn0fIDo6mvj4eEv8LrHEb0wzV1FRwb59+8Kmfd/PunS6xxK/Mc1cdnY2qkq7du3cDuUQlvjdY4nfmGZu9+7dALRvH17TZvsTv9frdTuUFscSvzHN3O7du4mKiqrsSRMukpOT8Xq9FBUV1b2yaVSW+I1p5vbs2UO7du2IiAivj7uN0ume8PpPMMY0KlVl9+7dYde+Dz/25bcJWULPEr8xzVh+fj6lpaVh174P1pffTZb4jWnG/Cd2w7HG36pVKxITEy3xu8ASvzHNWDgnfnCae6ypJ/Qs8RvTjO3YsYOMjAyio6PdDqVaqamplVNCmtCxxG9MM6WqZGVl0blzZ7dDqVFqaiqFhYWUldU12K9pTJb4jWmmcnNzKSkpCfvED9azJ9Qs8RvTTGVlZQE0icRvzT2hVdd4/MaYJiorK4vWrVsTqrmsVZXNmzezYMECvvzyS7Zs2cK+ffuIjIwkJSWFY489liFDhnDWWWeRmZkJ/NiX3xJ/aFniN6aZysrKolOnTkEfg//AgQP897//5dlnn2XVqlWA04uoZ8+eHHfccagq2dnZzJ07l6lTpwIwdOhQJkyYwGWXXUZcXJwl/hCzph5jmqHS0lL27t0b1GaeiooKJk+ezNFHH83NN99Mq1ateOKJJ9i6dSu7d+9myZIlzJkzh7lz5/Lxxx+TnZ3N+vXrefjhhykoKOCaa67hmGOOwePxWOIPMUv8xjRDO3fuRFWDlvhXrVrFwIEDmThxIr169WLp0qWsWLGCiRMn0q1bt2q3ERF69uzJXXfdxZo1a1iwYAGdO3dm+fLlrFu3jo8//jgosZqfssRvTDMUrBO7qsoTTzzBkCFDyM7OZvbs2bz//vuccsop9dqPiHD22Wfz6aefcsYZZxAbG8sZZ5zBddddZz18QsASvzHNUFZWFmlpacTGxjbaPvft28fo0aO55ZZbOPvss1m1ahVjxow5onMIERERnHbaaYgIt99+O88//zy9e/dm9uzZjRa3+SlL/MY0M/4Lt7p06dJo+9y2bRvDhw9nwYIFPPnkk7z11luN1lvI36Xz6quvZvny5bRv356xY8cyZswYdu7c2ShlmEMFLfGLSBcR+UBE1ovIWhG5JVhlGWN+tG/fPg4cOECnTp0aZX8rVqxg6NCh7Nq1i0WLFnHzzTc3ak+hqn35BwwYwPLly3nooYd499136d27N88++6zN0tXIglnjrwBuV9VewFDgJhHpHcTyjDH82L7fGDX+hQsXctpppxEbG8unn37KaaeddsT7PFxsbCwxMTGVPXuioqK4++67Wb16Nf3792fChAkMGjSId955B1Vt9PJboqD141fVXcAu3/1CEVkPdALWBatMY4yT+KOjo4+4KeZ///sfY8aMoVevXsyfPz9oY/qLCKmpqT85qXvsscfy/vvv8/LLL/OnP/2JX/ziF/Tq1YsrrriCc889l+OPP56oqCjAad7Kzc1l48aNbNq0qfK2efNmVJVOnToxcuRILr300rCcmyDUJBTfoCKSCSwFTlDVgsOemwBMADjqqKMGbt++PejxGNOcTZ06lVatWjF+/PgG7+PNN9/kV7/6FX369GHRokWVzTHBMnv2bHbs2MHvfve7ap8vLS3ltdde46mnnmLZsmWA88sgLS2NVq1asX///kPm7o2KiuKYY47h2GOPJTIykk2bNrFu3ToSExP561//ysSJE8NuKsqGEpGVqjqoPtsE/cpdEUkAZgO3Hp70AVR1CjAFYNCgQfY7zpgj4PF42L17N0OGDGnwPmbPns24ceMYMGAACxYsqJwbN5hSU1NZu3YtFRUVtGr107TUunVrrrzySq688kqysrJYunQpa9asITc3l7KyMpKTk+natSvHHXccxx13HJmZmT/Zz/r167ntttu49dZbWbVqFc8++yyRkZFBf23hKKiJX0SicJL+DFWdE8yyjDGQnZ2Nx+OhQ4cODdp+5syZXH755QwZMoR33323cnrEYEtPT0dVycvLo23btrWu27lzZy677LJ6l9GrVy/eeecd7r//fu6//35EhKlTpwZ9SItwFLTEL87RfA5Yr6r/ClY5xpgf+bs/duzYsd7bzpgxg6uuuorhw4fzv//9j8TExMYOr0bp6ekA5OTk1Jn4j4SIMGnSJDweDw8++CDDhw/nmmuuCVp54SqYjVzDgSuBn4vI177buUEsz5gWb9euXbRu3brebfLTp0/nyiuv5LTTTuPdd98NadIHSEtLA5zEHwqTJk1ixIgR3HTTTWzYsCEkZYaToCV+Vf1YVUVV+6pqf9/tnWCVZ4xxEn+HDh3q1Xwxbdo0rr76as4880zefvtt4uPjgxhh9aKjo0lKSgpZ4o+MjOTFF1+kdevW3HrrrS2um2jzOK1tjKk8sVuf9v1nn32Wa6+9lrPOOou33nqLuLi4IEZYu/T09JAlfoD27dszadIkFixYwNtvvx2ycsOBJX5jmgn/id1A2/cnT57MhAkTGDVqFG+++WajjuvTEP7EH8ra90033USvXr2488478Xg8ISvXbZb4jWkm/Cd266rxqyp//vOfmThxIueffz5z584lJiYmFCHWKj09nfLycgoKftLrO2iioqK4//772bhxI3PmtJyOh5b4jWkmdu7cWeeJ3YqKCiZMmMCDDz7Iddddx6xZs2jdunUIo6xZ1Z49oTRmzBh69OjB3/72txbT1m+J35hmoq4Tu3l5efzyl79k6tSp/PGPf2TKlCnVXizlFrcSf2RkJPfccw+rVq1i/vz5IS3bLZb4jWkGPB4Pe/bsqbGZZ9WqVQwePJjFixfzzDPP8OCDD4bdhUvx8fHExMSQnZ0d8rIvu+wy2rdvz+TJk0Nethss8RvTDOzdu7fGE7uvvPIKw4YNo6SkhA8//JAJEya4EGHdRIT09HRyc3NDXnZUVBQTJkzg3Xff5dtvvw15+aFmid+YZmDXrl3AoVfsVlRUcNttt3HZZZcxcOBAVq5cybBhw9wKMSCh7tJZ1YQJE4iIiODpp592pfxQssRvTDPgP7GbkpICOL8AzjrrLB577DEmTpzI4sWLm8RwxOnp6RQVFVFSUhLysjt16sQFF1zA888/T1lZWcjLDyVL/MY0A1VP7H7xxRcMHDiQZcuW8cILL/DEE08QHR3tdogB8Z/gdaOdH5zpH3Nzc5v9SV5L/MY0cf4Tux07dmTatGmccsopREZG8sknn3DllVe6HV69+CePcSvxn3322WRkZPDCCy+4Un6oWOI3ponzn9h99913ufbaaznllFNYuXIlAwYMcDu0ektJSSEqKoq9e/e6Un5UVBSXXnop8+bN+8mMYM2JJX5jmrgtW7YAzgibd955J/Pnz68c7bKpEREyMjJcS/wAV111FWVlZbz++uuuxRBslviNacI2b97MM888Q0lJCY8++ij/93//1+RnlWrbti179uxxrfwBAwbQq1evZt3cY4nfmCbqyy+/5KSTTqJNmzZ06NCBq6++2u2QGkW7du04cODAIXPohpKIcNVVV/HJJ5+wdetWV2IINkv8xjRBH330EWeccQYJCQl06NCBXr16uR1So/HPwOVmc8/ll1+OiPDSSy+5FkMwWeI3polZsGABI0eOpEOHDsyZMwev19ugqRbDVbt27QBcbe7p0qULp59+Oi+++GKzHLjNEr8xTciHH37I+eefT48ePVi6dClerxeoeyjmpiQ+Pp74+HhXa/zg1Pq3bNnCypUrXY0jGCzxG9NEfPXVV4wePZqjjz6aRYsW0bZtW3bu3ElMTEzlFbvNRdu2bV1P/GPGjCEqKopXX33V1TiCwRK/MU3A5s2bOeecc0hKSmLBggWVV7g2ZI7dpqBdu3bs2bPH1VmxUlJSOOecc5g5c2blL6vmwhK/MWFu586dnH322Xi9XhYuXEiXLl2Auodibso6dOiAx+NxbcA2v3HjxpGVlcWnn37qahyNzRK/MWEsLy+PkSNHkpOTw7vvvkvPnj0rn9u9ezder5dOnTq5GGFw+L/M/KOOumX06NHExsY2u+aeoCV+EZkmIntFZE2wyjCmOSsuLua8885j06ZNvPnmmwwaNOiQ57OysgDo3LmzG+EFVVpaGlFRUa4n/oSEBM477zxef/11KioqXI2lMQWzxv88cE4Q929Ms1VWVsbYsWP5/PPPeeWVV/j5z3/+k3WysrJo06YNbdq0cSHC4IqIiKB9+/auJ35wmnv27t3LBx984HYojSZoiV9VlwJ5wdq/Mc2V1+tl/PjxzJ8/n2eeeYYxY8ZUu15WVlazrO37dejQobI5y02jRo0iMTGxWTX3WBu/MWFEVbnlllt45ZVX+Mc//sF1111X7XpFRUXs37+/2Sf+8vJyV6ZirCo2NpYLL7yQOXPmUFpa6mosjcX1xC8iE0RkhYiscGsMbmPCxQMPPMDkyZO5/fbbufvuu2tcrzm37/uFywlecJp79u/fz8KFC90OpVG4nvhVdYqqDlLVQf5JGIxpif75z38yadIkxo8fzyOPPFJr3/ysrCwiIiKaZVdOv4yMDKKjo/nhhx/cDoURI0aQmprabJp7XE/8xhiYPHkyd9xxB7/61a949tln67wga9u2bXTs2JFWrVqFKMLQi4iIoHPnzmGR+KOiohg7dixvvvkmBw4ccDucIxbM7pyvAJ8BPUQkS0SuDVZZxjRlzz77LBMnTuT8889nxowZdSbzgwcPsnPnTo4++ugQReieLl26sGfPHlcmXz/cuHHjKC4u5u2333Y7lCMWzF49l6pqB1WNUtXOqvpcsMoypql67LHHmDBhAqNGjWLmzJlERUXVuc22bdtQ1RaR+I866ijgx3Mabjr11FPp1KkT06dPdzuUI2ZNPca4wOv1ctddd3HbbbcxZswY5syZQ+vWrQPaduvWrURHRzfrE7t+nTt3RkT4/vvv3Q6FyMhIfv3rXzN//nx27NjhdjhHxBK/MSFWUlJSeQL3xhtv5LXXXiMmJibg7bdu3UpmZmaTn2IxENHR0XTo0CEs2vkBrr76arxeb5OfltESvzEhtHnzZk466SRefPFFHnjgAf7f//t/9Urg+/btIy8vr0U08/h16dKFrKwsysvL3Q6F7t27c9pppzFt2rQmPUGLJX5jQmTmzJkMHDiQ7du3M2/ePP785z/XezjldevWAXDccccFI8Sw1L17dyoqKti2bZvboQBwzTXXsGXLFj766CO3Q2kwS/zGBNnWrVs5//zzGTduHH379uXrr7/mvPPOa9C+1qxZQ6dOnZrdxCu1yczMJCoqik2bNrkdCgAXXXQRiYmJTJs2ze1QGswSvzFBUlhYyH333Ufv3r1ZvHgxDz/8MB988EHlePr1lZOTw+7duznhhBMaOdLw1qpVK4455hg2bdoUFs0r8fHxjBs3jtdff52CggK3w2kQS/zGNLLc3Fzuu+8+jjrqKB544AHGjBnDxo0bueuuuwLqrlmTNWucEc579+7dWKE2GccddxwFBQWuTsBe1bXXXsuBAweYOXOm26E0iCV+YxqBqvL5558zYcIEunbtygMPPMAZZ5zB8uXLefnll494shSv18uqVavIzMxslsMw1+XYY48FYMOGDS5H4hgyZAh9+/blySefDItfIfVlid+YI5CTk8Pjjz9O3759GTp0KDNmzODiiy9mzZo1zJkzh8GDBzdKOevXr2f//v0MGTKkUfbX1CQkJJCZmcnq1avDItGKCLfeeivffPMNixcvdjucemu+A32YsOf1elmzZg0rVqyoHHc9PT2d/v37c+KJJwZ8QVOoeb1e3n//faZOncrcuXMpKytjyJAhTJkyhUsuuaTRa+SqymeffUZKSgo9evRo1H03Jf379+eNN95g27ZtdOvWze1wuOyyy7jnnnt47LHHGDFihNvh1IslfhNyWVlZTJ48menTp7N79+5q10lMTOSCCy5g4sSJjVZrPlJ79uxh2rRpTJkyhW3btpGamsoNN9zAtddeS9++fYNW7vbt29mxYwejRo0iIqLl/kjv3bs3Cxcu5PPPPw+LxN+6dWtuuukm/vKXv7B69eqg/g80tpb7X2RCrqCggNtvv53u3bvz6KOPMmTIEKZPn87mzZs5ePAgpaWlbNu2jdmzZ3PxxRfz1ltvMWTIEM466yzef/99V37iqyoffPABl1xyCV26dOHee++lW7duvPzyy+zYsYN///vfQf3Ae71eFixYQJs2bTjxxBODVk5TEBUVxeDBg9m4cSM7d+50OxwAbr75Ztq0acP999/vdij1o6phcxs4cKCa5mnevHnaoUMHFRG9+uqr9bvvvqtzm4KCAn3kkUe0ffv2Cujpp5+uy5YtC36wqpqbm6v/+te/9LjjjlNAU1JS9Pe//72uX78+JOX7rVy5UidNmqSrV68Oabnh6uDBg/rII4/o1KlT1ev1uh2Oqqr+5S9/UUC//vprV8oHVmg9c63ryb7qzRJ/83PgwAG96aabFNC+ffvq8uXL672PgwcP6pNPPqlt27ZVQMeMGROUBOz1evWTTz7Rq666Slu3bq2ADhs2TKdPn64HDhxo9PLqkp+frw8//LA+99xzYZPkwsGXX36pkyZNci3RHm7fvn2alJSko0aNcqV8S/wmrKxevVqPP/54BfT3v/+9lpSUHNH+CgsL9f7779eEhASNjIzU66+/XrOyso44zv379+vkyZP1hBNOUEATExP1xhtvdDWxeDwenTZtmv7tb3/TnJwc1+IIR16vV6dOnar/+Mc/NDc31+1wVFX1n//8pwL69ttvh7xsS/wmLHi9Xp0yZYrGxMRou3btdP78+Y26/z179ujvfvc7jYqK0piYGL3pppt07dq19drHgQMHdNasWXrxxRdrbGysAjpgwACdMmWKFhYWNmq89eX1evWtt97SSZMm6apVq1yNJVzt27dPH3roIf3Pf/6jZWVlboejpaWl2qNHD+3evbsePHgwpGVb4jeuy8/P13HjximgI0aM0N27dwetrK1bt+r48eM1OjpaAe3Xr5/ee++9+s477+h3332nBw4cUK/XqwcPHtTvvvtOFyxYoPfdd5+OGDFC4+LiFNCMjAy94YYb9IsvvghanPXh8Xj0f//7n06aNEkXLVrkdjhhbePGjTpp0iR99dVX1ePxuB2OLly4UAG97bbbQlquJX7jqhUrVugxxxyjERER+re//S1kH8Y9e/boY489pieddJJGRkYqUOMtIiJC+/fvrzfffLO+9957Wl5eHpIYA1FUVKQvvPCCTpo0SRcsWGDt+gFYtmyZTpo0Sd9+++2wOF433nijArp48eKQldmQxC/OduFh0KBBumLFCrfDMPVUUlLC3//+dx566CHatWvHK6+8wsknn+xKLEVFRXz55Zds2rSJ7OxsDh48SGxsLG3btqVbt24MGjQo7IY8qKioYOXKlSxZsoTy8nLOPfdcBgwY4HZYTcaiRYv49NNPOfPMM137v/MrLi5m4MCB5OXlsXz5cjIzM4NepoisVNVB9drGEr85Eh9++CG/+c1v2LhxI1dccQWPP/44aWlpbofVJBQWFvLVV1+xfPlyiouL6datG6NGjSIjI8Pt0JoUVWXu3Ll88803nHvuua5f8Ldx40aGDh1Kp06dWLp0KampqUEtryGJ367cNQ2ybNkyHnjgAd59910yMzOZP38+I0eOdDussFdSUsL69ev55ptvKidN7969O8OGDaNbt271npjFOOPmnH/++ZSWlvLOO+9QVlbGSSed5Nqx7NGjB7Nnz+bcc8/ltNNOY9GiRbRv396VWGpiNX4TsOLiYubNm8e0adNYtGgRaWlp3Hbbbdxyyy3Ex8e7HV7YKi8vZ/PmzXzzzTds3rwZj8dDamoqJ5xwAn369CE9Pd3tEJsFj8fDnDlzWLduHT169GDUqFEkJSW5Fs/ixYsZPXo0aWlpzJw5k2HDhgWlHGvqMY3K4/Gwbt06PvvsM5YsWcJbb71FcXExHTt25JZbbuG3v/0tCQkJbocZljweD9999x1r1qxhw4YNlJaWkpCQwPHHH0+fPn3o2LGj1e6DQFVZtmwZixcvxuv10rNnT44++mi6dOlCRkZGyMc6WrlyJRdffDHbt29n4sSJ/OUvf2n02dPCLvGLyDnAv4FIYKqqPlTb+k0p8asqBw4cYP/+/eTn55Ofn09hYSFerxf/MfWfQQfn56iIEBERUfm36v3D/6oqHo8Hr9dbeav6uLbnVLXWffvviwglJSUUFBRQWFhIYWEheXl5bNu2jW3btrFlyxYKCwsBSE9P58ILL+Syyy7jlFNOqdcE4c1FeXk5xcXFFBQUkJ+fT3FxMR6P55Bjf+DAAfLy8ionB2/dujW9evWiT58+ZGZmtuhB1kJp//79fP7556xdu7byfzg6OpqOHTvStWtXunfvTseOHUPyfuTn53PnnXcydepUEhISuPrqq7niiisYNGhQo3z5h1XiF5FIYBNwFpAFfAFcqqrratom2Inf6/VSUVFBcXExxcXFFBUVUVRUdMj9wsLCykReNalXt8zj8QQtVrfExcXRtWtXMjMzOeaYYxg8eDDDhg2je/fuYVtDVVVKSko4ePDgT24lJSVERUURFxdHXFwcsbGxlTcRwePxUFZWdsj/gf/vgQMHDnlcVlZWZyxxcXEkJyfTqVMnjj76aLp3706rVnYqzS2qyr59+8jKyqq87dq1C4DY2FiOPvpojjnmGNq2bUtqaiqxsbFBi2X16tU88sgjzJw5k/LycjIyMhg4cGDl7YILLmjQZyzcEv8wYJKqjvQ9vgdAVf9R0zYNTfy9e/emqKgIj8dDRUVFjX/r81pFhDZt2pCUlERycjJJSUmH3KpblpiYSGRkZGVt2r8fEUFVK2vjVf/WtMxfM4+MjKy8f/jjmp7zHedq9314uTExMSQmJlbeoqOj6338Q2n27Nn88MMPVFRUVN6C8QUcGxtLQkIC8fHxJCQkEBcXV/nY/38RHx9Pq1atKo97uH4xmkMdOHCArVu3smXLFr799luKiooqn4uIiCAqKoqoqCgiIyMZPHgww4cPb9Ty8/LymDdvHkuWLOHLL79k7dq1tG3btsEjjoZbr55OwA9VHmcBPzt8JRGZAEzwPSwSkVwgJ4hxBURVK2v233//fX02TScM4m8gi909TTl+i/0I7dq1q6EVh3Sga303Cmbir+5V/KTKrapTgCmVG4msqO+3VzhpyvFb7O5pyvFb7O7xxZ9Z3+2CeWYjC+hS5XFnIDxmTzDGmBYsmIn/C+BYEekmItHAOOCtIJZnjDEmAEFr6lHVChG5GViA051zmqquDWDTKXWvEtaacvwWu3uacvwWu3saFH9YXcBljDEm+OxqEmOMaWEs8RtjTAvjeuIXkVQRWSQim31/fzKQhYh0EZEPRGS9iKwVkVvciLVKPOeIyEYR2SIif6jmeRGRJ3zPrxaRsBpcPYD4L/fFvVpEPhWRfm7EWZ26Yq+y3mAR8YjI2FDGV5tAYheR00Xka9//+YehjrE2AfzfJInIPBFZ5Yv/ajfirI6ITBORvSKypobnw/YzG0Ds9f+81nfmlsa+Af8H/MF3/w/Aw9Ws0wEY4LufiDMURG+X4o0EvgWOBqKBVYfHApwLvItzLcNQ4HO3j3M94z8JSPHdHxUu8QcSe5X13gfeAca6HXc9jnsysA44yve4rdtx1zP+e/2fXyADyAOi3Y7dF8+pwABgTQ3Ph/Nntq7Y6/15db3GD5wPTPfdnw5ccPgKqrpLVb/03S8E1uNcGeyGIcAWVd2qqmXAqzivoarzgRfUsQxIFpEOoQ60BnXGr6qfquo+38NlONdghINAjj3ARGA2sDeUwdUhkNgvA+ao6vcAqtrU4lcgUZxLUBNwEn9FaMOsnqouxYmnJmH7ma0r9oZ8XsMh8bdT1V3gJHigbW0ri0gmcCLwefBDq1Z1Q1Ec/iUUyDpuqW9s1+LUhMJBnbGLSCfgQuA/IYwrEIEc9+OAFBFZIiIrReSqkEVXt0Dinwz0wrlQ8xvgFlX1hia8IxbOn9n6COjzGpJhA0XkPaC6KWj+WM/9JODU5G5V1YLGiK0BAhmKIqDhKlwScGwicgbOP5K7E5n+KJDYHwfuVlVPmA2aFkjsrYCBwJlALPCZiCxT1U3BDi4AgcQ/Evga+DlwDLBIRD5y8bNaH+H8mQ1IfT6vIUn8qjqipudEZI+IdFDVXb6fVtX+vBWRKJykP0NV5wQp1EAEMhRFOA9XEVBsItIXmAqMUtXcEMVWl0BiHwS86kv66cC5IlKhqm+EJMKaBfp/k6OqxUCxiCwF+uGc03JbIPFfDTykTmPzFhH5DugJLA9NiEcknD+zdarv5zUcmnreAn7tu/9r4M3DV/C1GT4HrFfVf4UwtuoEMhTFW8BVvp4CQ4F8f3NWGKgzfhE5CpgDXBkmtU2/OmNX1W6qmqnOwFWzgN+GQdKHwP5v3gROEZFWIhKHM5rt+hDHWZNA4v8e59cKItIO6AFsDWmUDRfOn9laNejzGgZnrNOAxcBm399U3/KOwDu++yfj/OxajfNT8mvgXBdjPhenFvYt8EffshuAG3z3Bfh/vue/AQa5fZzrGf9UYF+VY73C7ZgDjf2wdZ8nTHr1BBo7cCdOz541OE2arsddj/+bjsBC3//8GuAKt2OuEvsrwC6gHKd2f21T+cwGEHu9P682ZIMxxrQw4dDUY4wxJoQs8RtjTAtjid8YY1oYS/zGGNPCWOI3xpgWxhK/Mca0MJb4TdgSkY4iMsvtOMKViBS5HYNpmqwfvwkZ3xXYok1n4K6AiEgrVQ35KJQiUqSqCaEu1zR9VuM3QSUimeJMoPMU8CXwZxH5wjdpxP2+dR4Wkd9W2WaSiNzu23aNb1mkiDxSZdvf+JY/JSKjfffnisg03/1rReTBGmKKF5H/+SYMWSMil/iWD/ZNZLFKRJaLSKKIxIjIf0XkGxH5yjcQFiIyXkReF5F5wELfPqf54vtKRM73rXe8b19f++I+toaYajoGCSKyWES+9MXwk2GoxZm85e0qjyeLyHjf/YEi8qFvtM8FEiZDDRuXuX05st2a9w3IBLw4k1ucDUzBuTw+AngbZ5KJE4EPq2yzDjjKt+0a37IJwJ9891sDK4BuOGPGPOJbvhxY5rv/X2BkDTFdBDxb5XESzuQiW4HBvmVtcAYxvB34r29ZT5zxaGKA8TiXz/uHGPk7viEKcCZU2QTEA08Cl/uWRwOxNcRU0zFoBbTxLUsHtvDjL/Ui39/TgberbDvZF18U8CmQ4Vt+CTDN7f8Ju7l/C8nonKbF266qy0TkUZzk/5VveQJwrKo+JyJtRaQjzsxN+1T1e3HmXvA7G+grP06lmAQcC3wE3CoivXGSZYqvVjsM+F0N8XwDPCoiD+MkzI9EpA+wS1W/AFDfUMIicjJO8kZVN4jIdpxx8wEWqap/goyzgdEicofvcQxO4v4M+KOIdMaZZGVzdQGp6lc1HIMo4O8icirOF2gnoB2wu4bXVlUP4ASc4ZHBmUWrSQw8ZoLLEr8JhWLfXwH+oarPVLPOLGAszrwNr1bzvAATVXXBT55w5mk+B1gKpAIX49SGC6sLRlU3ichAnEHH/iEiC4E3qH789doG9S+ucl+Ai1R142HrrBeRz4FfAAtE5DpVfb+G/VV3DC7H+SIYqKrlIrIN50ulqgoObbb1Py/AWlUdVstrMC2QtfGbUFoAXCPOhDqISCcR8c+49ipOs81YnARY3bY3+mrAiMhxIhLve+4z4FacxP8RcIfvb7V8teoDqvoS8CjOfKYbgI4iMti3TqKItPLt83J/mTi1+MOTuz++ib4T2IjIib6/RwNbVfUJnKF/+9ZyfKo7BknAXl/SPwPoWs1224HeItJaRJLwDY3sizNDRIb5YokSkeNrKd+0EFbjNyGjqgtFpBfOzFIARcAVOIltrYgkAju0+nHQp+K0+X/pS67Z/Dg/80fA2aq6xdcUk0otiR/oAzwiIl6coW5vVNUy30neJ0UkFjgIjACeAv4jIt/g1KzHq2qp/HR2r7/izP612hffNuA8nHb1K0SkHKd55oFajk91x2AGME9EVuAMubuhmu1+EJHXcIYt34yvKc33msYCT/i+EFr5Ylxby7ExLYB15zTGmBbGmnqMMaaFsaYe02yJiH92t8OdqS7NIxyOMZmWx5p6jDGmhbGmHmOMaWEs8RtjTAtjid8YY1oYS/zGGNPC/H89gJCWHitq8gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "ax1=sns.distplot(y_test,hist=False,color='black',label='actual value')\n",
    "ax2=sns.distplot(pred_test_rr,hist=False,color='gray',label='predicted value')\n",
    "plt.title(\"Listing Columns + Count_Vectors\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f8fc4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lambda_values = 10**np.linspace(10,-2,100)*0.5\n",
    "ridge_model = Ridge()\n",
    "coefficients = []\n",
    "\n",
    "for i in lambda_values:\n",
    "    ridge_model.set_params(alpha = i)\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    coefficients.append(ridge_model.coef_)\n",
    "    \n",
    "ax = plt.gca()\n",
    "ax.plot(lambda_values, coefficients) \n",
    "plt.ylabel(\"Coeff weights\")\n",
    "plt.xlabel(\"Alpha values\")\n",
    "plt.title(\"Coeff Weights changing with Alpha\")\n",
    "ax.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bfcfc480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "840 fits failed out of a total of 1680.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.28295943 0.31205824        nan        nan 0.2313147  0.2313147\n",
      "        nan        nan 0.28295943 0.31205824        nan        nan\n",
      " 0.2313147  0.2313147         nan        nan 0.28295943 0.31205824\n",
      "        nan        nan 0.23313297 0.23313297        nan        nan\n",
      " 0.28295943 0.31205824        nan        nan 0.28559638 0.28577815\n",
      "        nan        nan 0.28295943 0.31205824        nan        nan\n",
      " 0.33979354 0.33988449        nan        nan 0.28295943 0.31205824\n",
      "        nan        nan 0.3345142  0.33451445        nan        nan\n",
      " 0.28295943 0.31205824        nan        nan 0.32123978 0.32069506\n",
      "        nan        nan 0.28295943 0.31205824        nan        nan\n",
      " 0.31678358 0.31960378        nan        nan 0.28295943 0.31205824\n",
      "        nan        nan 0.30623729 0.31632969        nan        nan\n",
      " 0.28295943 0.31205824        nan        nan 0.3023258  0.31323862\n",
      "        nan        nan 0.28295943 0.31205824        nan        nan\n",
      " 0.29959903 0.31132878        nan        nan 0.28295943 0.31205824\n",
      "        nan        nan 0.29768869 0.31151167        nan        nan\n",
      " 0.28295943 0.31205824        nan        nan 0.29905171 0.3116932\n",
      "        nan        nan 0.28295943 0.31205824        nan        nan\n",
      " 0.29723407 0.31178378        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.33988449047312813\n",
      "Best Hyperparameters: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "#convert y values to categorical values\n",
    "lab = preprocessing.LabelEncoder()\n",
    "y_transformed_train = lab.fit_transform(y_train)\n",
    "y_transformed_test = lab.fit_transform(y_test)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# define model\n",
    "model = LogisticRegression()\n",
    "# define evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "# define search space\n",
    "space = dict()\n",
    "space['solver'] = ['newton-cg', 'lbfgs']\n",
    "space['penalty'] = ['none', 'l1', 'l2', 'elasticnet']\n",
    "space['C'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 20, 50, 100, 200, 250, 300, 500]\n",
    "# define search\n",
    "search = GridSearchCV(model, space, scoring='accuracy', n_jobs=-1, cv=cv)\n",
    "# execute search\n",
    "result = search.fit(X_train, y_transformed_train)\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf8d9363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "C =  10\n",
      "Train accuracy score: 0.5837\n",
      "Test accuracy score: 0.3418\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logit = LogisticRegression(C=1, random_state=0, solver='lbfgs',multi_class='multinomial', max_iter=20000 ,penalty = 'l2')\n",
    "logit.fit(X_train, y_transformed_train)\n",
    "l_pred = logit.predict(X_test)\n",
    "print(\"\\n\")\n",
    "print(\"C = \",10)\n",
    "print('Train accuracy score:',round(logit.score(X_train, y_transformed_train),4))\n",
    "print('Test accuracy score:',round(logit.score(X_test, y_transformed_test),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24aa0d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28699918233851185\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn.fit(X_train, y_transformed_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(accuracy_score(y_transformed_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "32f38199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1223, 5)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binarizer = LabelBinarizer().fit(y_transformed_train)\n",
    "y_onehot_test = label_binarizer.transform(y_transformed_test)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e2cca39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/arraysetops.py:608: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_binarizer.transform([\"0\"])\n",
    "label_binarizer.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "71a693b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_of_interest = 0\n",
    "class_id = np.flatnonzero(label_binarizer.classes_ == class_of_interest)[0]\n",
    "class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e8024a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "y_score = logit.fit(X_train, y_transformed_train).predict_proba(X_test)\n",
    "\n",
    "RocCurveDisplay.from_predictions(\n",
    "    y_onehot_test[:, class_id],\n",
    "    y_score[:, class_id],\n",
    "    name=f\"{class_of_interest} vs the rest\",\n",
    "    color=\"darkorange\",\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n",
    "plt.axis(\"square\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"One-vs-Rest ROC curves:\\n 0 vs (1,2,3,4)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab716e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.40      0.41       257\n",
      "           1       0.30      0.39      0.34       292\n",
      "           2       0.23      0.12      0.16       198\n",
      "           3       0.37      0.44      0.40       234\n",
      "           4       0.34      0.30      0.32       242\n",
      "\n",
      "    accuracy                           0.34      1223\n",
      "   macro avg       0.33      0.33      0.33      1223\n",
      "weighted avg       0.34      0.34      0.33      1223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(classification_report(y_transformed_test, l_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e72416a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.30      0.32       257\n",
      "           1       0.28      0.50      0.36       292\n",
      "           2       0.23      0.33      0.27       198\n",
      "           3       0.33      0.28      0.30       234\n",
      "           4       0.00      0.00      0.00       242\n",
      "\n",
      "    accuracy                           0.29      1223\n",
      "   macro avg       0.24      0.28      0.25      1223\n",
      "weighted avg       0.24      0.29      0.25      1223\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/pallavit/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(classification_report(y_transformed_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05a32d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
